{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 4b: Beer Game DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+HMTY9zW1UUsk7itGo390",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/Part_4b_Beer_Game_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN for the Beer Game\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filenameâ€”not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University.\n",
        "---"
      ],
      "metadata": {
        "id": "4REAg0yRBTmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following Oroojlooyjadid, et al. (2022), we'll consider the following 4-node series system:\n",
        "\n",
        "![beer game system](https://raw.githubusercontent.com/LarrySnyder/RLforInventory/main/images/beer-game-schematic.png)\n",
        "\n",
        "The long-run systemwide expected cost is given by\n",
        "\n",
        "$$\\sum_{t=1}^T \\sum_{i=1}^4 h^i(IL_t^i)^+ + p^i(-IL_t^i)^+$$\n",
        "\n",
        "where $h^i$ and $p^i$ are the holding and stockout costs at node $i$, $IL_t^i$ is the inventory level at node $i$ at the end of period $t$, $T$ is the number of periods in one play of the game, and $z^+ \\equiv \\max\\{0,z\\}$.\n",
        "\n",
        "The inventory levels $IL_t^i$ are complicated random functions of the decision variables (i.e., the ordering policies), so this cost is difficult to formulate, let alone to optimize. Under certain assumptions (e.g., no fixed costs, stationary demands, etc.), and if there is a centralized decision maker who can make all of the ordering decisions, then a base-stock policy is optimal (Clark and Scarf 1960), and the optimal base-stock levels can be found relatively easily by optimizing a sequence of single-variable, convex problems (Chen and Zheng 1994).\n",
        "\n",
        "However, in the beer game, there is no centralized decision maker: Each node is controlled by a different player, each of whom make independent decisions about their ordering policies. Moreover, each player only knows the values of the state variables at their own node, not at the other nodes. The goal of our RL agent is to **choose order quantities at a single node to minimize the total systemwide cost under incomplete information.**"
      ],
      "metadata": {
        "id": "mZgYaBvFBbYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chess, Go, Atari, and other games that have successfully been solved by deep RL algorithms tend to have the following characteristics:\n",
        "\n",
        "* Competitive\n",
        "* Zero-sum\n",
        "* Full information\n",
        "* Instant reward signal (in some cases)\n",
        "\n",
        "But the beer game differs along all of these dimensions:\n",
        "\n",
        "* It is cooperative (the 4 players try to minimize their total cost)\n",
        "* It is not zero-sum (when one player succeeds, the whole team succeeds)\n",
        "* Players have only partial information (they have state information about only their own node)\n",
        "* The reward signal is delayed until the end of the game (since costs at other nodes are unknown during the game)\n",
        "\n",
        "Oroojlooyjadid, et al. (2022) propose an DQN-based algorithm they call the *shaped-reward DQN* (SRDQN). The SRDQN algorithm deals with the partial information by restricting the state variables that are available to the agent when making decisions. It deals with the delayed reward signal using **reward shaping,** which updates the reward information retroactively after the game ends. (We won't consider this in our simplified algorithm in this notebook, though.)"
      ],
      "metadata": {
        "id": "m5WGzhCaX3NJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff\n",
        "\n",
        "First we'll install the Python packages we need that are not pre-installed in Colab. The `pip install` commands below worked for me; I hope they work for you. I recommend not modifying the version numbers in the commands. Once you start tinkering with the dependencies, things can get messy. (Take my word for it.) "
      ],
      "metadata": {
        "id": "vL2mOvQrCZh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.2\n",
        "!pip install gym==0.23\n",
        "!pip install keras==2.8.0\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "id": "JijBa08URv4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "id": "Zpegaj6mXC3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import the packages we need."
      ],
      "metadata": {
        "id": "0NFSAGmCD48x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mqn9M8MZRym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stockpyl import sim\n",
        "from stockpyl.supply_chain_network import serial_system"
      ],
      "metadata": {
        "id": "16f2ytIQXl0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beer Game Environment\n",
        "\n",
        "#### States \n",
        "\n",
        "Assume that the RL agent is the decision maker at node $i\\in \\{1,\\ldots,4\\}$. (For example, if the RL is playing the role of the warehouse, then $i=2$.)\n",
        "As in OroojlooyJadid, et al., we assume that **state space** has 4 components:\n",
        "\n",
        "* $IL_t^i$, the inventory level at node $i$ in period $t$\n",
        "* $OO_t^i$, the on-order quantity at node $i$ in period $t$\n",
        "* $AO_t^i$, the arriving order (i.e., the demand received from the downstream neighbor) at node $i$ in period $t$\n",
        "* $AS_t^i$, the arriving shipment (i.e., the units received from the upstream neighbor) at node $i$ in period $t$\n",
        "\n",
        "In fact, the SRDQN algorithm assumes that we store the history of these state variables for the most recent 5 or 10 periods, but we will only use 1 period's worth of information in the algorithm below.\n",
        "\n",
        "It's natural to store the state as a tuple $(IL, OO, AO, AS)$, but it can be tricky to handle a tuple-based state in Tensorflow. Therefore, we convert the state tuple to a unique integer. The state space is therefore of type `Discrete` (using `gym` state classes). We'll never use the state integer directly; we'll covert the state tuple to an integer for storage and indexing, and convert back to a tuple when we need to know the individual state components. The `tuple_to_int()` and `int_to_tuple()` methods in the `BeerGameEnv` class do these conversions."
      ],
      "metadata": {
        "id": "2mOI44FjEPkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constants below provide the indices of the state-space components, so we don't have to remember them."
      ],
      "metadata": {
        "id": "0xWWeCzWdz1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortcuts to indices of the various states in the state space tuple.\n",
        "kIL = 0\n",
        "kOO = 1\n",
        "kAO = 2\n",
        "kAS = 3"
      ],
      "metadata": {
        "id": "AZ2aWem5FqVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actions\n",
        "\n",
        "Actions represent order quantities. In theory, any nonzero order quantity is allowed. However, to keep the state space manageable, we will require that the order quantity differs from the most recent demand by at most a fixed number (e.g., 5). In other words, if $AO$ is the most recent demand (arriving order), the order quantity is $AO+a$, where $a$ is constrained to be in some set such as $\\{-5,\\ldots,5\\}$. (This is sometimes called a \"$d+x$\" rule.)\n",
        "\n",
        "$a$ is the action, and can be different in different time periods."
      ],
      "metadata": {
        "id": "a8_J5eUXeWKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is the `BeerGameEnv` environment class. The code is missing some pieces. Your job is to fill in the missing pieces.\n",
        "\n",
        "---\n",
        "> **Note:** In the code below, the portions that you need to complete are marked with\n",
        "> \n",
        "> ```python\n",
        "> # #################\n",
        "> # TODO:\n",
        "> ```\n",
        "> \n",
        "> In place of the missing code is a line that says \n",
        "> \n",
        "> ```python\n",
        "> \traise NotImplementedError\n",
        "> ```\n",
        "> \n",
        "> This is a way of telling Python to raise an exception (error) because there's something missing here. You should **delete (or comment out) this line** after you write your code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PEzDFPXxd6xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BeerGameEnv(Env):\n",
        "    \"\"\"Beer game problem environment. A state represents a tuple (IL, OO, AO, AS),\n",
        "    where:\n",
        "    \n",
        "        * IL = inventory level at the agent at the end of the time period\n",
        "        * OO = on-order quantity at the end of the time period (items the agent \n",
        "            has ordered but not yet received)\n",
        "        * AO = arriving order, i.e., demand during the time period\n",
        "        * AS = arriving shipment, i.e., units received during the time period\n",
        "\n",
        "    However, this tuple is converted to an int via tuple_to_int() so that the\n",
        "    observation space is a 1-dimensional array.\n",
        "    \n",
        "    Actions represent differences from the demand observed in the time period.\n",
        "    That is, if the action is a, then the order quantity is AO + a. \n",
        "    a is restricted to be in a certain range, e.g., {-2, 1, 0, 1, 2}.\n",
        "    (This is sometimes called a \"d+x\" rule.)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : SupplyChainNetwork\n",
        "        The network to simulate.\n",
        "    episode_length : int\n",
        "        The number of periods in one episode.\n",
        "\tagent_node_index : int\n",
        "\t\tIndex of the node that the RL agent will play (e.g., 2 = wholesaler).\n",
        "    min_state : tuple\n",
        "        The minimum value of each state to consider: IL, OO, AO, AS.\n",
        "    max_state : tuple\n",
        "        The maximum value of each state to consider: IL, OO, AO, AS.\n",
        "    min_action : int\n",
        "        The minimum allowable action.\n",
        "    max_action : int\n",
        "        The maximum allowable action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network, episode_length: int, agent_node_index: int,\n",
        "\t            min_state: tuple, max_state: tuple, min_action: int, max_action: int):\n",
        "\n",
        "        # Store problem data.\n",
        "        self.network = network\n",
        "        self.episode_length = episode_length\n",
        "        self.agent_node_index = agent_node_index\n",
        "        self.min_state = min_state\n",
        "        self.max_state = max_state\n",
        "        self.min_action = min_action\n",
        "        self.max_action = max_action\n",
        "\n",
        "        # #################\n",
        "        # Set self.action_space to a gym Discrete space with elements\n",
        "        # min_action, min_action + 1, ..., max_action.\n",
        "        # (Hint: remember that you can use the `start` parameter; see \n",
        "\t\t# `MPNVEnv.__init__()` in the \"MPNV DQN\" notebook.)\n",
        "        # Also set self.action_space_list to a list with the same elements.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Determine the sizes of each component of the state space, and the\n",
        "        # total number of states in integer form.\n",
        "        self.state_size = [max_state[i] - min_state[i] + 1 for i in range(4)]\n",
        "        self.num_int_states = self.tuple_to_int(tuple(max_state[i] for i in range(4))) + 1\n",
        "        # Set the observation space as a Discrete space, as well as a list version.\n",
        "        self.observation_space = Discrete(self.num_int_states)\n",
        "        self.observation_space_list = list(range(self.num_int_states))\n",
        "\n",
        "        # #################\n",
        "        # Set self.initial_state assuming all components start at 0. That is,\n",
        "\t\t# use tuple_to_int() to set it to the integer version of the tuple (0, 0, 0, 0).\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Initialize current state info.\n",
        "        self.state = None\n",
        "\n",
        "        # Get shortcuts to the RL agent node (as a SupplyChainNode object) \n",
        "\t\t# and its predecessor and successor node indices.\n",
        "        self.agent_node = network.get_node_from_index(self.agent_node_index)\n",
        "        self.predecessor_index = self.agent_node.predecessor_indices(include_external=True)[0]\n",
        "        self.successor_index = self.agent_node.successor_indices(include_external=True)[0]\n",
        "\n",
        "    def tuple_to_int(self, the_tuple: tuple):\n",
        "        \"\"\"Convert a tuple (n_0, ..., n_{m-1}) to a unique integer, where element\n",
        "        n_i can take one of self.state_sizes[i] values beginning at self.min_state[i]; that is, \n",
        "        n_i can be in {self.min_state[i], self.min_state[i] + 1, ..., self.min_state[i] + self.state_size[i] - 1}.\n",
        "        \"\"\"\n",
        "        # Get length of tuple/lists.\n",
        "        m = len(self.state_size)\n",
        "        # Convert tuple to a tuple in which each element starts at 0.\n",
        "        new_tuple = tuple(the_tuple[i] - self.min_state[i] for i in range(m))\n",
        "        # Convert new_tuple to int.\n",
        "        the_int = 0\n",
        "        for i in range(m):\n",
        "            the_int += int(np.prod([self.state_size[j] for j in range(i + 1, m)]) * new_tuple[i])\n",
        "        return the_int\n",
        "\n",
        "    def int_to_tuple(self, the_int: int):\n",
        "        \"\"\"Convert an integer to a unique tuple (n_0, ..., n_{m-1}), where element\n",
        "        n_i can take one of self.state_size[i] values beginning at self.min_state[i]; that is, \n",
        "        n_i can be in {self.min_state[i], self.min_state[i] + 1, ..., self.min_state[i] + self.state_size[i] - 1}.\n",
        "        \"\"\"\n",
        "        # Get length of tuple/lists.\n",
        "        m = len(self.state_size)\n",
        "        # Convert int to a tuple assuming each element starts at 0.\n",
        "        the_list = []\n",
        "        for i in range(m):\n",
        "            base = int(np.prod([self.state_size[j] for j in range(i + 1, m)]))\n",
        "            the_list.append(the_int // base)\n",
        "            the_int = the_int % base\n",
        "        # Convert list to new list accounting for min values.\n",
        "        new_list = [the_list[i] + self.min_state[i] for i in range(m)]\n",
        "        return tuple(new_list)\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment and the simulation. Choose an initial state randomly from\n",
        "        the list of possible initial states. Return it and set it in self.inventory_level.\"\"\"\n",
        "\n",
        "        # #################\n",
        "        # Reset the environment, following the same steps as in the reset()\n",
        "\t\t# method of the `MPNVEnv` class in the \"MPNV DQN\" notebook.)\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Run one time step of the environment by taking the specified action.\n",
        "        Update the environment state to the new state. \n",
        "        Return a tuple (new_state, reward, done).\"\"\"\n",
        "\n",
        "\t\t# #################\n",
        "        # Convert self.state to a tuple.\n",
        "        raise NotImplementedError\n",
        "\n",
        "\t\t# #################\n",
        "        # Determine the order quantity.\n",
        "\t\t# Note: remember that the order quantity equals the most recent AO\n",
        "\t\t# (which is already stored in the state) plus the action.\n",
        "\t\t# Also: make sure to clip the order quantity so that it does not bring\n",
        "\t\t# the IL above its max value.\n",
        "        raise NotImplementedError\n",
        "\n",
        "\t\t# #################\n",
        "        # Build dict specifying order quantity to use in this time period.\n",
        "        # (This will override the order quantities that the stockpyl simulation\n",
        "        # would choose on its own.) \n",
        "\t\t# Note: the dict should contain only one entry, for the RL agent's\n",
        "\t\t# node; the other nodes are not included because we are not overriding\n",
        "\t\t# their order quantities.\n",
        "        raise NotImplementedError\n",
        "\n",
        "\t\t# #################\n",
        "        # Simulate one time period.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Determine reward by querying the simulation's state variables.\n",
        "        # NOTE: reward includes ALL nodes even though the agent only knows\n",
        "        # its own information. This is a simplification of the assumptions in\n",
        "        # Oroojlooyjadid et al (2021).\n",
        "        reward = -np.sum([n.state_vars_current.total_cost_incurred for \\\n",
        "                          n in self.network.nodes])\n",
        "\n",
        "\t\t# #################\n",
        "        # If episode length has been reached, terminate.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Get new state variables from simulation. (Round to int -- should \n",
        "        # already be integer but sometimes there are small rounding errors.)\n",
        "        # Clip states to state-space bounds.\n",
        "        IL = int(np.clip(self.agent_node.state_vars_current.inventory_level, \\\n",
        "                self.min_state[kIL], self.max_state[kIL]))\n",
        "        OO = int(np.clip(self.agent_node.state_vars_current.on_order, \\\n",
        "                self.min_state[kOO], self.max_state[kOO]))\n",
        "        AO = int(np.clip(self.agent_node.state_vars_current.inbound_order[self.successor_index], \\\n",
        "                self.min_state[kAO], self.max_state[kAO]))\n",
        "        AS = int(np.clip(self.agent_node.state_vars_current.inbound_shipment[self.predecessor_index], \\\n",
        "                self.min_state[kAS], self.max_state[kAS]))\n",
        "\n",
        "\t\t# #################\n",
        "        # Update the state: first determine the new state tuple, then convert\n",
        "\t\t# it to an integer and store it in self.state.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Fill the demand into the info dict. (This repeats what's already in AO.)\n",
        "        info = {'demand': self.agent_node.state_vars_current.inbound_order[self.successor_index]}\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"This function can contain code for drawing the environment to\n",
        "        a graphics window, or printing it in ASCII format to the terminal.\n",
        "        But we'll just do something very simple and print the state.\n",
        "        (Feel free to add some nicer visualization code here if you want!)\"\"\"\n",
        "        print(self.state)\n",
        "\n",
        "    def play_episode(self, policy, messages=False):\n",
        "        \"\"\"Play one episode of the environment following the specified policy. \n",
        "        Return the total discounted reward over the episode.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\n",
        "        \"\"\"\n",
        "        \n",
        "\t\t# #################\n",
        "        # Write this function, using the analogous function in `MPNVEnv` as a template.\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "fyiDsHkocwa0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beer Game Instance\n",
        "\n",
        "We'll use the following beer game instance. (This is similar to the \"simple instance\" in Â§4.1 of Oroojlooyjadid, et al. (2021).) The vectors below give the values for stages $1, ..., 4$, respectively. (Node 4 is upstream, node 1 is downstream.)\n",
        "\n",
        "* $h = [2, 2, 2, 2]$ \n",
        "* $p = [2, 0, 0, 0]$\n",
        "* $l^{tr} = [2, 2, 2, 2]$ (shipment lead time)\n",
        "* $l^{in} = [2, 2, 2, 2]$ (order lead time)\n",
        "* $D \\sim \\text{Poisson}(1)$ (demand uniformly drawn from Poisson distribution with mean 1)\n",
        "* Coplayers use base-stock policies with base-stock level 2\n",
        "\n",
        "We'll restrict the spaces as follows:\n",
        "\n",
        "* Action space: ${\\mathcal A} = \\{-2, -1, 0, 1, 2\\}$ (remember that the order quantity equals the action plus the observed demand)\n",
        "* State space: \n",
        "    * ${\\mathcal S}_{IL} = \\{-4, -3, ..., 4\\}$\n",
        "    * ${\\mathcal S}_{OO} = \\{0, 1, ..., 8\\}$\n",
        "    * ${\\mathcal S}_{AO} = \\{0, 1, ..., 4\\}$\n",
        "    * ${\\mathcal S}_{AS} = \\{0, 1, ..., 4\\}$\n",
        "\n",
        "And we'll use episodes of length 100.\n",
        "    \n"
      ],
      "metadata": {
        "id": "v2WJKrrz6S8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the network as a SupplyChainNetwork object.\n",
        "network = serial_system(\n",
        "    num_nodes=4,\n",
        "    node_order_in_system=[4, 3, 2, 1],  # in the network, nodes go 4 > 3 > 2 > 1\n",
        "    node_order_in_lists=[1, 2, 3, 4],   # in the lists below, nodes go 1 > 2 > 3 > 4\n",
        "    local_holding_cost=[2, 2, 2, 2],\n",
        "    stockout_cost=[2, 0, 0, 0],\n",
        "    shipment_lead_time=[2, 2, 2, 2],\n",
        "    order_lead_time=[2, 2, 2, 2],\n",
        "    demand_type='P', \n",
        "    mean=1,                         \n",
        "    policy_type='BS',                   \n",
        "    base_stock_level=2             \n",
        ")"
      ],
      "metadata": {
        "id": "sJKUIo8TXiGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_state = (-4, 0, 0, 0)\n",
        "max_state = (4, 8, 8, 8)\n",
        "min_action = -2\n",
        "max_action = 2\n",
        "episode_length = 100"
      ],
      "metadata": {
        "id": "nSUhScVD611O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's build our `BeerGameEnv` environment.\n",
        "\n",
        "Remember: This is now a well-defined `gym` environment. It's possible to \"register\" a custom environment to take advantage of the full `gym` API, but we won't need to do that here."
      ],
      "metadata": {
        "id": "j87w-pJB67pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build BeerGameEnv object.\n",
        "env = BeerGameEnv(\n",
        "    network=network,\n",
        "    episode_length=episode_length,\n",
        "    agent_node_index=2, # wholesaler\n",
        "    min_state=min_state,\n",
        "    max_state=max_state,\n",
        "    min_action=min_action,\n",
        "    max_action=max_action\n",
        ")"
      ],
      "metadata": {
        "id": "d39RKRwp6wa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give our new environment a quick spin. First, we'll create a base-stock policy with a base-stock level of 2 at every node. Then we'll ask our environment to play one episode of the beer game. In each time period, it will print the starting state, the action (order quantity), the demand, the new state, and the reward."
      ],
      "metadata": {
        "id": "cP2Y_QgE8jKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_stock_policy = {}\n",
        "for s in env.observation_space_list:\n",
        "    state_tuple = env.int_to_tuple(s)\n",
        "    base_stock_policy[s] = max(0, 2 - state_tuple[kIL])\n",
        "\n",
        "env.play_episode(base_stock_policy, messages=True)"
      ],
      "metadata": {
        "id": "ASsC_ztI3sWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up TensorFlow\n",
        "\n",
        "Next we'll set up our model in TensorFlow. First, some imports:"
      ],
      "metadata": {
        "id": "_HPX1ppg-TmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wSYk6yFwSZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy \n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "IcG-gFxhSqiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then a helper function to build the TF **model:**"
      ],
      "metadata": {
        "id": "RFO0_ZuC-sTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_states, num_actions):\n",
        "    model = Sequential()    \n",
        "    model.add(Dense(24, activation='relu', input_shape=(1,))) \n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(num_actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "5BNytuonShes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll build the model itself:"
      ],
      "metadata": {
        "id": "Eos3SOdM_KWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shortcut to size of observation and action spaces.\n",
        "num_states = env.observation_space.n\n",
        "#num_states = np.sum([sp.n for sp in env.observation_space.spaces])\n",
        "num_actions = env.action_space.n\n",
        "# Build the model.\n",
        "# NOTE: This must happen *after* the `from rl.x` imports.\n",
        "# (See https://stackoverflow.com/a/72438856/3453768)\n",
        "model = build_model(num_states, num_actions)"
      ],
      "metadata": {
        "id": "B4MEcBFtSndQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a summary of the model:"
      ],
      "metadata": {
        "id": "ZNol9lDX_cfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fizqVg6MSpQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need an RL **agent.** We'll use the `DQNAgent` class built into `keras` (part of TensorFlow). \n",
        "\n",
        "Our agent also needs a **policy.** We'll use the `EpsGreedyQPolicy`, again built into `keras`. (Feel free to play around with different policies. You'll have to `import` them like we did for `EpsGreedyQPolicy` above. I haven't been able to find good documentation for these policies, but you can find different policies to try by looking at the [source code](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).)"
      ],
      "metadata": {
        "id": "j3ziPVMQ_jI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = EpsGreedyQPolicy(eps=0.1) \n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "-1CEDkM4SyOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, build the DQN agent, store it in a variable called `dqn`, and \"compile\" it (a preprocessing step)."
      ],
      "metadata": {
        "id": "gmu1d7Z8BOoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, num_actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "id": "jJASpBtRS1RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DQN Agent\n",
        "\n",
        "Now we're finally ready for the main step: training the DQN agent. The command below trains it for 60,000 episodes, which should take about 10 minutes and produce medium-good results. Feel free to change this number to do more or less training."
      ],
      "metadata": {
        "id": "LzS1LLIYBgin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "id": "y51byZphBblU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most likely, you'll see the `episode_reward` get gradually better as the training progresses (though not necessarily monotonically so)."
      ],
      "metadata": {
        "id": "z6xdFPo6DlvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Results\n",
        "\n",
        "The DQN agent has a feature to test the learned policy by playing multiple episodes and print the results. Let's play 50 of them."
      ],
      "metadata": {
        "id": "aMOS9JTBCrvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = dqn.test(env, nb_episodes=50, visualize=False)\n",
        "print(f\"Average reward per episode = {np.mean(results.history['episode_reward'])}\")"
      ],
      "metadata": {
        "id": "0INhM5lxS-ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My DQN resulted in an average reward per episode of $-9405.72.52$. (Your mileage may vary.) Since this is an undiscounted episode with 100 periods, the average cost per period is $94.06$.\n",
        "\n",
        "Using a base-stock policy with a base-stock level of 2 at each node is a reasonable benchmark. `network` is already set up like this, so we can just simulate it."
      ],
      "metadata": {
        "id": "lo7ci-mXv7Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_cost_per_period, _ = sim.run_multiple_trials(network, num_trials=50, num_periods=episode_length)\n",
        "avg_cost_per_period"
      ],
      "metadata": {
        "id": "-ao7pQlbr303"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average cost per period from my simulation is $26.25$. The DQN is not competitive with the base-stock policy, but it's at least in the same ballpark, confirming that we are on the right track. More intensive training should improve the results."
      ],
      "metadata": {
        "id": "DIJo0uRVyLSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If You Have Extra Time\n",
        "\n",
        "Try to improve the results using different hyperparameters, training agents, etc.\n",
        "\n",
        "Or, try using DQN to optimize different supply chain networks other than the beer game system."
      ],
      "metadata": {
        "id": "wB_HNtgc0LJz"
      }
    }
  ]
}