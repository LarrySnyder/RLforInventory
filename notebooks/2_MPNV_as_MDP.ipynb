{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2: MPNV as MDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMs4DS6wPXpTyVCLVcgps71",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06bdd9aef73f469e84cdf166ee1786ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f88a053586054ef1961ecc604fcc9bb2",
              "IPY_MODEL_358ac0d69e8a452b8322e9871bca0b87",
              "IPY_MODEL_bd8e7214f945440c876e8726dc9d7b0b"
            ],
            "layout": "IPY_MODEL_49632a7ac66a4fc7a9e2e4339d14d570"
          }
        },
        "f88a053586054ef1961ecc604fcc9bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea9534423e04459ba1f03f15f0168741",
            "placeholder": "​",
            "style": "IPY_MODEL_989b52907f584a2e936ebd83e339c6b4",
            "value": "100%"
          }
        },
        "358ac0d69e8a452b8322e9871bca0b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c832f42e4e994823912532b06a5fd72e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50b5a00cf11948f88683bf11332e3f77",
            "value": 100
          }
        },
        "bd8e7214f945440c876e8726dc9d7b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4049725a8e404bf2a92f920312060dc6",
            "placeholder": "​",
            "style": "IPY_MODEL_31683e5822e8482aa37279d1e52aa5b5",
            "value": " 100/100 [00:19&lt;00:00, 15.50it/s]"
          }
        },
        "49632a7ac66a4fc7a9e2e4339d14d570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea9534423e04459ba1f03f15f0168741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989b52907f584a2e936ebd83e339c6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c832f42e4e994823912532b06a5fd72e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b5a00cf11948f88683bf11332e3f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4049725a8e404bf2a92f920312060dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31683e5822e8482aa37279d1e52aa5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/2_MPNV_as_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Multi-Period Newsvendor Problem (MPNV) as an MDP\n",
        "\n",
        "This notebook contains code for an MDP implementation of the multi-period newsvendor (MPNV) problem. \n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below Intro to Jupyter—not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. Double-click the filename at the top of the window and rename it Intro to Jupyter [your name(s)].\n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Profs. Larry Snyder, Lehigh University.\n",
        "---"
      ],
      "metadata": {
        "id": "mQmitR3h8t3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **multi-period newsvendor (MPNV)** problem, we must decide, in each time period, how much to order in advance of observing a random demand. Items that we order are received immediately (there is no lead time). Unmet demands are *backordered* (those customers are willing to wait until the next time period, with a penalty), and leftover items in inventory may be carried over to the next period (again, with a penalty).\n",
        "\n",
        "The *inventory level* is the number of on-hand items minus the number of backorders:\n",
        "\n",
        "$$IL = OH - BO.$$\n",
        "\n",
        "Note that $IL$ can be positive, negative, or zero. The number of on-hand items and the number of backorders can never both be positive at the same time. Therefore, the number of on-hand items is $OH = IL^+$ and the number of backorders is $BO = (-IL)^+$, where $a^+ \\equiv \\max\\{0, a\\}$. \n",
        "\n",
        "Suppose we begin the time period with an inventory level (IL) of $s$ and order $a$ units. We call $y \\eaiv s + a$ the *order-up-to level*. If $y$ is greater than the demand $d$ in a given period (that is, we have more items than we need), we incur a *holding cost* of $h$ per item. On the other hand, if $y$ is less than the demand (we have fewer items than we need), we incur a *stockout cost* of $p$ per item. That is, the cost incurred in the time period is\n",
        "\n",
        "$$h(y-d)^+ + p(d-y)^+ = h(s+a-d)^+ + p(d-(s+a))^+.$$\n",
        "\n",
        "Of course, the demand is stochastic, and we must choose $a$ (or, equivalently, $y$) before we observe it. Two common objective functions are used, one in which we minimize the expected cost per period and one in which we minimize the discounted cost over the time horizon. We'll assume an *infinite horizon*, though finite-horizon versions of the problem exist, too. (See Snyder and Shen (2nd edition, 2019).)\n",
        "\n"
      ],
      "metadata": {
        "id": "vYxyDehK89q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem can be formulated as a **Markov decision process (MDP)**. In this MDP, the state represents the IL at the beginning of the time period, and the action represents the order quantity. \n",
        "\n",
        "Let $v_\\pi(s)$ denote the **value function**: the expected return when starting in state $s$ (that is, starting a period with an IL of $s$) and folowing the **policy** $\\pi$ thereafter. As usual, the policy $\\pi$ is a mapping from states to actions. We'll assume it is a *deterministic policy* such that $\\pi(s)$ is the action the policy specifies in state $s$.\n",
        "\n",
        "The **Bellman equation** for the value function $v_\\pi$ is:\n",
        "\n",
        "$$v_\\pi(s) = {\\mathbb E}_D\\left[ -\\left(h(s + a -d)^+ + p(d - (s+a))^+\\right) + \\gamma v_\\pi(s+a-D)\\right],$$\n",
        "\n",
        "where ${\\mathbb E}_D$ denotes expectation over the random demand. For the optimal policy, the **Bellman optimality equation** is:\n",
        "\n",
        "$$v_*(s) = \\max_{a\\ge 0} {\\mathbb E}_D\\left[ -\\left(h(s+a-d)^+ + p(d-(s+a))^+\\right) + \\gamma v_*(s+a-D)\\right].$$\n",
        "\n",
        "The maximization is taken over all order quantities $a$, and since we cannot place an order of negative size, we require $a\\ge 0$.\n",
        "\n"
      ],
      "metadata": {
        "id": "TmcgRVt0MoKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we solve this MDP, we will find that the optimal policy follows a very particular structure. In particular, the optimal actions are given by:\n",
        "\n",
        "$$a(s) = \\begin{cases} B - s, & s < B \\\\ 0, & s \\ge B \\end{cases}$$\n",
        "\n",
        "This type of ordering policy is called a **base-stock policy,** and the quantity $B$ is called the **base-stock level.**\n",
        "\n",
        "---\n",
        "> **Note:** The term *policy* is used somewhat differently in inventory theory than in MDP/RL theory. In inventory theory, a policy is a simple ordering rule, whereas in MDP/RL, it is a much more general mapping of states to actions. In other words, an inventory policy is a special case of an MDP/RL policy in which the optimal actions follow some simple or convenient structure.\n",
        "\n",
        ">Also: Usually the base-stock level is denoted by $S$, but we use $B$ here to avoid confusion with the common notation for a state.\n",
        "---\n",
        "\n",
        "Once we know that a base-stock policy is optimal, we can express the expected cost per period as a function of the base-stock level $S$ as follows:\n",
        "\n",
        "$$g(B) = h\\sum_0^B (B-d)f(d) + p\\sum_B^\\infty (d-B)f(d),$$\n",
        "\n",
        "where $f(d)$ is the pmf of the probability distribution of the demand. One can show that the optimal base-stock level, $B^*$, is the smallest $B$ such that\n",
        "\n",
        "$$F(B) \\ge \\frac{p}{h+p}.$$\n",
        "\n",
        "The fraction on the right-hand side is known as the *newsvendor fractile* or the *critical fractile*. The expected cost function and the optimal solution are mathematically identical to those for the (single-period) **newsvendor problem,** even though that problem assumes that unmet demands are lost (rather than backordered) and that extra inventory units are scrapped (rather than held).\n",
        "\n",
        "(The results are similar if the demand has a continuous rather than discrete distribution, but we will only consider discrete distributions here.)"
      ],
      "metadata": {
        "id": "ocugkmFoQLF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will formulate and solve this problem as an MDP. This is possible since the dynamics of the system are fully known. We'll confirm, by solving the MDP numerically, that the optimal actions follow a base-stock policy. In the next section, we'll pretend that we don't know the dynamics and instead attack the problem using RL."
      ],
      "metadata": {
        "id": "hga80um_P5Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Python Stuff\n"
      ],
      "metadata": {
        "id": "XMUCsMuHnN2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytg9sReD8fRd"
      },
      "outputs": [],
      "source": [
        "# Import the packages we will need.\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import poisson\n",
        "from tqdm.notebook import tqdm\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDP Class\n",
        "\n",
        "The code below defines a Python class that implements algorithms for generic Markov decision processes (MDPs). The algorithm implementations are based on the discussions in Sutton and Barto (2nd edition, 2018).\n",
        "\n",
        "The main input to the MDP is the `dynamics` dictionary, which represents the function $p(s',r|s,a)$. In particular, `dynamics[s,a][s_prime,r]` is the probability that state `s_prime` occurs next and earns reward `r` given that we are in state `s` and take action `a`. \n",
        "\n",
        "Note that `dynamics` is a dict of dicts: Its keys are (state, action) pairs, and its values are dicts whose keys are (next state, reward) pairs and whose values are probabilities.\n",
        "\n",
        "Feel free to explore the code if you want, but all that's required is for you to execute the cell."
      ],
      "metadata": {
        "id": "29VrdwOu9RXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP(object):\n",
        "\t\"\"\"Generic class for an MDP.\n",
        "\n",
        "\t``dynamics`` is a dict whose keys are (state, action) pairs and whose values are dicts\n",
        "\twhose keys are (next_state, reward) pairs and whose values are probabilities.\n",
        "\tThat is, dynamics[(s, a)][(s_prime, r)] is the probability that state s_prime and reward r\n",
        "\tresults from taking action a in state s.\n",
        "\tOnly (next_state, reward) pairs that have nonzero probability need to be included.\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, dynamics, initial_state, terminal_states, gamma=0.9):\n",
        "\t\t# Parameters.\n",
        "\t\tself.dynamics = dynamics\n",
        "\t\tself.initial_state = initial_state\n",
        "\t\tself.terminal_states = terminal_states\n",
        "\t\tself.gamma = gamma\n",
        "\n",
        "\t\t# Build list of states.\n",
        "\t\tself.state_list = list({s for s, _ in self.dynamics.keys()})\n",
        "\t\t# Add terminal states. (These won't be in dynamics.)\n",
        "\t\tfor s in terminal_states:\n",
        "\t\t\tif s not in self.state_list:\n",
        "\t\t\t\tself.state_list.append(s)\n",
        " \n",
        " \t\t# Build dict of allowable actions.\n",
        "\t\tself.allowable_actions = {}\n",
        "\t\tfor s in self.state_list:\n",
        "\t\t\tself.allowable_actions[s] = list({a for state, a in self.dynamics if state == s})\n",
        "\n",
        "\t@property\n",
        "\tdef nonterminal_state_list(self):\n",
        "\t\treturn list(set(self.state_list).difference(self.terminal_states))\n",
        "\n",
        "\tdef get_next_state_and_reward(self, state, action):\n",
        "\t\t\"\"\"Return a next state and reward for a given (state, action) pair, \n",
        "    \tdrawn from the pair's dynamics.\n",
        "\t\t\"\"\"\n",
        "\t\t# Build lists of outcomes and probabilities.\n",
        "\t\toutcomes = list(self.dynamics[state, action].keys())\n",
        "\t\tprobabilities = list(self.dynamics[state, action].values())\n",
        "\n",
        "\t\t# Draw next state and reward from the distribution.\n",
        "\t\ti = np.random.choice(range(len(outcomes)), p=probabilities)\n",
        "\t\treturn outcomes[i]\n",
        "\n",
        "\tdef get_action_from_policy(self, state, policy):\n",
        "\t\t\"\"\"For a given state, return an action sampled from the specified policy.\"\"\"\n",
        "\t\tactions = list(policy[state].keys())\n",
        "\t\tprobabilities = list(policy[state].values())\n",
        "\n",
        "\t\ti = np.random.choice(range(len(actions)), p=probabilities)\n",
        "\n",
        "\t\treturn actions[i]\n",
        "\n",
        "\tdef get_action_equiprobable(self, state):\n",
        "\t\t\"\"\"For a given state, return an action sampled from an equiprobable policy.\"\"\"\n",
        "\t\ti = np.random.randint(len(self.allowable_actions[state]))\n",
        "\t\treturn self.allowable_actions[state][i]\n",
        "\n",
        "\tdef iterative_policy_evaluation(self, policy, initial_values=None, \n",
        "                                 theta=0.001, max_iter=None):\n",
        "\t\t\"\"\"Iterative policy evaluation algorithm for estimating V \\approx v_pi.\n",
        "\t\t(Section 4.1, p. 75.)\n",
        "\t\t\n",
        "\t\t``policy`` is a dict whose keys are states and whose values are dicts in \n",
        "    \twhich keys are actions and values are probabilities. That is, policy[s][a] \n",
        "    \tis the probability of action a in state s.\n",
        "\t\t\n",
        "\t\tNOTE: If there is a state from which no terminal state is reachable -- \n",
        "    \te.g., the policy goes left from (2, 2) and right from (1, 2) -- this \n",
        "    \talgorithm will not converge. The method does not check for this.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# Initialize value function estimate.\n",
        "\t\tV = {}\n",
        "\t\tfor s in self.state_list:\n",
        "\t\t\tif (s not in self.terminal_states) and (initial_values is not None) \\\n",
        "        and (s in initial_values):\n",
        "\t\t\t\tV[s] = initial_values[s]\n",
        "\t\t\telse:\n",
        "\t\t\t\tV[s] = 0\n",
        "\t\tk = 0\n",
        "\n",
        "\t\t# Loop.\n",
        "\t\twhile True:\n",
        "\t\t\tDelta = 0\n",
        "\t\t\tk += 1\n",
        "\t\t\t# Make static copy of V. (Although Sutton and Barto's pseudocode doesn't \n",
        "      \t\t# indicate this, their results suggest that they are using the same V on \n",
        "      \t\t# the RHS throughout the entire iteration, even though the V[s] values \n",
        "      \t\t# will change throughout the iteration.)\n",
        "\t\t\tV_copy = copy.deepcopy(V)\n",
        "\t\t\t# Loop through nonterminal states.\n",
        "\t\t\tfor s in self.nonterminal_state_list:\n",
        "\t\t\t\tv = V[s]\n",
        "\t\t\t\t# Update V[s].\n",
        "\t\t\t\tV[s] = 0\n",
        "\t\t\t\tfor a in self.allowable_actions[s]:\n",
        "\t\t\t\t\tif a in policy[s]:\n",
        "\t\t\t\t\t\tpi = policy[s][a]\n",
        "\t\t\t\t\t\tfor s_prime, r in self.dynamics[(s, a)].keys():\n",
        "\t\t\t\t\t\t\tp = self.dynamics[(s, a)][(s_prime, r)]\n",
        "\t\t\t\t\t\t\tV[s] += pi * p * (r + self.gamma * V_copy[s_prime])\n",
        "\n",
        "\t\t\t\t# Update Delta.\n",
        "\t\t\t\tDelta = max(Delta, abs(v - V[s]))\n",
        "\n",
        "\t\t\t# Terminate?\n",
        "\t\t\tif Delta < theta or (max_iter is not None and k >= max_iter):\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\treturn V\n",
        "\n",
        "\tdef greedy_policy(self, value_function, deterministic=False):\n",
        "\t\t\"\"\"Return greedy policy from given value function as described in \n",
        "    \tSection 4.2, p.79.\n",
        "\t\t\n",
        "\t\tIf a given state has multiple maximizing actions, then:\n",
        "\t\t* If `deterministic` is True, the action that comes earliest in the state list is chosen.\n",
        "\t\t* If `deterministic` is False, the they are assigned equal probability in the policy.\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# Initialize policy.\n",
        "\t\tpolicy = {s: {} for s in self.state_list}\n",
        "\n",
        "\t\t# Loop through states.\n",
        "\t\tfor s in self.state_list:\n",
        "\n",
        "\t\t\t# Determine maximizing action.\n",
        "\t\t\tbest_value = -np.inf\n",
        "\t\t\tfor a in self.allowable_actions[s]:\n",
        "\t\t\t\tvalue = 0\n",
        "\t\t\t\tfor s_prime, r in self.dynamics[(s, a)].keys():\n",
        "\t\t\t\t\tp = self.dynamics[(s, a)][(s_prime, r)]\n",
        "\t\t\t\t\tvalue += p * (r + self.gamma * value_function[s_prime])\n",
        "\t\t\t\tif value == best_value and not deterministic:\n",
        "\t\t\t\t\tbest_a.append(a)\n",
        "\t\t\t\telif value > best_value:\n",
        "\t\t\t\t\tbest_value = value\n",
        "\t\t\t\t\tbest_a = [a]\n",
        "\n",
        "\t\t\t# Build policy.\n",
        "\t\t\tfor a in self.allowable_actions[s]:\n",
        "\t\t\t\tif a in best_a:\n",
        "\t\t\t\t\tpolicy[s][a] = 1.0 / len(best_a)\n",
        "\t\t\t\n",
        "\t\treturn policy\n",
        "\n",
        "\tdef policy_iteration(self, initial_values=None, initial_policy=None, \n",
        "                      theta=0.001, max_iter=None, deterministic=False):\n",
        "\t\t\"\"\"Policy iteration algorithm for estimating pi \\approx pi_*.\n",
        "\t\t(Section 4.3, p. 80.)\n",
        "\t\t\n",
        "\t\t``initial_values`` is an optional dict whose keys are states and whose \n",
        "    \tvalues are initial estimates for the value function.\n",
        "\n",
        "\t\t``initial_policy`` is an optional dict whose keys are states and whose \n",
        "    \tvalues are dicts whose keys are actions and whose values are probabilities.\n",
        "\n",
        "\t\t``theta`` and ``max_iter`` are termination parameters for the policy \n",
        "    \tevaluation step.\n",
        "\n",
        "\t\tSet ``deterministic`` to True to use deterministic policies, ``False`` to \n",
        "    \tuse stochastic. (If initial policy is not set, will use equiprobably policy, \n",
        "    \teven if ``deterministic`` is True. Otherwise, random policy may be unbounded.)\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# Initialize value function and policy estimates.\n",
        "\t\tV = {}\n",
        "\t\tpi = {s: {} for s in self.state_list}\n",
        "\t\tfor s in self.state_list:\n",
        "\t\t\tif (s not in self.terminal_states) and (initial_values is not None) and (s in initial_values):\n",
        "\t\t\t\tV[s] = initial_values[s]\n",
        "\t\t\telse:\n",
        "\t\t\t\tV[s] = 0\n",
        "\t\t\tfor a in self.allowable_actions[s]:\n",
        "\t\t\t\tif (initial_policy is not None) and (a in initial_policy[s]):\n",
        "\t\t\t\t\tpi[s][a] = initial_policy[s][a]\n",
        "\t\t\tif np.all([pi[s][a] == 0 for a in self.allowable_actions[s] if a in pi[s]]):\n",
        "\t\t\t\t# Use eqiuprobable policy.\n",
        "\t\t\t\tfor a in self.allowable_actions[s]:\n",
        "\t\t\t\t\tpi[s][a] = 1.0 / len(self.allowable_actions[s])\n",
        "\t\tk = 0\n",
        "\n",
        "\t\t# Main loop.\n",
        "\t\tpolicy_stable = False\n",
        "\t\twhile not policy_stable:\n",
        "\n",
        "\t\t\tk += 1\n",
        "\n",
        "\t\t\t# Policy evaluation.\n",
        "\t\t\tV = self.iterative_policy_evaluation(pi, V, theta, max_iter, messages=False)\n",
        "\n",
        "\t\t\t# Policy improvement.\n",
        "\t\t\told_pi = copy.deepcopy(pi)\n",
        "\t\t\tpi = self.greedy_policy(V, deterministic=deterministic)\n",
        "\n",
        "\t\t\t# Check for termination.\n",
        "\t\t\tif pi == old_pi:\n",
        "\t\t\t\tpolicy_stable = True\n",
        "\n",
        "\t\treturn V, pi\n",
        "\n",
        "\tdef value_iteration(self, initial_values=None, theta=0.001, detailed_outputs=False):\n",
        "\t\t\"\"\"Value iteration algorithm for estimating pi \\approx pi_*.\n",
        "\t\t(Section 4.4, p. 83.)\n",
        "\t\t\n",
        "\t\t``initial_values`` is an optional dict whose keys are states and whose \n",
        "    \tvalues are initial estimates for the value function.\n",
        "\n",
        "\t\t``theta`` is a termination parameter for the policy evaluation step.\n",
        "\n",
        "\t\tIf ``detailed_outputs`` is False, returns only pi.\n",
        "\t\tOtherwise, returns:\n",
        "\t\t\t* ``pi``: final policy\n",
        "\t\t\t* ``V_final``: final value function\n",
        "\t\t\t* ``V_by_iter``: list of value functions, one per iteration of the algorithm\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# Initialize value function.\n",
        "\t\tV = {}\n",
        "\t\tfor s in self.state_list:\n",
        "\t\t\tif (s not in self.terminal_states) and (initial_values is not None) and (s in initial_values):\n",
        "\t\t\t\tV[s] = initial_values[s]\n",
        "\t\t\telse:\n",
        "\t\t\t\tV[s] = 0\n",
        "\t\tk = 0\n",
        "\n",
        "\t\t# Intialize other outputs.\n",
        "\t\tif detailed_outputs:\n",
        "\t\t\tV_by_iter = [V]\n",
        "\n",
        "\t\t# Main loop.\n",
        "\t\tDelta = np.inf\n",
        "\t\twhile Delta >= theta:\n",
        "\n",
        "\t\t\tk += 1\n",
        "\t\t\tDelta = 0\n",
        "\n",
        "\t\t\t# Make static copy of V.\n",
        "\t\t\tV_copy = copy.deepcopy(V)\n",
        "\n",
        "\t\t\t# Loop through states.\n",
        "\t\t\tfor s in self.nonterminal_state_list:\n",
        "\n",
        "\t\t\t\tv = V[s]\n",
        "\n",
        "\t\t\t\t# Update V[s].\n",
        "\t\t\t\tV[s] = np.max([np.sum([self.dynamics[(s, a)][(s_prime, r)] * (r + self.gamma * V_copy[s_prime]) \\\n",
        "\t\t\t\t\tfor s_prime, r in self.dynamics[(s, a)]]) for a in self.allowable_actions[s]])\n",
        "\n",
        "\t\t\t\t# Update Delta.\n",
        "\t\t\t\tDelta = max(Delta, abs(v - V[s]))\n",
        "\n",
        "\t\t\t# Remember value function.\n",
        "\t\t\tif detailed_outputs:\n",
        "\t\t\t\tV_by_iter.append(copy.deepcopy(V))\n",
        "\n",
        "\t\t# Build greedy policy.\n",
        "\t\tpi = self.greedy_policy(V, deterministic=True)\n",
        "\n",
        "\t\tif detailed_outputs:\n",
        "\t\t\treturn pi, V, V_by_iter\n",
        "\t\telse:\n",
        "\t\t\treturn pi\n",
        "\n",
        "\tdef play(self, num_time_steps, policy=None, messages=True):\n",
        "\t\t\"\"\" \"Play\" the MDP for a fixed number of time steps or until a terminal state is reached. \n",
        "\t\tIf policy is not specified, uses equiprobable policy.\n",
        "\n",
        "\t\tReturns both the total reward for the episode and a dict indicating the average reward for each state.\"\"\"\n",
        "\n",
        "\t\t# Initial state.\n",
        "\t\ts = self.initial_state\n",
        "\t\tif messages:\n",
        "\t\t\tprint(f\"Initial state: {s}\")\n",
        "\n",
        "\t\t# Initialize value function and discount factor.\n",
        "\t\tv = {s: 0 for s in self.state_list}\n",
        "\t\tcount = {s: 0 for s in self.state_list}\n",
        "\n",
        "\t\t# Initialize total_reward.\n",
        "\t\ttotal_reward = 0\n",
        "\n",
        "\t\t# Play.\n",
        "\t\tfor t in range(num_time_steps):\n",
        "\n",
        "\t\t\t# Choose action.\n",
        "\t\t\tif policy is None:\n",
        "\t\t\t\ta = self.get_action_equiprobable(s)\n",
        "\t\t\telse:\n",
        "\t\t\t\ta = self.get_action_from_policy(s, policy)\n",
        "\n",
        "\t\t\t# Calculate reward and update value function and return.\n",
        "\t\t\ts_next, r = self.get_next_state_and_reward(s, a)\n",
        "\t\t\ttotal_reward += r * self.gamma**t\n",
        "\t\t\tv[s] += r * self.gamma\n",
        "\t\t\tcount[s] += 1\n",
        "\n",
        "\t\t\tif messages:\n",
        "\t\t\t\tprint(f\"From state {s} taking action {self.action_abbr(a)} ==> reward {r} new state {s_next}\")\n",
        "\n",
        "\t\t\t# Update state.\n",
        "\t\t\ts = s_next\n",
        "\t\t\tif s in self.terminal_states:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif messages:\n",
        "\t\t\tprint(\"Final reward matrix:\")\n",
        "\t\t\trewards = {}\n",
        "\t\t\tfor s in self.state_list:\n",
        "\t\t\t\tif count[s] == 0:\n",
        "\t\t\t\t\trewards[s] = np.nan\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\trewards[s] = v[s] / count[s]\n",
        "\t\t\tself.print_values_by_state(rewards)\n",
        "\n",
        "\t\treturn total_reward, v"
      ],
      "metadata": {
        "id": "np4COQYT9KOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the MPNV MDP\n",
        "\n",
        "The next two functions build the `dynamics` dict and the `MDP` object for the MPNV problem. Recall that if we begin a time period with an IL of $s$ and order $a$ units, we begin the next time period in state $s' = s + a - d$ and incur a cost of\n",
        "$$h(s+a-d)^+ + p(d-(s+a))^+,$$\n",
        "i.e., we earn a reward of\n",
        "$$-\\left[h(s+a-d)^+ + p(d-(s+a))^+\\right].$$\n",
        "In other words, \n",
        "$$p(s',r|s,a) = f(s + a - s')$$\n",
        "if $s' \\le s + a$ and $r = h(s+a-d)^+ + p(d-(s+a))^+$, and $p(s',r|s,a)=0$ otherwise. This is how we calculate `dynamics`.\n",
        "\n",
        "---\n",
        "> **EXERCISE:** Fill in the missing piece of the code below to calculate the `dynamics` dict. \n",
        ">\n",
        "> There's a small catch: For a given $s, a$, you'll calculate $p(s',r|s,a)$ only for $s'$ that are in within the truncated state space (indicated by the `min_state` and `max_state` parameters). But since $s'$ can be arbitrarily small (since $d$ can be arbitrarily large), this is only an approximation of the state space. Therefore, the probabilities $p(s',r|s,a)$ will not sum to 1 for a given $s, a$. My code will pick up where yours left off, and fix this discrepancy for you.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IG-R0G1WEycz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_MPNV_dynamics(h: float, p: float, mu: int, min_state: int, max_state: int, \n",
        "                        max_order_quantity: int = None) -> dict:\n",
        "\t\"\"\"Build a ``dynamics`` object for the MPNV suitable for sending to an MDP object. \n",
        "\tAssumes the demand has a Poisson distribution.\n",
        "\n",
        "\t``dynamics`` is a dict whose keys are (state, action) pairs and whose values are dicts\n",
        "\twhose keys are (next_state, reward) pairs and whose values are probabilities.\n",
        "\tThat is, dynamics[(s, a)][(s_prime, r)] is the probability that state s_prime and reward r\n",
        "\tresults from taking action a in state s.\n",
        "\tOnly (next_state, reward) pairs that have nonzero probability are included.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\th : \n",
        "        Per-period holding cost.\n",
        "\tp : \n",
        "        Per-period stockout cost.\n",
        "\tmu : \n",
        "        Mean demand per period.\n",
        "\tmin_state, max_state :\n",
        "        Min and max of the state space. The state space will be \n",
        "        truncated to these limits. Although the true state space goes to negative \n",
        "        infinity (because the demand is unbounded) and to positive infinity (because \n",
        "        we don't know in advance how large the order will be), we must truncate it.\n",
        "    max_order_quantity : Maximum allowable order quantity. Used to reduce the \n",
        "        action space, if desired. Set to None (the default) to allow all order \n",
        "        quantities (up to the maximum of the state space).\n",
        "    gamma : \n",
        "        Discount factor. Defaults to 0.95.\n",
        "\t\"\"\"\n",
        "\t# Constants.\n",
        "\tmax_order_quantity = max_order_quantity or max_state - min_state + 1\n",
        "\n",
        "\t# Build state space.\n",
        "\tstate_space = list(range(min_state, max_state + 1))\n",
        "\taction_space = list(range(max_order_quantity + 1))\n",
        "\n",
        "\t# Build dynamics:\n",
        "\t# for each s in the state space:\n",
        "\t#\tfor each a in the action space such that s + a is in the state space: \n",
        "\t#\t\tfor each demand d such that s + a - d is in the state space:\n",
        "\t#\t\t\tset dynamics[s, a][s_prime, r] = P(D = d) for the appropriate\n",
        "\t#\t\t\tvalues of s_prime and r (where P(D = d) is the probability that\n",
        "\t#\t\t\tthe demand equals d)\n",
        "\tdynamics = {}\n",
        "\tfor s in state_space:\n",
        "\t\tfor a in action_space:\n",
        "\t\t\t# Make sure OUL is within state space truncation.\n",
        "\t\t\tif s + a in state_space:\n",
        "\t\t\t\tdynamics[s, a] = {}\n",
        "\t\t\t\t# Only consider demands that would not take us past the minimum state.\n",
        "\t\t\t\tfor d in range(s + a - min_state + 1):\n",
        "\t\t\t\t\toul = s + a\n",
        "\t\t\t\t\ts_prime = oul - d\n",
        "\t\t\t\t\tr = -(h * max(0, oul - d) + p * max(0, d - oul))\n",
        "\t\t\t\t\tdynamics[s, a][s_prime, r] = poisson.pmf(d, mu)\n",
        "\n",
        "\t# The probabilities for each (s, a) pair will not sum to 1 because of the\n",
        "\t# state-space truncation. Artificically inflate the probability for the\n",
        "\t# minimum state to account for this.\n",
        "\tfor s, a in list(dynamics.keys()):\n",
        "\t\tr = [rew for s_prime, rew in list(dynamics[s, a].keys()) if s_prime == min_state][0]\n",
        "\t\tdynamics[s, a][min_state, r] += 1 - np.sum(list(dynamics[s, a].values()))\n",
        "\t\t# Double-check that the probabilities sum to 1.\n",
        "\t\tassert np.isclose(np.sum(list(dynamics[s, a].values())), 1, atol=1.0e-6)\n",
        "   \n",
        "\treturn dynamics"
      ],
      "metadata": {
        "id": "oIAX12l_O4M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_MPNV_dynamics_INCOMPLETE(h: float, p: float, mu: int, min_state: int, max_state: int, \n",
        "                        max_order_quantity: int = None) -> dict:\n",
        "\t\"\"\"Build a ``dynamics`` object for the MPNV suitable for sending to an MDP object. \n",
        "\tAssumes the demand has a Poisson distribution.\n",
        "\n",
        "\t``dynamics`` is a dict whose keys are (state, action) pairs and whose values are dicts\n",
        "\twhose keys are (next_state, reward) pairs and whose values are probabilities.\n",
        "\tThat is, dynamics[(s, a)][(s_prime, r)] is the probability that state s_prime and reward r\n",
        "\tresults from taking action a in state s.\n",
        "\tOnly (next_state, reward) pairs that have nonzero probability are included.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\th : \n",
        "        Per-period holding cost.\n",
        "\tp : \n",
        "        Per-period stockout cost.\n",
        "\tmu : \n",
        "        Mean demand per period.\n",
        "\tmin_state, max_state :\n",
        "        Min and max of the state space. The state space will be \n",
        "        truncated to these limits. Although the true state space goes to negative \n",
        "        infinity (because the demand is unbounded) and to positive infinity (because \n",
        "        we don't know in advance how large the order will be), we must truncate it.\n",
        "    max_order_quantity : Maximum allowable order quantity. Used to reduce the \n",
        "        action space, if desired. Set to None (the default) to allow all order \n",
        "        quantities (up to the maximum of the state space).\n",
        "    gamma : \n",
        "        Discount factor. Defaults to 0.95.\n",
        "\t\"\"\"\n",
        "\t# #################\n",
        "\t# Write code to implement the missing step described in \n",
        "\t# comments below. When you are finished, delete the line below.\n",
        "\t# #################\n",
        "\traise NotImplementedError\n",
        "\n",
        "\t# Constants.\n",
        "\tmax_order_quantity = max_order_quantity or max_state - min_state + 1\n",
        "\n",
        "\t# Build state space.\n",
        "\tstate_space = list(range(min_state, max_state + 1))\n",
        "\taction_space = list(range(max_order_quantity + 1))\n",
        "\n",
        "\t# Build dynamics:\n",
        "\t# for each s in the state space:\n",
        "\t#\tfor each a in the action space such that s + a is in the state space: \n",
        "\t#\t\tfor each demand d such that s + a - d is in the state space:\n",
        "\t#\t\t\tset dynamics[s, a][s_prime, r] = P(D = d) for the appropriate\n",
        "\t#\t\t\tvalues of s_prime and r (where P(D = d) is the probability that\n",
        "\t#\t\t\tthe demand equals d)\n",
        "\t# \n",
        "\t# ---\n",
        "\n",
        "\t# The probabilities for each (s, a) pair will not sum to 1 because of the\n",
        "\t# state-space truncation. Artificically inflate the probability for the\n",
        "\t# minimum state to account for this.\n",
        "\tfor s, a in list(dynamics.keys()):\n",
        "\t\tr = [rew for s_prime, rew in list(dynamics[s, a].keys()) if s_prime == min_state][0]\n",
        "\t\tdynamics[s, a][min_state, r] += 1 - np.sum(list(dynamics[s, a].values()))\n",
        "\t\t# Double-check that the probabilities sum to 1.\n",
        "\t\tassert np.isclose(np.sum(list(dynamics[s, a].values())), 1, atol=1.0e-6)\n",
        "\n",
        "\treturn dynamics"
      ],
      "metadata": {
        "id": "f-9Pn-J9PgWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_MPNV_MDP(h: float, p: float, mu: int, min_state: int, max_state: int, \n",
        "                   max_order_quantity: int = None, initial_state: int = 0, \n",
        "                   gamma: float = 0.95) -> MDP:\n",
        "\t\"\"\"Build an MDP object representing an MPNV problem. Assumes the demand has a Poisson distribution.\n",
        "\n",
        "\tIn the MDP, a state represents an inventory position (= on-hand inventory minus backorders plus\n",
        "\ton-order inventory) and an action represents an order quantity. The objective is to maximize the\n",
        "\tnegative of the total discounted cost over the infinite horizon.\n",
        "\n",
        "\tFor parameter descriptions, see docstring for `build_MPNV_dynamics()`.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdynamics = build_MPNV_dynamics(h, p, mu, min_state, max_state, max_order_quantity)\n",
        "\t\n",
        "\tBSMDP = MDP(\n",
        "\t\tdynamics=dynamics,\n",
        "\t\tinitial_state=initial_state, \n",
        "\t\tterminal_states=[],\n",
        "\t\tgamma=gamma\n",
        "\t)\n",
        "\n",
        "\treturn BSMDP"
      ],
      "metadata": {
        "id": "e0HD_PiFC_Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function plots a given policy in two ways: order quantity vs. inventory position and order-up-to level (= inventory position + order quantity) vs. inventory position."
      ],
      "metadata": {
        "id": "CKpO9745O8vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy(pi: dict, title: str = None):\n",
        "\t\"\"\"Plot the policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tpi : \n",
        "\t\tProbability distribution for a policy. A dict whose keys are states and\n",
        "\t\twhose values are dicts whose keys are actions and whose values are probabilities.\n",
        "\t\tThat is, pi[s][a] = probability of taking action a in state s. Although the\n",
        "\t\tpolicies considered in this MDP are deterministic (and this function will\n",
        "\t\tassume they are), this stochastic format is what is returned by the optimization\n",
        "\t\talgorithms in the MDP class.\n",
        "\ttitle : \n",
        "\t\tOptional title for the figure.\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Initialize the figure and set the title.\n",
        "\tfig = plt.figure(figsize=plt.figaspect(1/2))\n",
        "\tfig.suptitle(title)\n",
        "\n",
        "\t# Build a list of x values and a dict of order quantities from the policy.\n",
        "\tx_list = sorted(list(pi.keys()))\n",
        "\tQ_dict = {x: a for x in x_list for a in pi[x] if pi[x][a] > 0}\n",
        "\n",
        "\t# Order quantity plot.\n",
        "\tax = plt.subplot(121)\n",
        "\tQ_list = [Q_dict[x] for x in x_list]\n",
        "\tplt.plot(x_list, Q_list)\n",
        "\tplt.xlabel('Inventory Level')\n",
        "\tplt.ylabel('Order Quantity')\n",
        "\n",
        "\t# Order-up-to level plot.\n",
        "\tax = plt.subplot(122)\n",
        "\ty_list = [x + Q_dict[x] for x in x_list]\n",
        "\tplt.plot(x_list, y_list)\n",
        "\tplt.xlabel('Inventory Level')\n",
        "\tplt.ylabel('Order-Up-To Level')\n",
        "\tplt.ylim([min(x_list), max(x_list)])\n",
        "\n",
        "\tplt.show()"
      ],
      "metadata": {
        "id": "2eyDTSLCO4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPNV Instance\n",
        "\n",
        "Finally, we are ready to solve an MPNV problem! We'll solve the following instance:\n",
        "\n",
        "* $h = 1$\n",
        "* $p = 10$\n",
        "* $\\mu = 5$\n",
        "\n",
        "We'll build an MDP for this instance using the `build_MPNV_MDP()` function.\n",
        "\n",
        "It's unlikely that we'd want to order up to something larger than 20, so we'll use 20 as the upper bound of our state space. And if we order up to 5, the probability of a demand greater than 20 (and hence a new inventory position less than $-$15) is less than $10^{-7}$, so we'll use $-$15 as the lower bound of our state space. Thus, we'll set `state_space_limits = (-15, 20)`.\n",
        "\n",
        "We'll use the default values for the other parameters of `build_MPNV_MDP()`."
      ],
      "metadata": {
        "id": "LCq1LFyaRIqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set problem parameters.\n",
        "h = 1\n",
        "p = 10\n",
        "mu = 5\n",
        "min_state = -15\n",
        "max_state = 20\n",
        "gamma = 0.95"
      ],
      "metadata": {
        "id": "5ZwIuLONRy5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pseudorandom number generator seed, for reproducibility when debugging.\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "_iU19daAPS50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MDP.\n",
        "mpnv_mdp = build_MPNV_MDP(h=h, p=p, mu=mu, min_state=min_state, max_state=max_state, gamma=gamma)"
      ],
      "metadata": {
        "id": "wmf8b3zYTJte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solving the MPNV MDP\n",
        "\n",
        "Here comes the main event -- optimization! This step should take a few seconds."
      ],
      "metadata": {
        "id": "8ugZA_ToB4xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize the order quantities using value iteration.\n",
        "pi = mpnv_mdp.value_iteration()\n",
        "\n",
        "# Alternately, use policy iteration.\n",
        "#_, pi = mpnv_mdp.policy_iteration()"
      ],
      "metadata": {
        "id": "Fw0uj5yHTPN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the results.\n",
        "\n",
        "First, let's plot the policy."
      ],
      "metadata": {
        "id": "eZnbhyA9CFfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the policy.\n",
        "plot_policy(pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "DIti-Wn5YP0N",
        "outputId": "69500cd1-2067-4116-aae1-cd4fe4abdfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all went well, the plot on the left (order quantity vs. inventory position) should decrease linearly with a slope of $-1$ until it flattens out at $y=0$; and the plot on the right (order-up-to level vs. inventory position) should be flat at first and then increase linearly with a slope of $1$. \n",
        "\n",
        "These shapes indicate that the optimal policy found by the MDP is a **base-stock policy:** We order up to a fixed value (the *base-stock level*), unless the inventory position is already greater than that value, in which case we order nothing. \n",
        "\n",
        "For this instance, the optimal base-stock level is 8. We can see this because the order-up-to level is 8 for any inventory positions less than or equal to 8; or because the order quantity is 0 when the inventory position is greater than or equal to 8."
      ],
      "metadata": {
        "id": "kzlqHdDqaBJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp_base_stock_level = list(pi[min_state].keys())[0] + min_state\n",
        "print(f\"Base-stock level found by MDP = {mdp_base_stock_level}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1zdv7m2FwcI",
        "outputId": "0bec3a79-7499-48b0-d082-f158145ab55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base-stock level found by MDP = 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's evaluate this policy by running a handful of episodes. The `play()` function of the `MDP` class makes this easy."
      ],
      "metadata": {
        "id": "PZD1vZ8wbJFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 100\n",
        "T = 1000 # number of time steps per episode\n",
        "avg_reward = 0\n",
        "pbar = tqdm(total=num_episodes) # progress bar\n",
        "for _ in range(num_episodes):\n",
        "    pbar.update()\n",
        "    total_reward, _ = mpnv_mdp.play(num_time_steps=T, policy=pi, messages=False)\n",
        "    avg_reward += total_reward / num_episodes\n",
        "\n",
        "print(f\"After {num_episodes} episodes, average total discounted reward per episode = {avg_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "06bdd9aef73f469e84cdf166ee1786ac",
            "f88a053586054ef1961ecc604fcc9bb2",
            "358ac0d69e8a452b8322e9871bca0b87",
            "bd8e7214f945440c876e8726dc9d7b0b",
            "49632a7ac66a4fc7a9e2e4339d14d570",
            "ea9534423e04459ba1f03f15f0168741",
            "989b52907f584a2e936ebd83e339c6b4",
            "c832f42e4e994823912532b06a5fd72e",
            "50b5a00cf11948f88683bf11332e3f77",
            "4049725a8e404bf2a92f920312060dc6",
            "31683e5822e8482aa37279d1e52aa5b5"
          ]
        },
        "id": "H5iY-tjZYgFZ",
        "outputId": "44de3cb9-a307-42b6-9b99-089fb2ecfe07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06bdd9aef73f469e84cdf166ee1786ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 100 episodes, average total discounted reward per episode = -84.62076961346955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Using `stockpyl`\n",
        "\n",
        "In this section, we'll validate the results of the MDP using the `stockpyl` Python package (https://pypi.org/project/stockpyl/). \n",
        "\n",
        "First we have to install the package. (It doesn't come pre-installed on Colab like `numpy`, etc. do.) You should only need to do this once.\n",
        "\n",
        "If you get a message like\n",
        "\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [sphinxcontrib]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```\n",
        "\n",
        "you can ignore it.\n",
        "\n"
      ],
      "metadata": {
        "id": "7DBqjy8AhOR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "id": "NIdhTSW0fRrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7d5c60a-106d-40e2-dca6-91c054c84e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stockpyl\n",
            "  Downloading stockpyl-0.0.13-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting build>=0.0.2\n",
            "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
            "Collecting sphinx-toolbox>=3.1.2\n",
            "  Downloading sphinx_toolbox-3.1.2-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (4.64.0)\n",
            "Collecting jsonpickle>=1.0\n",
            "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: setuptools>=49.6 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.21.6)\n",
            "Collecting sphinx-rtd-theme>=1.0.0\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (0.8.10)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (2.6.3)\n",
            "Collecting sphinx==4.5.0\n",
            "  Downloading Sphinx-4.5.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 48.2 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.10.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.11.3)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.2.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.4.1)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.17.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (4.12.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.7.12)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.23.0)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (21.3)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx==4.5.0->stockpyl) (2022.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: pep517>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (0.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx==4.5.0->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0->stockpyl) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2022.6.15)\n",
            "Collecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.1-py3-none-any.whl (10.0 kB)\n",
            "Collecting domdf-python-tools>=2.9.0\n",
            "  Downloading domdf_python_tools-3.3.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 55.0 MB/s \n",
            "\u001b[?25hCollecting autodocsumm>=0.2.0\n",
            "  Downloading autodocsumm-0.2.9-py3-none-any.whl (13 kB)\n",
            "Collecting sphinx-prompt>=1.1.0\n",
            "  Downloading sphinx_prompt-1.5.0-py3-none-any.whl (4.5 kB)\n",
            "Collecting html5lib>=1.1\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting sphinx-jinja2-compat>=0.1.0\n",
            "  Downloading sphinx_jinja2_compat-0.1.2-py3-none-any.whl (12 kB)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting dict2css>=0.2.3\n",
            "  Downloading dict2css-0.3.0-py3-none-any.whl (25 kB)\n",
            "Collecting beautifulsoup4>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting apeye>=0.4.0\n",
            "  Downloading apeye-1.2.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.2-py3-none-any.whl (12 kB)\n",
            "Collecting ruamel.yaml>=0.16.12\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachecontrol[filecache]>=0.12.6 in /usr/local/lib/python3.7/dist-packages (from sphinx-toolbox>=3.1.2->stockpyl) (0.12.11)\n",
            "Collecting platformdirs>=2.3.0\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting requests>=2.5.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4>=4.9.1->sphinx-toolbox>=3.1.2->stockpyl) (2.3.2.post1)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from cachecontrol[filecache]>=0.12.6->sphinx-toolbox>=3.1.2->stockpyl) (1.0.4)\n",
            "Collecting cssutils>=2.2.0\n",
            "  Downloading cssutils-2.5.1-py3-none-any.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting natsort>=7.0.1\n",
            "  Downloading natsort-8.1.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.1->sphinx-toolbox>=3.1.2->stockpyl) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.1.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.1-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, requests, natsort, sphinx, ruamel.yaml.clib, platformdirs, mypy-extensions, lockfile, domdf-python-tools, cssutils, typing-inspect, sphinx-tabs, sphinx-prompt, sphinx-jinja2-compat, sphinx-autodoc-typehints, ruamel.yaml, html5lib, dict2css, beautifulsoup4, autodocsumm, apeye, sphinx-toolbox, sphinx-rtd-theme, jsonpickle, build, stockpyl\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: natsort\n",
            "    Found existing installation: natsort 5.5.0\n",
            "    Uninstalling natsort-5.5.0:\n",
            "      Successfully uninstalled natsort-5.5.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed apeye-1.2.0 autodocsumm-0.2.9 beautifulsoup4-4.11.1 build-0.8.0 cssutils-2.5.1 dict2css-0.3.0 domdf-python-tools-3.3.0 html5lib-1.1 jsonpickle-2.2.0 lockfile-0.12.2 mypy-extensions-0.4.3 natsort-8.1.0 platformdirs-2.5.2 requests-2.28.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sphinx-4.5.0 sphinx-autodoc-typehints-1.19.1 sphinx-jinja2-compat-0.1.2 sphinx-prompt-1.5.0 sphinx-rtd-theme-1.0.0 sphinx-tabs-3.4.0 sphinx-toolbox-3.1.2 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 stockpyl-0.0.13 typing-inspect-0.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll validate the base-stock level and average reward produced by the MDP. The MPNV problem with Poisson demand has a simple analytical solution, which is implemented in the `newsvendor.newsvendor_poisson()` function in `stockpyl`. (Documentation for this function is available [here](https://stockpyl.readthedocs.io/en/latest/api/seio/newsvendor.html#stockpyl.newsvendor.newsvendor_poisson).)\n",
        "\n",
        "> **Note:** The `newsvendor_poisson()` function is actually solving the (single-period) newsvendor problem. However, the single-period and infinite-horizon versions of the problem have the same optimal solution and optimal expected cost per period (provided that either the discount factor or the purchase cost is 0; here, we are assuming the purchase cost is 0); see Snyder and Shen (2nd edition, 2019). Hence, we can use `newsvendor_poisson()` here.\n"
      ],
      "metadata": {
        "id": "JkoNb4xM1Sly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the stockpyl modules we'll need.\n",
        "from stockpyl.newsvendor import newsvendor_poisson\n",
        "from stockpyl import sim\n",
        "from stockpyl.supply_chain_network import single_stage_system"
      ],
      "metadata": {
        "id": "AHZztKJm1pjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve newsvendor problem with Poisson demand.\n",
        "base_stock_level, cost = newsvendor_poisson(h, p, mu)\n",
        "print(f\"Optimal base-stock level is {base_stock_level}, with expected cost per period {cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkL4G5qejoT2",
        "outputId": "624d7275-268a-4030-80df-52a2a761f3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal base-stock level is 8.0, with expected cost per period 4.34320221832773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal base-stock level matches what the MDP found. The costs differ, though. That's because the MDP `play()` function reports the total discounted cost over the episode (which we set to 1000 periods), while the `newsvendor_poisson()` function reports the expected (undiscounted) cost per period. \n",
        "\n",
        "But if the expected cost per period is $g$, then the expected discounted cost over $T$ time periods is\n",
        "\n",
        "$$\\sum_{t=0}^{T-1} g\\gamma^i = g\\frac{1 - \\gamma^{T}}{1-\\gamma}$$\n",
        "\n",
        "using standard formulas for geometric series. So:"
      ],
      "metadata": {
        "id": "CE3GdF1_39or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected_discounted_cost_per_episode = cost * (1 - gamma**T) / (1 - gamma)\n",
        "print(f\"Expected discounted cost over {T} periods is {expected_discounted_cost_per_episode}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDcw9qFv7TTr",
        "outputId": "12289c88-5789-4980-bd63-659ef98bfcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected discounted cost over 1000 periods is 86.86404436655452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cost is close to the (negative of the) reward reported by the MDP `play()` function."
      ],
      "metadata": {
        "id": "_izF3AgS8Bkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll simulate the system using `stockpyl`'s simulation features. This is equivalent to using the `MDP.play()` approach.\n",
        "\n",
        "First we have to build the supply chain network to be simulated. (It consists of a single node, and `stockpyl` provides a function to build it for us.) We'll tell it to use the base-stock level returned by the `newsvendor_poisson()` function."
      ],
      "metadata": {
        "id": "7hrx4rAR8ENM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build network to simulate.\n",
        "network = single_stage_system(\n",
        "    local_holding_cost=h,\n",
        "    stockout_cost=p,\n",
        "    demand_type='P',                    # Poisson demand\n",
        "    mean=mu,\n",
        "    policy_type='BS',                   # base-stock inventory policy\n",
        "    base_stock_level=base_stock_level,\n",
        "    shipment_lead_time=1\n",
        ")"
      ],
      "metadata": {
        "id": "8SqLGCwl8kGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we run the simulation. The `simulation()` function returns the average (undiscounted) cost per period, which again matches what we found above."
      ],
      "metadata": {
        "id": "kFwifkVzD7xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate system.\n",
        "mean_cost, _ = sim.run_multiple_trials(network, 100, 1000, progress_bar=False)\n",
        "print(f\"Average cost per period is {mean_cost}\")"
      ],
      "metadata": {
        "id": "et0jSRs0mErl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86be5a4f-2405-4571-fd22-1d4b8bff35d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cost per period is 4.30184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize:"
      ],
      "metadata": {
        "id": "yhRQ2hmLEMf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate([\n",
        "    [\"Base-stock level found by MDP\", mdp_base_stock_level],\n",
        "    [\"Optimal base-stock level\", base_stock_level],\n",
        "    [\"Average discounted reward per episode from MDP play()\", avg_reward],\n",
        "    [\"Optimal expected discounted reward per episode\", expected_discounted_cost_per_episode],\n",
        "    [\"Optimal expected cost per period\", cost],\n",
        "    [\"Simulated average cost per period\", mean_cost]\n",
        "]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtSPG04I81GN",
        "outputId": "6ce58586-f5c0-4d95-b402-ad080af01ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------  ---------\n",
            "Base-stock level found by MDP                            8\n",
            "Optimal base-stock level                                 8\n",
            "Average discounted reward per episode from MDP play()  -84.6208\n",
            "Optimal expected discounted reward per episode          86.864\n",
            "Optimal expected cost per period                         4.3432\n",
            "Simulated average cost per period                        4.30184\n",
            "-----------------------------------------------------  ---------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "\n",
        "**TBD...**"
      ],
      "metadata": {
        "id": "9IYS7ZjSU707"
      }
    }
  ]
}