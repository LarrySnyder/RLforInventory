{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4a: MPNV DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnUtXoA/wabS0K/POfB4C0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/4a_MPNV_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN for the Multi-Period Newsvendor Problem (MPNV)\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below Intro to Jupyterâ€”not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. Double-click the filename at the top of the window and rename it Intro to Jupyter [your name(s)].\n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Profs. Larry Snyder, Lehigh University.\n",
        "---"
      ],
      "metadata": {
        "id": "4REAg0yRBTmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll implement a simple DQN approach for solving the MPNV. We'll use the OpenAI `gym` package to define and manage our MPNV environment, and we'll use Tensorflow to do the deep RL.\n",
        "\n",
        "This notebook is just a demonstration; there are no exercises. Instead, you'll use it as a model to build a DQN approach for the beer game problem in the next notebook.\n",
        "\n",
        "The code used in this notebook is based on the approach outlined in the blog post \"[Building a Reinforcement Learning Environment using OpenAI Gym](https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/)\" by Lilian Tonia."
      ],
      "metadata": {
        "id": "mZgYaBvFBbYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff\n",
        "\n",
        "First we'll install the Python packages we need that are not pre-installed in Colab. The `pip install` commands below worked for me; I hope they work for you. I recommend not modifying the version numbers in the commands. Once you start tinkering with the dependencies, things can get messy. (Take my word for it.) "
      ],
      "metadata": {
        "id": "vL2mOvQrCZh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.2\n",
        "!pip install gym==0.23\n",
        "!pip install keras==2.8.0\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JijBa08URv4N",
        "outputId": "ac0652c8-3cce-4fc2-89fc-404821919f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.8.2 in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (14.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.47.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.14.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8.2) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.28.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.23\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 624 kB 4.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.23) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.23) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.23) (1.21.6)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym==0.23) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym==0.23) (3.8.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697660 sha256=ad50184150925737a75b6143501ad10f22bf2360bfd83639f0fb5c5c5d0a3ab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/7e/16/4d727df048fdb96518ec5c02266e55b98bc398837353852a6a\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.23.0 gym-notices-0.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52 kB 756 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.47.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zpegaj6mXC3B",
        "outputId": "3e480da7-769b-42ac-ad0b-b459a2bb7be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stockpyl\n",
            "  Downloading stockpyl-0.0.13-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (4.64.0)\n",
            "Collecting jsonpickle>=1.0\n",
            "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: matplotlib>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (3.2.2)\n",
            "Collecting sphinx==4.5.0\n",
            "  Downloading Sphinx-4.5.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1 MB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.7.3)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (0.8.10)\n",
            "Requirement already satisfied: setuptools>=49.6 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (57.4.0)\n",
            "Collecting sphinx-rtd-theme>=1.0.0\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.8 MB 28.4 MB/s \n",
            "\u001b[?25hCollecting build>=0.0.2\n",
            "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
            "Collecting sphinx-toolbox>=3.1.2\n",
            "  Downloading sphinx_toolbox-3.1.2-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90 kB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (4.12.0)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100 kB 12.2 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.17.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.10.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.7.12)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.1.5)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 121 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (21.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.2.0)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx==4.5.0->stockpyl) (2022.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: pep517>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx==4.5.0->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0->stockpyl) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (3.0.4)\n",
            "Collecting ruamel.yaml>=0.16.12\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109 kB 60.8 MB/s \n",
            "\u001b[?25hCollecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.1-py3-none-any.whl (10.0 kB)\n",
            "Collecting apeye>=0.4.0\n",
            "  Downloading apeye-1.2.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118 kB 55.6 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128 kB 56.7 MB/s \n",
            "\u001b[?25hCollecting domdf-python-tools>=2.9.0\n",
            "  Downloading domdf_python_tools-3.3.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting sphinx-prompt>=1.1.0\n",
            "  Downloading sphinx_prompt-1.5.0-py3-none-any.whl (4.5 kB)\n",
            "Collecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.2-py3-none-any.whl (12 kB)\n",
            "Collecting html5lib>=1.1\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting sphinx-jinja2-compat>=0.1.0\n",
            "  Downloading sphinx_jinja2_compat-0.1.2-py3-none-any.whl (12 kB)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: cachecontrol[filecache]>=0.12.6 in /usr/local/lib/python3.7/dist-packages (from sphinx-toolbox>=3.1.2->stockpyl) (0.12.11)\n",
            "Collecting dict2css>=0.2.3\n",
            "  Downloading dict2css-0.3.0-py3-none-any.whl (25 kB)\n",
            "Collecting autodocsumm>=0.2.0\n",
            "  Downloading autodocsumm-0.2.9-py3-none-any.whl (13 kB)\n",
            "Collecting platformdirs>=2.3.0\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting requests>=2.5.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4>=4.9.1->sphinx-toolbox>=3.1.2->stockpyl) (2.3.2.post1)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from cachecontrol[filecache]>=0.12.6->sphinx-toolbox>=3.1.2->stockpyl) (1.0.4)\n",
            "Collecting cssutils>=2.2.0\n",
            "  Downloading cssutils-2.5.1-py3-none-any.whl (399 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 399 kB 51.7 MB/s \n",
            "\u001b[?25hCollecting natsort>=7.0.1\n",
            "  Downloading natsort-8.1.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.1->sphinx-toolbox>=3.1.2->stockpyl) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.1.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.1-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, requests, natsort, sphinx, ruamel.yaml.clib, platformdirs, mypy-extensions, lockfile, domdf-python-tools, cssutils, typing-inspect, sphinx-tabs, sphinx-prompt, sphinx-jinja2-compat, sphinx-autodoc-typehints, ruamel.yaml, html5lib, dict2css, beautifulsoup4, autodocsumm, apeye, sphinx-toolbox, sphinx-rtd-theme, jsonpickle, build, stockpyl\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: natsort\n",
            "    Found existing installation: natsort 5.5.0\n",
            "    Uninstalling natsort-5.5.0:\n",
            "      Successfully uninstalled natsort-5.5.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed apeye-1.2.0 autodocsumm-0.2.9 beautifulsoup4-4.11.1 build-0.8.0 cssutils-2.5.1 dict2css-0.3.0 domdf-python-tools-3.3.0 html5lib-1.1 jsonpickle-2.2.0 lockfile-0.12.2 mypy-extensions-0.4.3 natsort-8.1.0 platformdirs-2.5.2 requests-2.28.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sphinx-4.5.0 sphinx-autodoc-typehints-1.19.1 sphinx-jinja2-compat-0.1.2 sphinx-prompt-1.5.0 sphinx-rtd-theme-1.0.0 sphinx-tabs-3.4.0 sphinx-toolbox-3.1.2 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 stockpyl-0.0.13 typing-inspect-0.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import the packages we need."
      ],
      "metadata": {
        "id": "0NFSAGmCD48x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mqn9M8MZRym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stockpyl import sim\n",
        "from stockpyl.supply_chain_network import single_stage_system"
      ],
      "metadata": {
        "id": "16f2ytIQXl0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Environment\n",
        "\n",
        "The code below defines an environment for the MPNV. This class is identical to the `MPNVEnv` class we defined in the \"RL for MPNV\" notebook, except:\n",
        "\n",
        "* The class is subclassed from the `gym.Env` class.\n",
        "* There is no `gamma` parameter. (We'll assume no discounting.)\n",
        "* The `__init__()` method defines the `action_space` and `observation_space` (aka state space) using datatypes provided by `gym`. These are a little annoying to query, so we also define `action_space_list` and `observation_space_list` as simple lists that can be accessed when needed.\n",
        "* There is no `allowable_actions` attribute. All actions are considered allowable for every state, but actions that bring the inventory level above `state_max` just bring the IL to `state_max`. \n",
        "* There are no methods to get actions (e.g., `get_epsilon_greedy_action()`. Tensorflow will handle this for us."
      ],
      "metadata": {
        "id": "2mOI44FjEPkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNVEnv(Env):\n",
        "\t\"\"\"Multi-period newsvendor (MPNV) problem environment. A state represents an inventory level. \n",
        "\tAn action is an order quantity.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tnetwork : SupplyChainNetwork\n",
        "\t\tThe network to simulate.\n",
        "\tepisode_length : int\n",
        "\t\tThe number of periods in one episode.\n",
        "\tmin_state : int\n",
        "\t\tThe minimum value of the state space to consider.\n",
        "\tmax_state : int\n",
        "\t\tThe maximum value of the state space to consider.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, network, episode_length: int, min_state: int, max_state: int):\n",
        "\n",
        "\t\tself.action_space = Discrete(max_state + 1)\n",
        "\t\tself.observation_space = Discrete(max_state - min_state + 1, start=min_state)\n",
        "\t\t# for convenience\n",
        "\t\tself.action_space_list = list(range(max_state + 1))\n",
        "\t\tself.observation_space_list = list(range(min_state, max_state + 1))\n",
        "\n",
        "\t\t# Store problem data.\n",
        "\t\tself.network = network\n",
        "\t\tself.episode_length = episode_length\n",
        "\t\tself.min_state = min_state\n",
        "\t\tself.max_state = max_state\n",
        "\n",
        "\t\t# Initial states are the non-negative states. (Of course it's possible to start with a negative\n",
        "\t\t# IL, but let's keep things simpler.)\n",
        "\t\tself.initial_states = list(range(max_state + 1))\n",
        "\n",
        "\t\t# Initialize current state info.\n",
        "\t\tself.state = None\n",
        "\n",
        "\t\t# Get a shortcut to the (single) node in the network, for convenience.\n",
        "\t\tself.node = network.nodes[0]\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t\"\"\"Reset the environment and the simulation. Choose an initial state randomly from\n",
        "\t\tthe list of possible initial states. Return it and set it in self.inventory_level.\"\"\"\n",
        "\n",
        "\t\t# Determine initial IL and store it in environment's state.\n",
        "\t\tinitial_state = np.random.choice(self.initial_states)\n",
        "\t\tself.state = initial_state\n",
        "\t\t\n",
        "\t\t# Set node's initial IL attribute. (This will force the simulation to start with\n",
        "\t\t# the node at this inventory level.)\n",
        "\t\tself.node.initial_inventory_level = initial_state\n",
        "\n",
        "\t\t# Reset the simulation environment.\n",
        "\t\tsim.initialize(self.network, self.episode_length)\n",
        "\n",
        "\t\treturn initial_state\n",
        "\n",
        "\tdef step(self, action):\n",
        "\t\t\"\"\"Run one time step of the environment by taking the specified action.\n",
        "\t\tUpdate the environment state to the new state. \n",
        "\t\tReturn a tuple (new_state, reward, done).\"\"\"\n",
        "\t\t# Build dict specifying order quantity to use in this time period.\n",
        "\t\t# (This will override the order quantities that the stockpyl simulation\n",
        "\t\t# would choose on its own.) Make sure the order quantity does not bring\n",
        "\t\t# the IL above its max value.\n",
        "\t\torder_quantity = min(action, self.max_state - self.state)\n",
        "\t\torder_quantity_override = {self.node: order_quantity}\n",
        "\n",
        "\t\t# Simulate one time period.\n",
        "\t\tsim.step(self.network, order_quantity_override=order_quantity_override)\n",
        "\n",
        "\t\t# Determine reward by querying the simulation's state variables.\n",
        "\t\treward = -self.node.state_vars_current.total_cost_incurred\n",
        "\n",
        "\t\t# If episode length has been reached, terminate.\n",
        "\t\tdone = self.network.period == self.episode_length - 1\n",
        "\n",
        "\t\t# Get new inventory level from simulation. \n",
        "\t\t# (Round to int -- should already be integer but sometimes there are small rounding errors.)\n",
        "\t\tIL = int(self.node.state_vars_current.inventory_position())\n",
        "\t\t# If new IL is outside the bounds of state space, truncate.\n",
        "\t\tIL = max(min(IL, self.max_state), self.min_state)\n",
        "\n",
        "\t\t# Update state.\n",
        "\t\tself.state = IL\n",
        "\n",
        "\t\t# Fill the demand into the info dict.\n",
        "\t\tinfo = {'demand': self.node.state_vars_current.inbound_order[None]}\n",
        "\n",
        "\t\treturn self.state, reward, done, info\n",
        "\n",
        "\tdef render(self):\n",
        "\t\t\"\"\"This function can contain code for drawing the environment to\n",
        "\t\ta graphics window, or printing it in ASCII format to the terminal.\n",
        "\t\tBut we'll just do something very simple and print the state.\n",
        "\t\t(Feel free to add some nicer visualization code here if you want!)\"\"\"\n",
        "\t\tprint(self.state)\n",
        "\n",
        "\tdef play_episode(self, policy, messages=False):\n",
        "\t\t\"\"\"Play one episode of the environment following the specified policy. \n",
        "\t\tReturn the total discounted reward over the episode.\n",
        "\n",
        "\t\t`policy` is a dict in which keys are states and values are actions.\n",
        "\t\tIf `messages` is True, will print state and action in each time step.\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t# Initialize environment.\n",
        "\t\tself.reset()\n",
        "\t\tcumul_reward = 0\n",
        "\n",
        "\t\tif messages:\n",
        "\t\t\tprint(f\"policy = {policy}\")\n",
        "\t\t\tprint(f\"Initial state = {self.state}, total reward = {cumul_reward}\")\n",
        "\n",
        "\t\t# Step through until terminal state reached.\n",
        "\t\tfor t in range(self.episode_length):\n",
        "\t\t\t\n",
        "\t\t\t# Determine action.\n",
        "\t\t\taction = policy[self.state]\n",
        "\n",
        "\t\t\tif messages:\n",
        "\t\t\t\tprint(f\"timestep {t:6} state = {self.state:4} action = {action:4} \", end=\"\")\n",
        "\n",
        "\t\t\t# Step.\n",
        "\t\t\tnew_state, reward, done, info = self.step(action)\n",
        "\n",
        "\t\t\t# Update cumulative reward.\n",
        "\t\t\tcumul_reward += reward\n",
        "\n",
        "\t\t\tif messages:\n",
        "\t\t\t\tprint(f\"demand = {info['demand']:4} new_state = {new_state:4} reward = {reward:8.2f} cumulative reward = {cumul_reward:8.2f}\")\n",
        "   \n",
        "\t\treturn cumul_reward"
      ],
      "metadata": {
        "id": "fyiDsHkocwa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's our policy-plotting function:"
      ],
      "metadata": {
        "id": "MRMDP5RO6Mar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy(env: MPNVEnv, policy: dict, title: str = None):\n",
        "\t\"\"\"Plot the policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tpi : \n",
        "\t\tProbability distribution for a policy. A dict whose keys are states and\n",
        "\t\twhose values are actions. (Note that this is a different structure\n",
        "\t\tthan what was used in the \"MPNV as MDP\" notebook.)\n",
        "\ttitle : \n",
        "\t\tOptional title for the figure.\n",
        "\t\"\"\"\n",
        "\n",
        "\tfig = plt.figure(figsize=plt.figaspect(1/2))\n",
        "\tfig.suptitle(title)\n",
        "\n",
        "\t# Order quantity plot.\n",
        "\tax = plt.subplot(121)\n",
        "\tx_list = env.observation_space_list\n",
        "\ty_list = [policy[x] for x in x_list]\n",
        "\tplt.plot(x_list, y_list)\n",
        "\tplt.xlabel('Starting Inventory Level')\n",
        "\tplt.ylabel('Order Quantity')\n",
        "\n",
        "\t# Order-up-to level plot.\n",
        "\tax = plt.subplot(122)\n",
        "\ty_list = [x + policy[x] for x in x_list]\n",
        "\tplt.plot(x_list, y_list)\n",
        "\tplt.xlabel('Starting Inventory Level')\n",
        "\tplt.ylabel('Order-Up-To Level')\n",
        "\n",
        "\tplt.show()\n"
      ],
      "metadata": {
        "id": "wy_uJzgqw80t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Instance\n",
        "\n",
        "Again we'll use the same MPNV instance:\n",
        "\n",
        "* $h = 1$\n",
        "* $p = 10$\n",
        "* $\\mu = 5$\n"
      ],
      "metadata": {
        "id": "v2WJKrrz6S8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build stockpyl SupplyChainNetwork object.\n",
        "network = single_stage_system(\n",
        "    local_holding_cost=1,\n",
        "    stockout_cost=10,\n",
        "    demand_type='P',\n",
        "    mean=5\n",
        ")"
      ],
      "metadata": {
        "id": "sJKUIo8TXiGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll use the same state space, $[-10,10]$. \n",
        "\n",
        "This time, we'll consider episodes of length 100 periods. Since we're working up to solving the beer game, we'll start to think of this inventory problem as a \"game\" with a fixed number of time periods, like the beer game has."
      ],
      "metadata": {
        "id": "SmIy2fO16b0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_state = -10\n",
        "max_state = 10\n",
        "episode_length = 100"
      ],
      "metadata": {
        "id": "nSUhScVD611O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's build our `MPNVEnv` environment.\n",
        "\n",
        "Remember: This is now a well-defined `gym` environment. It's possible to \"register\" a custom environment to take advantage of the full `gym` API, but we won't need to do that here."
      ],
      "metadata": {
        "id": "j87w-pJB67pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MPNVEnv object.\n",
        "env = MPNVEnv(\n",
        "    network=network,\n",
        "    episode_length=episode_length,\n",
        "    min_state=min_state,\n",
        "    max_state=max_state\n",
        ")"
      ],
      "metadata": {
        "id": "d39RKRwp6wa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give our new environment a quick spin. First, we'll create a base-stock policy with a base-stock level of 6. Then we'll ask our environment to play one episode of the MPNV \"game\". In each time period, it will print the starting state, the action (order quantity), the demand, the new state, and the reward.\n",
        "\n",
        "(I recommend that you think through a few periods to sanity-check the logic of the dynamics.)"
      ],
      "metadata": {
        "id": "cP2Y_QgE8jKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_stock_policy = {s: max(0, 6 - s) for s in env.observation_space_list}\n",
        "env.play_episode(base_stock_policy, messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASsC_ztI3sWP",
        "outputId": "a0194904-0f0a-41c7-bb41-5550f0bab703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy = {-10: 16, -9: 15, -8: 14, -7: 13, -6: 12, -5: 11, -4: 10, -3: 9, -2: 8, -1: 7, 0: 6, 1: 5, 2: 4, 3: 3, 4: 2, 5: 1, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0}\n",
            "Initial state = 10, total reward = 0\n",
            "timestep      0 state =   10 action =    0 demand =    5 new_state =    5 reward =    -5.00 total reward =    -5.00\n",
            "timestep      1 state =    5 action =    1 demand =    7 new_state =   -1 reward =   -10.00 total reward =   -15.00\n",
            "timestep      2 state =   -1 action =    7 demand =    6 new_state =    0 reward =    -0.00 total reward =   -15.00\n",
            "timestep      3 state =    0 action =    6 demand =    2 new_state =    4 reward =    -4.00 total reward =   -19.00\n",
            "timestep      4 state =    4 action =    2 demand =    6 new_state =    0 reward =    -0.00 total reward =   -19.00\n",
            "timestep      5 state =    0 action =    6 demand =    5 new_state =    1 reward =    -1.00 total reward =   -20.00\n",
            "timestep      6 state =    1 action =    5 demand =    6 new_state =    0 reward =    -0.00 total reward =   -20.00\n",
            "timestep      7 state =    0 action =    6 demand =    4 new_state =    2 reward =    -2.00 total reward =   -22.00\n",
            "timestep      8 state =    2 action =    4 demand =    8 new_state =   -2 reward =   -20.00 total reward =   -42.00\n",
            "timestep      9 state =   -2 action =    8 demand =    9 new_state =   -3 reward =   -30.00 total reward =   -72.00\n",
            "timestep     10 state =   -3 action =    9 demand =    6 new_state =    0 reward =    -0.00 total reward =   -72.00\n",
            "timestep     11 state =    0 action =    6 demand =    5 new_state =    1 reward =    -1.00 total reward =   -73.00\n",
            "timestep     12 state =    1 action =    5 demand =    7 new_state =   -1 reward =   -10.00 total reward =   -83.00\n",
            "timestep     13 state =   -1 action =    7 demand =    2 new_state =    4 reward =    -4.00 total reward =   -87.00\n",
            "timestep     14 state =    4 action =    2 demand =    6 new_state =    0 reward =    -0.00 total reward =   -87.00\n",
            "timestep     15 state =    0 action =    6 demand =    8 new_state =   -2 reward =   -20.00 total reward =  -107.00\n",
            "timestep     16 state =   -2 action =    8 demand =    8 new_state =   -2 reward =   -20.00 total reward =  -127.00\n",
            "timestep     17 state =   -2 action =    8 demand =    2 new_state =    4 reward =    -4.00 total reward =  -131.00\n",
            "timestep     18 state =    4 action =    2 demand =    4 new_state =    2 reward =    -2.00 total reward =  -133.00\n",
            "timestep     19 state =    2 action =    4 demand =    5 new_state =    1 reward =    -1.00 total reward =  -134.00\n",
            "timestep     20 state =    1 action =    5 demand =    5 new_state =    1 reward =    -1.00 total reward =  -135.00\n",
            "timestep     21 state =    1 action =    5 demand =    3 new_state =    3 reward =    -3.00 total reward =  -138.00\n",
            "timestep     22 state =    3 action =    3 demand =    4 new_state =    2 reward =    -2.00 total reward =  -140.00\n",
            "timestep     23 state =    2 action =    4 demand =    4 new_state =    2 reward =    -2.00 total reward =  -142.00\n",
            "timestep     24 state =    2 action =    4 demand =   10 new_state =   -4 reward =   -40.00 total reward =  -182.00\n",
            "timestep     25 state =   -4 action =   10 demand =    2 new_state =    4 reward =    -4.00 total reward =  -186.00\n",
            "timestep     26 state =    4 action =    2 demand =    9 new_state =   -3 reward =   -30.00 total reward =  -216.00\n",
            "timestep     27 state =   -3 action =    9 demand =    6 new_state =    0 reward =    -0.00 total reward =  -216.00\n",
            "timestep     28 state =    0 action =    6 demand =    2 new_state =    4 reward =    -4.00 total reward =  -220.00\n",
            "timestep     29 state =    4 action =    2 demand =    2 new_state =    4 reward =    -4.00 total reward =  -224.00\n",
            "timestep     30 state =    4 action =    2 demand =    6 new_state =    0 reward =    -0.00 total reward =  -224.00\n",
            "timestep     31 state =    0 action =    6 demand =    6 new_state =    0 reward =    -0.00 total reward =  -224.00\n",
            "timestep     32 state =    0 action =    6 demand =    7 new_state =   -1 reward =   -10.00 total reward =  -234.00\n",
            "timestep     33 state =   -1 action =    7 demand =    5 new_state =    1 reward =    -1.00 total reward =  -235.00\n",
            "timestep     34 state =    1 action =    5 demand =    5 new_state =    1 reward =    -1.00 total reward =  -236.00\n",
            "timestep     35 state =    1 action =    5 demand =    3 new_state =    3 reward =    -3.00 total reward =  -239.00\n",
            "timestep     36 state =    3 action =    3 demand =    1 new_state =    5 reward =    -5.00 total reward =  -244.00\n",
            "timestep     37 state =    5 action =    1 demand =    6 new_state =    0 reward =    -0.00 total reward =  -244.00\n",
            "timestep     38 state =    0 action =    6 demand =    3 new_state =    3 reward =    -3.00 total reward =  -247.00\n",
            "timestep     39 state =    3 action =    3 demand =    1 new_state =    5 reward =    -5.00 total reward =  -252.00\n",
            "timestep     40 state =    5 action =    1 demand =    3 new_state =    3 reward =    -3.00 total reward =  -255.00\n",
            "timestep     41 state =    3 action =    3 demand =    8 new_state =   -2 reward =   -20.00 total reward =  -275.00\n",
            "timestep     42 state =   -2 action =    8 demand =    4 new_state =    2 reward =    -2.00 total reward =  -277.00\n",
            "timestep     43 state =    2 action =    4 demand =    3 new_state =    3 reward =    -3.00 total reward =  -280.00\n",
            "timestep     44 state =    3 action =    3 demand =    7 new_state =   -1 reward =   -10.00 total reward =  -290.00\n",
            "timestep     45 state =   -1 action =    7 demand =    6 new_state =    0 reward =    -0.00 total reward =  -290.00\n",
            "timestep     46 state =    0 action =    6 demand =    4 new_state =    2 reward =    -2.00 total reward =  -292.00\n",
            "timestep     47 state =    2 action =    4 demand =    4 new_state =    2 reward =    -2.00 total reward =  -294.00\n",
            "timestep     48 state =    2 action =    4 demand =    5 new_state =    1 reward =    -1.00 total reward =  -295.00\n",
            "timestep     49 state =    1 action =    5 demand =    5 new_state =    1 reward =    -1.00 total reward =  -296.00\n",
            "timestep     50 state =    1 action =    5 demand =    4 new_state =    2 reward =    -2.00 total reward =  -298.00\n",
            "timestep     51 state =    2 action =    4 demand =    2 new_state =    4 reward =    -4.00 total reward =  -302.00\n",
            "timestep     52 state =    4 action =    2 demand =    4 new_state =    2 reward =    -2.00 total reward =  -304.00\n",
            "timestep     53 state =    2 action =    4 demand =    6 new_state =    0 reward =    -0.00 total reward =  -304.00\n",
            "timestep     54 state =    0 action =    6 demand =    6 new_state =    0 reward =    -0.00 total reward =  -304.00\n",
            "timestep     55 state =    0 action =    6 demand =    6 new_state =    0 reward =    -0.00 total reward =  -304.00\n",
            "timestep     56 state =    0 action =    6 demand =    5 new_state =    1 reward =    -1.00 total reward =  -305.00\n",
            "timestep     57 state =    1 action =    5 demand =    4 new_state =    2 reward =    -2.00 total reward =  -307.00\n",
            "timestep     58 state =    2 action =    4 demand =    5 new_state =    1 reward =    -1.00 total reward =  -308.00\n",
            "timestep     59 state =    1 action =    5 demand =    9 new_state =   -3 reward =   -30.00 total reward =  -338.00\n",
            "timestep     60 state =   -3 action =    9 demand =    3 new_state =    3 reward =    -3.00 total reward =  -341.00\n",
            "timestep     61 state =    3 action =    3 demand =    6 new_state =    0 reward =    -0.00 total reward =  -341.00\n",
            "timestep     62 state =    0 action =    6 demand =    4 new_state =    2 reward =    -2.00 total reward =  -343.00\n",
            "timestep     63 state =    2 action =    4 demand =    6 new_state =    0 reward =    -0.00 total reward =  -343.00\n",
            "timestep     64 state =    0 action =    6 demand =    6 new_state =    0 reward =    -0.00 total reward =  -343.00\n",
            "timestep     65 state =    0 action =    6 demand =    2 new_state =    4 reward =    -4.00 total reward =  -347.00\n",
            "timestep     66 state =    4 action =    2 demand =    1 new_state =    5 reward =    -5.00 total reward =  -352.00\n",
            "timestep     67 state =    5 action =    1 demand =    3 new_state =    3 reward =    -3.00 total reward =  -355.00\n",
            "timestep     68 state =    3 action =    3 demand =    4 new_state =    2 reward =    -2.00 total reward =  -357.00\n",
            "timestep     69 state =    2 action =    4 demand =    1 new_state =    5 reward =    -5.00 total reward =  -362.00\n",
            "timestep     70 state =    5 action =    1 demand =    3 new_state =    3 reward =    -3.00 total reward =  -365.00\n",
            "timestep     71 state =    3 action =    3 demand =   10 new_state =   -4 reward =   -40.00 total reward =  -405.00\n",
            "timestep     72 state =   -4 action =   10 demand =    4 new_state =    2 reward =    -2.00 total reward =  -407.00\n",
            "timestep     73 state =    2 action =    4 demand =    3 new_state =    3 reward =    -3.00 total reward =  -410.00\n",
            "timestep     74 state =    3 action =    3 demand =    2 new_state =    4 reward =    -4.00 total reward =  -414.00\n",
            "timestep     75 state =    4 action =    2 demand =    7 new_state =   -1 reward =   -10.00 total reward =  -424.00\n",
            "timestep     76 state =   -1 action =    7 demand =    8 new_state =   -2 reward =   -20.00 total reward =  -444.00\n",
            "timestep     77 state =   -2 action =    8 demand =    5 new_state =    1 reward =    -1.00 total reward =  -445.00\n",
            "timestep     78 state =    1 action =    5 demand =    6 new_state =    0 reward =    -0.00 total reward =  -445.00\n",
            "timestep     79 state =    0 action =    6 demand =    6 new_state =    0 reward =    -0.00 total reward =  -445.00\n",
            "timestep     80 state =    0 action =    6 demand =    3 new_state =    3 reward =    -3.00 total reward =  -448.00\n",
            "timestep     81 state =    3 action =    3 demand =    3 new_state =    3 reward =    -3.00 total reward =  -451.00\n",
            "timestep     82 state =    3 action =    3 demand =    3 new_state =    3 reward =    -3.00 total reward =  -454.00\n",
            "timestep     83 state =    3 action =    3 demand =    3 new_state =    3 reward =    -3.00 total reward =  -457.00\n",
            "timestep     84 state =    3 action =    3 demand =    6 new_state =    0 reward =    -0.00 total reward =  -457.00\n",
            "timestep     85 state =    0 action =    6 demand =    4 new_state =    2 reward =    -2.00 total reward =  -459.00\n",
            "timestep     86 state =    2 action =    4 demand =    5 new_state =    1 reward =    -1.00 total reward =  -460.00\n",
            "timestep     87 state =    1 action =    5 demand =    2 new_state =    4 reward =    -4.00 total reward =  -464.00\n",
            "timestep     88 state =    4 action =    2 demand =    3 new_state =    3 reward =    -3.00 total reward =  -467.00\n",
            "timestep     89 state =    3 action =    3 demand =    1 new_state =    5 reward =    -5.00 total reward =  -472.00\n",
            "timestep     90 state =    5 action =    1 demand =    5 new_state =    1 reward =    -1.00 total reward =  -473.00\n",
            "timestep     91 state =    1 action =    5 demand =    4 new_state =    2 reward =    -2.00 total reward =  -475.00\n",
            "timestep     92 state =    2 action =    4 demand =    3 new_state =    3 reward =    -3.00 total reward =  -478.00\n",
            "timestep     93 state =    3 action =    3 demand =    5 new_state =    1 reward =    -1.00 total reward =  -479.00\n",
            "timestep     94 state =    1 action =    5 demand =    3 new_state =    3 reward =    -3.00 total reward =  -482.00\n",
            "timestep     95 state =    3 action =    3 demand =    5 new_state =    1 reward =    -1.00 total reward =  -483.00\n",
            "timestep     96 state =    1 action =    5 demand =    7 new_state =   -1 reward =   -10.00 total reward =  -493.00\n",
            "timestep     97 state =   -1 action =    7 demand =    6 new_state =    0 reward =    -0.00 total reward =  -493.00\n",
            "timestep     98 state =    0 action =    6 demand =    6 new_state =    0 reward =    -0.00 total reward =  -493.00\n",
            "timestep     99 state =    0 action =    6 demand =    4 new_state =    2 reward =    -2.00 total reward =  -495.00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-495.0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up TensorFlow\n",
        "\n",
        "Next we'll set up our model in TensorFlow. First, some imports:"
      ],
      "metadata": {
        "id": "_HPX1ppg-TmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wSYk6yFwSZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy \n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "IcG-gFxhSqiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then a helper function to build the TF **model:**"
      ],
      "metadata": {
        "id": "RFO0_ZuC-sTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()    \n",
        "    model.add(Dense(24, activation='relu', input_shape=(1,))) \n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "5BNytuonShes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll build the model itself:"
      ],
      "metadata": {
        "id": "Eos3SOdM_KWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shortcut to size of observation and action spaces.\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "# Build the model.\n",
        "# NOTE: This must happen *after* the `from rl.x` imports.\n",
        "# (See https://stackoverflow.com/a/72438856/3453768)\n",
        "model = build_model(num_states, num_actions)"
      ],
      "metadata": {
        "id": "B4MEcBFtSndQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a summary of the model:"
      ],
      "metadata": {
        "id": "ZNol9lDX_cfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fizqVg6MSpQs",
        "outputId": "887956e7-d365-4c2b-a783-a2fc7c625435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 24)                48        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 11)                275       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 923\n",
            "Trainable params: 923\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need an RL **agent.** We'll use the `DQNAgent` class built into `keras` (part of TensorFlow). \n",
        "\n",
        "Our agent also needs a **policy.** We'll use the `EpsGreedyQPolicy`, again built into `keras`. (Feel free to play around with different policies. You'll have to `import` them like we did for `EpsGreedyQPolicy` above. I haven't been able to find good documentation for these policies, but you can find different policies to try by looking at the [source code](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).)"
      ],
      "metadata": {
        "id": "j3ziPVMQ_jI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = EpsGreedyQPolicy(eps=0.1) \n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "-1CEDkM4SyOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, build the DQN agent, store it in a variable called `dqn`, and \"compile\" it (a preprocessing step)."
      ],
      "metadata": {
        "id": "gmu1d7Z8BOoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, num_actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJASpBtRS1RF",
        "outputId": "33b869a1-c510-4a60-8ae4-7ce0666b766f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DQN Agent\n",
        "\n",
        "Now we're finally ready for the main step: training the DQN agent. The command below trains it for 60,000 episodes, which should take about 10 minutes and produce medium-good results. Feel free to change this number to do more or less training."
      ],
      "metadata": {
        "id": "LzS1LLIYBgin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y51byZphBblU",
        "outputId": "5df81cf4-07be-4fc7-e17e-fbab211ae3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 60000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "\r    1/10000 [..............................] - ETA: 12:37 - reward: -5.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 77s 8ms/step - reward: -10.9440\n",
            "100 episodes - episode_reward: -1094.400 [-44880.000, -450.000] - loss: 1438.496 - mae: 127.021 - mean_q: -112.029 - demand: 5.025\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 79s 8ms/step - reward: -5.7436\n",
            "100 episodes - episode_reward: -574.360 [-1029.000, -423.000] - loss: 371.234 - mae: 168.440 - mean_q: -169.325 - demand: 4.979\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 80s 8ms/step - reward: -6.0114\n",
            "100 episodes - episode_reward: -601.140 [-1000.000, -429.000] - loss: 323.917 - mae: 181.233 - mean_q: -184.709 - demand: 4.964\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 80s 8ms/step - reward: -5.8537\n",
            "100 episodes - episode_reward: -585.370 [-1414.000, -437.000] - loss: 303.344 - mae: 185.871 - mean_q: -190.633 - demand: 5.002\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 86s 9ms/step - reward: -5.7997\n",
            "100 episodes - episode_reward: -579.970 [-1093.000, -421.000] - loss: 283.152 - mae: 186.174 - mean_q: -191.886 - demand: 5.023\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 81s 8ms/step - reward: -5.9079\n",
            "done, took 483.038 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cde6f6e10>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most likely, you'll see the `episode_reward` get gradually better as the training progresses (though not necessarily monotonically so)."
      ],
      "metadata": {
        "id": "z6xdFPo6DlvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Results\n",
        "\n",
        "The DQN agent has a feature to test the learned policy by playing multiple episodes and print the results. Let's play 50 of them."
      ],
      "metadata": {
        "id": "aMOS9JTBCrvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = dqn.test(env, nb_episodes=50, visualize=False)\n",
        "print(f\"Average reward per episode = {np.mean(results.history['episode_reward'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0INhM5lxS-ii",
        "outputId": "73286fda-7480-406d-f7c7-e52dae69cd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: -520.000, steps: 100\n",
            "Episode 2: reward: -546.000, steps: 100\n",
            "Episode 3: reward: -547.000, steps: 100\n",
            "Episode 4: reward: -488.000, steps: 100\n",
            "Episode 5: reward: -582.000, steps: 100\n",
            "Episode 6: reward: -489.000, steps: 100\n",
            "Episode 7: reward: -514.000, steps: 100\n",
            "Episode 8: reward: -540.000, steps: 100\n",
            "Episode 9: reward: -621.000, steps: 100\n",
            "Episode 10: reward: -529.000, steps: 100\n",
            "Episode 11: reward: -512.000, steps: 100\n",
            "Episode 12: reward: -531.000, steps: 100\n",
            "Episode 13: reward: -550.000, steps: 100\n",
            "Episode 14: reward: -496.000, steps: 100\n",
            "Episode 15: reward: -545.000, steps: 100\n",
            "Episode 16: reward: -528.000, steps: 100\n",
            "Episode 17: reward: -508.000, steps: 100\n",
            "Episode 18: reward: -585.000, steps: 100\n",
            "Episode 19: reward: -486.000, steps: 100\n",
            "Episode 20: reward: -515.000, steps: 100\n",
            "Episode 21: reward: -581.000, steps: 100\n",
            "Episode 22: reward: -508.000, steps: 100\n",
            "Episode 23: reward: -495.000, steps: 100\n",
            "Episode 24: reward: -523.000, steps: 100\n",
            "Episode 25: reward: -509.000, steps: 100\n",
            "Episode 26: reward: -555.000, steps: 100\n",
            "Episode 27: reward: -558.000, steps: 100\n",
            "Episode 28: reward: -566.000, steps: 100\n",
            "Episode 29: reward: -554.000, steps: 100\n",
            "Episode 30: reward: -541.000, steps: 100\n",
            "Episode 31: reward: -554.000, steps: 100\n",
            "Episode 32: reward: -540.000, steps: 100\n",
            "Episode 33: reward: -505.000, steps: 100\n",
            "Episode 34: reward: -475.000, steps: 100\n",
            "Episode 35: reward: -476.000, steps: 100\n",
            "Episode 36: reward: -529.000, steps: 100\n",
            "Episode 37: reward: -553.000, steps: 100\n",
            "Episode 38: reward: -514.000, steps: 100\n",
            "Episode 39: reward: -520.000, steps: 100\n",
            "Episode 40: reward: -518.000, steps: 100\n",
            "Episode 41: reward: -478.000, steps: 100\n",
            "Episode 42: reward: -536.000, steps: 100\n",
            "Episode 43: reward: -533.000, steps: 100\n",
            "Episode 44: reward: -557.000, steps: 100\n",
            "Episode 45: reward: -494.000, steps: 100\n",
            "Episode 46: reward: -575.000, steps: 100\n",
            "Episode 47: reward: -502.000, steps: 100\n",
            "Episode 48: reward: -541.000, steps: 100\n",
            "Episode 49: reward: -465.000, steps: 100\n",
            "Episode 50: reward: -532.000, steps: 100\n",
            "Average reward per episode = -528.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My results showed an average reward per episode of $-528$. Recall that this problem can be solved analytically; the optimal policy is a base-stock policy with base-stock level 8 and expected cost $4.34$ per period. So a 100-period episode has an optimal reward of $-434$. My agent's reward of $-528$ is so-so. Of course we could improve it by more training, different hyperparameters, etc."
      ],
      "metadata": {
        "id": "P2huQdP1EU-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a closer look at the policy that the DQN agent learned. The cells below parse the DQN's policy, print it as a dict, and then plot it."
      ],
      "metadata": {
        "id": "qv3tfG_JHmS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the learned policy.\n",
        "dqn_policy = {s: np.argmax(dqn.compute_q_values(s)) for s in env.observation_space_list}\n",
        "dqn_policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E37MsswDqkyd",
        "outputId": "4e894657-6a24-4fba-f229-de94db250844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{-10: 8,\n",
              " -9: 8,\n",
              " -8: 8,\n",
              " -7: 8,\n",
              " -6: 9,\n",
              " -5: 9,\n",
              " -4: 9,\n",
              " -3: 9,\n",
              " -2: 9,\n",
              " -1: 9,\n",
              " 0: 7,\n",
              " 1: 7,\n",
              " 2: 8,\n",
              " 3: 8,\n",
              " 4: 8,\n",
              " 5: 8,\n",
              " 6: 8,\n",
              " 7: 8,\n",
              " 8: 2,\n",
              " 9: 2,\n",
              " 10: 2}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_policy(env, dqn_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "y1Lp4S0-x8Jj",
        "outputId": "a5eade78-6c9e-4eb3-d7c0-f794104f9a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is nothing like a base-stock policy! It's possible for a non-optimal policy still to have good performance. This policy is only so-so; with better training, it remains to be seen how closely the learned policy will approximate a base-stock policy. \n",
        "\n",
        "(It's worth remembering that TD learning approximated the policy more closely. DQN is much more powerful but also requires much more extensive training.)"
      ],
      "metadata": {
        "id": "A6g3T8E_Ic8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Up\n",
        "\n"
      ],
      "metadata": {
        "id": "HhHYPU8cKyIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gF7qQ-HXKzt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}