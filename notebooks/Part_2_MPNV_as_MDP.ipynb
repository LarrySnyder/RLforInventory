{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 2: MPNV as MDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQE0yx4hF5jfuSjjgeF9VC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/Part_2_MPNV_as_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Multi-Period Newsvendor Problem (MPNV) as an MDP\n",
        "\n",
        "This notebook contains code for an MDP implementation of the multi-period newsvendor (MPNV) problem. \n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filenameâ€”not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University.\n",
        "---"
      ],
      "metadata": {
        "id": "mQmitR3h8t3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **multi-period newsvendor (MPNV)** problem, we must decide, in each time period, how much to order in advance of observing a random demand. Items that we order are received immediately (there is no lead time). Unmet demands are *backordered* (those customers are willing to wait until the next time period, with a penalty), and leftover items in inventory may be carried over to the next period (again, with a penalty).\n",
        "\n",
        "The *inventory level* is the number of on-hand items minus the number of backorders:\n",
        "\n",
        "$$IL = OH - BO.$$\n",
        "\n",
        "Note that $IL$ can be positive, negative, or zero. The number of on-hand items and the number of backorders can never both be positive at the same time. Therefore, the number of on-hand items is $OH = IL^+$ and the number of backorders is $BO = (-IL)^+$, where $z^+ \\equiv \\max\\{0, z\\}$. \n",
        "\n",
        "Suppose we begin the time period with an inventory level (IL) of $x$ and order $Q$ units. We call $y \\equiv x + Q$ the *order-up-to level*. If $y$ is greater than the demand $d$ in a given period (that is, we have more items than we need), we incur a *holding cost* of $h$ per item. On the other hand, if $y$ is less than the demand (we have fewer items than we need), we incur a *stockout cost* of $p$ per item. That is, the cost incurred in the time period is\n",
        "\n",
        "$$h(y-d)^+ + p(d-y)^+ = h(x+Q-d)^+ + p(d-(x+Q))^+.$$\n",
        "\n",
        "Of course, the demand is stochastic, and we must choose $Q$ (or, equivalently, $y$) before we observe it. \n",
        "\n",
        "We'll assume an **infinite horizon** (though finite-horizon versions of the problem exist, too; see Snyder and Shen, 2nd edition, 2019). Two common objective functions are used, one in which we minimize the expected cost per period and one in which we minimize the total discounted cost over the time horizon. \n",
        "\n",
        "We'll use the discounted version, meaning we are trying to minimize\n",
        "\n",
        "$$\\sum_{t=1}^\\infty \\gamma^{t-1}g_t(Q_t),$$\n",
        "\n",
        "where $g_t(Q_t)$ is the expected cost in period $t$ if we use order quantity $Q_t$.\n",
        "\n"
      ],
      "metadata": {
        "id": "vYxyDehK89q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem can be formulated as a **Markov decision process (MDP)**. In this MDP, the state represents the IL at the beginning of the time period, and the action represents the order quantity. \n",
        "\n",
        "Now we'll start using more standard MDP notation:\n",
        "\n",
        "* State = $s$ = inventory level (instead of $x$)\n",
        "* Action = $a$ = order quantity (instead of $Q$)\n",
        "\n",
        "Let $v_\\pi(s)$ denote the **value function**: the expected return when starting in state $s$ (that is, starting a period with an IL of $s$) and folowing the **policy** $\\pi$ thereafter. As usual, the policy $\\pi$ is a mapping from states to actions. We'll assume it is a *deterministic policy* such that $\\pi(s)$ is the action the policy specifies in state $s$.\n",
        "\n",
        "The **Bellman equation** for the value function $v_\\pi$ is:\n",
        "\n",
        "$$v_\\pi(s) = {\\mathbb E}_D\\left[ -\\left(h(s + a -d)^+ + p(d - (s+a))^+\\right) + \\gamma v_\\pi(s+a-D)\\right],$$\n",
        "\n",
        "where ${\\mathbb E}_D$ denotes expectation over the random demand. For the optimal policy, the **Bellman optimality equation** is:\n",
        "\n",
        "$$v_*(s) = \\max_{a\\ge 0} {\\mathbb E}_D\\left[ -\\left(h(s+a-d)^+ + p(d-(s+a))^+\\right) + \\gamma v_*(s+a-D)\\right].$$\n",
        "\n",
        "The maximization is taken over all order quantities $a$, and since we cannot place an order of negative size, we require $a\\ge 0$.\n",
        "\n"
      ],
      "metadata": {
        "id": "TmcgRVt0MoKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we solve this MDP, we will find that the optimal policy follows a very particular structure. In particular, the optimal actions are given by:\n",
        "\n",
        "$$\\pi_*(s) = \\begin{cases} S - s, & s < S \\\\ 0 & s \\ge S \\end{cases}$$\n",
        "\n",
        "for some number $S$. Converting back to our inventory-theoretic notation, the optimal order quantity is given by\n",
        "\n",
        "$$Q = \\begin{cases} S - x, & x < S \\\\ 0 & x \\ge S, \\end{cases}$$\n",
        "\n",
        "where $x$ is the starting inventory level. This type of ordering policy is called a **base-stock policy,** and the quantity $S$ is called the **base-stock level.**\n",
        "\n",
        "---\n",
        "> **Note:** The term *policy* is used somewhat differently in inventory theory than in MDP/RL theory. In inventory theory, a policy is a simple ordering rule, whereas in MDP/RL, it is a much more general mapping of states to actions. In other words, an inventory policy is a special case of an MDP/RL policy in which the optimal actions follow some simple or convenient structure.\n",
        ">\n",
        "> Also: Note that $S$, above, stands for the base-stock level, not for a \"state\" (as it often does in RL).\n",
        "---\n",
        "\n",
        "Once we know that a base-stock policy is optimal, the optimal base-stock level has a simple form: $S^*$ is the smallest $S$ such that\n",
        "\n",
        "$$F(S) \\ge \\frac{p}{h+p}.$$\n",
        "\n",
        "---\n",
        "> **Note:** You might have been expecting the optimal base-stock level to depend on the discount factor, $\\gamma$. It turns out that $S^*$ only depends on $\\gamma$ if we also include a purchase cost $c$ per unit ordered from the supplier. Here, we assume $c=0$, so $S^*$ is independent of $\\gamma$.\n",
        "---\n",
        "\n",
        "The fraction on the right-hand side is known as the *newsvendor fractile* or the *critical fractile*. The expected cost function and the optimal solution are mathematically identical to those for the (single-period) **newsvendor problem,** even though that problem assumes that unmet demands are lost (rather than backordered) and that extra inventory units are scrapped (rather than held).\n",
        "\n",
        "(The results are similar if the demand has a continuous rather than discrete distribution, but we will only consider discrete distributions here.)"
      ],
      "metadata": {
        "id": "ocugkmFoQLF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will formulate and solve this problem as an MDP. This is possible since the dynamics of the system are fully known. We'll confirm, by solving the MDP numerically, that the optimal actions follow a base-stock policy. In the next section, we'll pretend that we don't know the dynamics and instead attack the problem using RL."
      ],
      "metadata": {
        "id": "hga80um_P5Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Python Stuff\n"
      ],
      "metadata": {
        "id": "XMUCsMuHnN2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytg9sReD8fRd"
      },
      "outputs": [],
      "source": [
        "# Import the packages we will need.\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import poisson\n",
        "from tqdm.notebook import tqdm\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, we'll use the `stockpyl` Python package (https://pypi.org/project/stockpyl/) for inventory optimization stuff. We have to install the `stockpyl` package ourselves. (It doesn't come pre-installed on Colab like `numpy`, etc. do.) You should only need to do this once per notebook.\n",
        "\n",
        "If you get a message like\n",
        "\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [sphinxcontrib]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```\n",
        "\n",
        "you can ignore it.\n"
      ],
      "metadata": {
        "id": "mvYa1KQUnuVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "id": "0-KOixlOny1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDP Class\n",
        "\n",
        "The code below defines a Python class that implements algorithms for generic Markov decision processes (MDPs). The algorithm implementations are based on the discussions in Sutton and Barto (2nd edition, 2018).\n",
        "\n",
        "The main input to the MDP is the `dynamics` dictionary, which represents the function $p(s',r|s,a)$. In particular, `dynamics[s,a][s_prime,r]` is the probability that state `s_prime` occurs next and earns reward `r` given that we are in state `s` and take action `a`. \n",
        "\n",
        "Note that `dynamics` is a dict of dicts: Its keys are (state, action) pairs, and its values are dicts whose keys are (next state, reward) pairs and whose values are probabilities.\n",
        "\n",
        "Feel free to explore the code if you want, but all that's required is for you to execute the cell."
      ],
      "metadata": {
        "id": "29VrdwOu9RXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP(object):\n",
        "    \"\"\"Generic class for an MDP.\n",
        "\n",
        "    ``dynamics`` is a dict whose keys are (state, action) pairs and whose values are dicts\n",
        "    whose keys are (next_state, reward) pairs and whose values are probabilities.\n",
        "    That is, dynamics[(s, a)][(s_prime, r)] is the probability that state s_prime and reward r\n",
        "    results from taking action a in state s.\n",
        "    Only (next_state, reward) pairs that have nonzero probability need to be included.\n",
        "    \"\"\"\n",
        "    def __init__(self, dynamics, initial_state, terminal_states, gamma=0.9):\n",
        "        # Parameters.\n",
        "        self.dynamics = dynamics\n",
        "        self.initial_state = initial_state\n",
        "        self.terminal_states = terminal_states\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Build list of states.\n",
        "        self.state_list = list({s for s, _ in self.dynamics.keys()})\n",
        "        # Add terminal states. (These won't be in dynamics.)\n",
        "        for s in terminal_states:\n",
        "            if s not in self.state_list:\n",
        "                self.state_list.append(s)\n",
        " \n",
        "         # Build dict of allowable actions.\n",
        "        self.allowable_actions = {}\n",
        "        for s in self.state_list:\n",
        "            self.allowable_actions[s] = list({a for state, a in self.dynamics if state == s})\n",
        "\n",
        "    @property\n",
        "    def nonterminal_state_list(self):\n",
        "        return list(set(self.state_list).difference(self.terminal_states))\n",
        "\n",
        "    def get_next_state_and_reward(self, state, action):\n",
        "        \"\"\"Return a next state and reward for a given (state, action) pair, \n",
        "        drawn from the pair's dynamics.\n",
        "        \"\"\"\n",
        "        # Build lists of outcomes and probabilities.\n",
        "        outcomes = list(self.dynamics[state, action].keys())\n",
        "        probabilities = list(self.dynamics[state, action].values())\n",
        "\n",
        "        # Draw next state and reward from the distribution.\n",
        "        i = np.random.choice(range(len(outcomes)), p=probabilities)\n",
        "        return outcomes[i]\n",
        "\n",
        "    def get_action_from_policy(self, state, policy):\n",
        "        \"\"\"For a given state, return an action sampled from the specified policy.\"\"\"\n",
        "        actions = list(policy[state].keys())\n",
        "        probabilities = list(policy[state].values())\n",
        "\n",
        "        i = np.random.choice(range(len(actions)), p=probabilities)\n",
        "\n",
        "        return actions[i]\n",
        "\n",
        "    def get_action_equiprobable(self, state):\n",
        "        \"\"\"For a given state, return an action sampled from an equiprobable policy.\"\"\"\n",
        "        i = np.random.randint(len(self.allowable_actions[state]))\n",
        "        return self.allowable_actions[state][i]\n",
        "\n",
        "    def iterative_policy_evaluation(self, policy, initial_values=None, \n",
        "                                 theta=0.001, max_iter=None):\n",
        "        \"\"\"Iterative policy evaluation algorithm for estimating V \\approx v_pi.\n",
        "        (Section 4.1, p. 75.)\n",
        "        \n",
        "        ``policy`` is a dict whose keys are states and whose values are dicts in \n",
        "        which keys are actions and values are probabilities. That is, policy[s][a] \n",
        "        is the probability of action a in state s.\n",
        "        \n",
        "        NOTE: If there is a state from which no terminal state is reachable -- \n",
        "        e.g., the policy goes left from (2, 2) and right from (1, 2) -- this \n",
        "        algorithm will not converge. The method does not check for this.\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize value function estimate.\n",
        "        V = {}\n",
        "        for s in self.state_list:\n",
        "            if (s not in self.terminal_states) and (initial_values is not None) \\\n",
        "        and (s in initial_values):\n",
        "                V[s] = initial_values[s]\n",
        "            else:\n",
        "                V[s] = 0\n",
        "        k = 0\n",
        "\n",
        "        # Loop.\n",
        "        while True:\n",
        "            Delta = 0\n",
        "            k += 1\n",
        "            # Make static copy of V. (Although Sutton and Barto's pseudocode doesn't \n",
        "              # indicate this, their results suggest that they are using the same V on \n",
        "              # the RHS throughout the entire iteration, even though the V[s] values \n",
        "              # will change throughout the iteration.)\n",
        "            V_copy = copy.deepcopy(V)\n",
        "            # Loop through nonterminal states.\n",
        "            for s in self.nonterminal_state_list:\n",
        "                v = V[s]\n",
        "                # Update V[s].\n",
        "                V[s] = 0\n",
        "                for a in self.allowable_actions[s]:\n",
        "                    if a in policy[s]:\n",
        "                        pi = policy[s][a]\n",
        "                        for s_prime, r in self.dynamics[(s, a)].keys():\n",
        "                            p = self.dynamics[(s, a)][(s_prime, r)]\n",
        "                            V[s] += pi * p * (r + self.gamma * V_copy[s_prime])\n",
        "\n",
        "                # Update Delta.\n",
        "                Delta = max(Delta, abs(v - V[s]))\n",
        "\n",
        "            # Terminate?\n",
        "            if Delta < theta or (max_iter is not None and k >= max_iter):\n",
        "                break\n",
        "\n",
        "        return V\n",
        "\n",
        "    def greedy_policy(self, value_function, deterministic=False):\n",
        "        \"\"\"Return greedy policy from given value function as described in \n",
        "        Section 4.2, p.79.\n",
        "        \n",
        "        If a given state has multiple maximizing actions, then:\n",
        "        * If `deterministic` is True, the action that comes earliest in the state list is chosen.\n",
        "        * If `deterministic` is False, the they are assigned equal probability in the policy.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize policy.\n",
        "        policy = {s: {} for s in self.state_list}\n",
        "\n",
        "        # Loop through states.\n",
        "        for s in self.state_list:\n",
        "\n",
        "            # Determine maximizing action.\n",
        "            best_value = -np.inf\n",
        "            for a in self.allowable_actions[s]:\n",
        "                value = 0\n",
        "                for s_prime, r in self.dynamics[(s, a)].keys():\n",
        "                    p = self.dynamics[(s, a)][(s_prime, r)]\n",
        "                    value += p * (r + self.gamma * value_function[s_prime])\n",
        "                if value == best_value and not deterministic:\n",
        "                    best_a.append(a)\n",
        "                elif value > best_value:\n",
        "                    best_value = value\n",
        "                    best_a = [a]\n",
        "\n",
        "            # Build policy.\n",
        "            for a in self.allowable_actions[s]:\n",
        "                if a in best_a:\n",
        "                    policy[s][a] = 1.0 / len(best_a)\n",
        "            \n",
        "        return policy\n",
        "\n",
        "    def policy_iteration(self, initial_values=None, initial_policy=None, \n",
        "                      theta=0.001, max_iter=None, deterministic=False):\n",
        "        \"\"\"Policy iteration algorithm for estimating pi \\approx pi_*.\n",
        "        (Section 4.3, p. 80.)\n",
        "        \n",
        "        ``initial_values`` is an optional dict whose keys are states and whose \n",
        "        values are initial estimates for the value function.\n",
        "\n",
        "        ``initial_policy`` is an optional dict whose keys are states and whose \n",
        "        values are dicts whose keys are actions and whose values are probabilities.\n",
        "\n",
        "        ``theta`` and ``max_iter`` are termination parameters for the policy \n",
        "        evaluation step.\n",
        "\n",
        "        Set ``deterministic`` to True to use deterministic policies, ``False`` to \n",
        "        use stochastic. (If initial policy is not set, will use equiprobably policy, \n",
        "        even if ``deterministic`` is True. Otherwise, random policy may be unbounded.)\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize value function and policy estimates.\n",
        "        V = {}\n",
        "        pi = {s: {} for s in self.state_list}\n",
        "        for s in self.state_list:\n",
        "            if (s not in self.terminal_states) and (initial_values is not None) and (s in initial_values):\n",
        "                V[s] = initial_values[s]\n",
        "            else:\n",
        "                V[s] = 0\n",
        "            for a in self.allowable_actions[s]:\n",
        "                if (initial_policy is not None) and (a in initial_policy[s]):\n",
        "                    pi[s][a] = initial_policy[s][a]\n",
        "            if np.all([pi[s][a] == 0 for a in self.allowable_actions[s] if a in pi[s]]):\n",
        "                # Use eqiuprobable policy.\n",
        "                for a in self.allowable_actions[s]:\n",
        "                    pi[s][a] = 1.0 / len(self.allowable_actions[s])\n",
        "        k = 0\n",
        "\n",
        "        # Main loop.\n",
        "        policy_stable = False\n",
        "        while not policy_stable:\n",
        "\n",
        "            k += 1\n",
        "\n",
        "            # Policy evaluation.\n",
        "            V = self.iterative_policy_evaluation(pi, V, theta, max_iter, messages=False)\n",
        "\n",
        "            # Policy improvement.\n",
        "            old_pi = copy.deepcopy(pi)\n",
        "            pi = self.greedy_policy(V, deterministic=deterministic)\n",
        "\n",
        "            # Check for termination.\n",
        "            if pi == old_pi:\n",
        "                policy_stable = True\n",
        "\n",
        "        return V, pi\n",
        "\n",
        "    def value_iteration(self, initial_values=None, theta=0.001, detailed_outputs=False):\n",
        "        \"\"\"Value iteration algorithm for estimating pi \\approx pi_*.\n",
        "        (Section 4.4, p. 83.)\n",
        "        \n",
        "        ``initial_values`` is an optional dict whose keys are states and whose \n",
        "        values are initial estimates for the value function.\n",
        "\n",
        "        ``theta`` is a termination parameter for the policy evaluation step.\n",
        "\n",
        "        If ``detailed_outputs`` is False, returns only pi.\n",
        "        Otherwise, returns:\n",
        "            * ``pi``: final policy\n",
        "            * ``V_final``: final value function\n",
        "            * ``V_by_iter``: list of value functions, one per iteration of the algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize value function.\n",
        "        V = {}\n",
        "        for s in self.state_list:\n",
        "            if (s not in self.terminal_states) and (initial_values is not None) and (s in initial_values):\n",
        "                V[s] = initial_values[s]\n",
        "            else:\n",
        "                V[s] = 0\n",
        "        k = 0\n",
        "\n",
        "        # Intialize other outputs.\n",
        "        if detailed_outputs:\n",
        "            V_by_iter = [V]\n",
        "\n",
        "        # Main loop.\n",
        "        Delta = np.inf\n",
        "        while Delta >= theta:\n",
        "\n",
        "            k += 1\n",
        "            Delta = 0\n",
        "\n",
        "            # Make static copy of V.\n",
        "            V_copy = copy.deepcopy(V)\n",
        "\n",
        "            # Loop through states.\n",
        "            for s in self.nonterminal_state_list:\n",
        "\n",
        "                v = V[s]\n",
        "\n",
        "                # Update V[s].\n",
        "                V[s] = np.max([np.sum([self.dynamics[(s, a)][(s_prime, r)] * (r + self.gamma * V_copy[s_prime]) \\\n",
        "                    for s_prime, r in self.dynamics[(s, a)]]) for a in self.allowable_actions[s]])\n",
        "\n",
        "                # Update Delta.\n",
        "                Delta = max(Delta, abs(v - V[s]))\n",
        "\n",
        "            # Remember value function.\n",
        "            if detailed_outputs:\n",
        "                V_by_iter.append(copy.deepcopy(V))\n",
        "\n",
        "        # Build greedy policy.\n",
        "        pi = self.greedy_policy(V, deterministic=True)\n",
        "\n",
        "        if detailed_outputs:\n",
        "            return pi, V, V_by_iter\n",
        "        else:\n",
        "            return pi\n",
        "\n",
        "    def play(self, num_time_steps, policy=None, messages=True):\n",
        "        \"\"\" \"Play\" the MDP for a fixed number of time steps or until a terminal state is reached. \n",
        "        If policy is not specified, uses equiprobable policy.\n",
        "\n",
        "        Returns both the total reward for the episode and a dict indicating the average reward for each state.\"\"\"\n",
        "\n",
        "        # Initial state.\n",
        "        s = self.initial_state\n",
        "        if messages:\n",
        "            print(f\"Initial state: {s}\")\n",
        "\n",
        "        # Initialize value function and discount factor.\n",
        "        v = {s: 0 for s in self.state_list}\n",
        "        count = {s: 0 for s in self.state_list}\n",
        "\n",
        "        # Initialize total_reward.\n",
        "        total_reward = 0\n",
        "\n",
        "        # Play.\n",
        "        for t in range(num_time_steps):\n",
        "\n",
        "            # Choose action.\n",
        "            if policy is None:\n",
        "                a = self.get_action_equiprobable(s)\n",
        "            else:\n",
        "                a = self.get_action_from_policy(s, policy)\n",
        "\n",
        "            # Calculate reward and update value function and return.\n",
        "            s_next, r = self.get_next_state_and_reward(s, a)\n",
        "            total_reward += r * self.gamma**t\n",
        "            v[s] += r * self.gamma\n",
        "            count[s] += 1\n",
        "\n",
        "            if messages:\n",
        "                print(f\"From state {s} taking action {self.action_abbr(a)} ==> reward {r} new state {s_next}\")\n",
        "\n",
        "            # Update state.\n",
        "            s = s_next\n",
        "            if s in self.terminal_states:\n",
        "                break\n",
        "\n",
        "        if messages:\n",
        "            print(\"Final reward matrix:\")\n",
        "            rewards = {}\n",
        "            for s in self.state_list:\n",
        "                if count[s] == 0:\n",
        "                    rewards[s] = np.nan\n",
        "                else:\n",
        "                    rewards[s] = v[s] / count[s]\n",
        "            self.print_values_by_state(rewards)\n",
        "\n",
        "        return total_reward, v"
      ],
      "metadata": {
        "id": "np4COQYT9KOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the MPNV MDP\n",
        "\n",
        "The next two functions build the `dynamics` dict and the `MDP` object for the MPNV problem. Recall that if we begin a time period with an IL of $s$ and order $a$ units, we begin the next time period in state $s' = s + a - d$ and incur a cost of\n",
        "$$h(s+a-d)^+ + p(d-(s+a))^+,$$\n",
        "i.e., we earn a reward of\n",
        "$$-\\left[h(s+a-d)^+ + p(d-(s+a))^+\\right].$$\n",
        "In other words, \n",
        "$$p(s',r|s,a) = \\begin{cases} f(s + a - s'), & \n",
        "\\text{if } s' \\le s + a \\text{ and } r = h(s+a-d)^+ + p(d-(s+a))^+ \\\\ 0, & \\text{otherwise}\\end{cases}.$$\n",
        " This is how we calculate `dynamics`.\n"
      ],
      "metadata": {
        "id": "IG-R0G1WEycz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `build_MPNV_dynamics()` function below is incomplete. Your job is to complete it, filling in the missing piece to calculate the `dynamics` dict. \n",
        "\n",
        "There's a small catch: For a given $s, a$, you'll calculate $p(s',r|s,a)$ only for $s'$ that are in within the truncated state space (indicated by the `min_state` and `max_state` parameters). But since $s'$ can be arbitrarily small (since $d$ can be arbitrarily large), this is only an approximation of the state space. Therefore, the probabilities $p(s',r|s,a)$ will not sum to 1 for a given $s, a$. My code will pick up where yours left off, and fix this discrepancy for you.\n",
        "\n",
        "---\n",
        "> **Note:** In the code below, the portions that you need to complete are marked with\n",
        "> \n",
        "> ```python\n",
        "> # #################\n",
        "> # TODO:\n",
        "> ```\n",
        "> \n",
        "> In place of the missing code is a line that says \n",
        "> \n",
        "> ```python\n",
        "> \traise NotImplementedError\n",
        "> ```\n",
        "> \n",
        "> This is a way of telling Python to raise an exception (error) because there's something missing here. You should **delete (or comment out) this line** after you write your code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MZIgetJ9Y7oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_MPNV_dynamics(h: float, p: float, mu: int, min_state: int, max_state: int, \n",
        "                        max_order_quantity: int = None) -> dict:\n",
        "    \"\"\"Build a `dynamics` object for the MPNV suitable for sending to an MDP object. \n",
        "    Assumes the demand has a Poisson distribution.\n",
        "\n",
        "    `dynamics` is a dict whose keys are (state, action) pairs and whose values are dicts\n",
        "    whose keys are (next_state, reward) pairs and whose values are probabilities.\n",
        "    That is, dynamics[(s, a)][(s_prime, r)] is the probability that state s_prime and reward r\n",
        "    results from taking action a in state s.\n",
        "    Only (next_state, reward) pairs that have nonzero probability are included.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    h : \n",
        "        Per-period holding cost.\n",
        "    p : \n",
        "        Per-period stockout cost.\n",
        "    mu : \n",
        "        Mean demand per period.\n",
        "    min_state, max_state :\n",
        "        Min and max of the state space. The state space will be \n",
        "        truncated to these limits. Although the true state space goes to negative \n",
        "        infinity (because the demand is unbounded) and to positive infinity (because \n",
        "        we don't know in advance how large the order will be), we must truncate it.\n",
        "    max_order_quantity : Maximum allowable order quantity. Used to reduce the \n",
        "        action space, if desired. Set to None (the default) to allow all order \n",
        "        quantities (up to the maximum of the state space).\n",
        "    gamma : \n",
        "        Discount factor. Defaults to 0.95.\n",
        "    \"\"\"\n",
        "    # Constants.\n",
        "    max_order_quantity = max_order_quantity or max_state - min_state + 1\n",
        "\n",
        "    # Build state space.\n",
        "    state_space = list(range(min_state, max_state + 1))\n",
        "    action_space = list(range(max_order_quantity + 1))\n",
        "\n",
        "    # #################\n",
        "    # TODO: Build the `dynamics` dict:\n",
        "    # \tfor each s in the state space:\n",
        "    #   \tfor each a in the action space such that s + a is in the state space: \n",
        "    #       \tfor each demand d such that s + a - d is in the state space:\n",
        "    #           \tset dynamics[s, a][s_prime, r] = P(D = d) for the appropriate\n",
        "    #           \tvalues of s_prime and r (where P(D = d) is the probability that\n",
        "    #           \tthe demand equals d)\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # The probabilities for each (s, a) pair will not sum to 1 because of the\n",
        "    # state-space truncation. Artificially inflate the probability for the\n",
        "    # minimum state to account for this.\n",
        "    for s, a in list(dynamics.keys()):\n",
        "        r = [rew for s_prime, rew in list(dynamics[s, a].keys()) if s_prime == min_state][0]\n",
        "        dynamics[s, a][min_state, r] += 1 - np.sum(list(dynamics[s, a].values()))\n",
        "        # Double-check that the probabilities sum to 1.\n",
        "        assert np.isclose(np.sum(list(dynamics[s, a].values())), 1, atol=1.0e-6)\n",
        "   \n",
        "    return dynamics"
      ],
      "metadata": {
        "id": "oIAX12l_O4M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_MPNV_MDP(h: float, p: float, mu: int, min_state: int, max_state: int, \n",
        "                   max_order_quantity: int = None, initial_state: int = 0, \n",
        "                   gamma: float = 0.95) -> MDP:\n",
        "    \"\"\"Build an MDP object representing an MPNV problem. Assumes the demand has a Poisson distribution.\n",
        "\n",
        "    In the MDP, a state represents an inventory position (= on-hand inventory minus backorders plus\n",
        "    on-order inventory) and an action represents an order quantity. The objective is to maximize the\n",
        "    negative of the total discounted cost over the infinite horizon.\n",
        "\n",
        "    For parameter descriptions, see docstring for `build_MPNV_dynamics()`.\n",
        "    \"\"\"\n",
        "\n",
        "    dynamics = build_MPNV_dynamics(h, p, mu, min_state, max_state, max_order_quantity)\n",
        "    \n",
        "    MPNV_MDP = MDP(\n",
        "        dynamics=dynamics,\n",
        "        initial_state=initial_state, \n",
        "        terminal_states=[],\n",
        "        gamma=gamma\n",
        "    )\n",
        "\n",
        "    return MPNV_MDP"
      ],
      "metadata": {
        "id": "e0HD_PiFC_Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function plots a given policy in two ways: order quantity vs. inventory level and order-up-to level (= inventory level + order quantity) vs. inventory level. This will be useful for visualizing the results."
      ],
      "metadata": {
        "id": "CKpO9745O8vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy(pi: dict, title: str = None):\n",
        "    \"\"\"Plot the policy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pi : \n",
        "        Probability distribution for a policy. A dict whose keys are states and\n",
        "        whose values are dicts whose keys are actions and whose values are probabilities.\n",
        "        That is, pi[s][a] = probability of taking action a in state s. Although the\n",
        "        policies considered in this MDP are deterministic (and this function will\n",
        "        assume they are), this stochastic format is what is returned by the optimization\n",
        "        algorithms in the MDP class.\n",
        "    title : \n",
        "        Optional title for the figure.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the figure and set the title.\n",
        "    fig = plt.figure(figsize=plt.figaspect(1/2))\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Build a list of x values and a dict of order quantities from the policy.\n",
        "    x_list = sorted(list(pi.keys()))\n",
        "    Q_dict = {x: a for x in x_list for a in pi[x] if pi[x][a] > 0}\n",
        "\n",
        "    # Order quantity plot.\n",
        "    ax = plt.subplot(121)\n",
        "    Q_list = [Q_dict[x] for x in x_list]\n",
        "    plt.plot(x_list, Q_list)\n",
        "    plt.xlabel('Inventory Level')\n",
        "    plt.ylabel('Order Quantity')\n",
        "\n",
        "    # Order-up-to level plot.\n",
        "    ax = plt.subplot(122)\n",
        "    y_list = [x + Q_dict[x] for x in x_list]\n",
        "    plt.plot(x_list, y_list)\n",
        "    plt.xlabel('Inventory Level')\n",
        "    plt.ylabel('Order-Up-To Level')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2eyDTSLCO4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPNV Instance\n",
        "\n",
        "Finally, we are ready to solve an MPNV problem. We'll solve the following instance:\n",
        "\n",
        "* $h = 1$\n",
        "* $p = 10$\n",
        "* $\\mu = 5$\n",
        "\n",
        "We'll build an MDP for this instance using the `build_MPNV_MDP()` function.\n",
        "\n",
        "Since the mean demand is 5, it's unlikely that we'd want to order up to something larger than 20, so we'll use 20 as the upper bound of our state space. And if we order up to 5, the probability of a demand greater than 20 (and hence a new inventory position less than $-$15) is less than $10^{-7}$, so we'll use $-$15 as the lower bound of our state space. Thus, we'll set `state_space_limits = (-15, 20)`.\n",
        "\n",
        "We'll use the default values for the other parameters of `build_MPNV_MDP()`."
      ],
      "metadata": {
        "id": "LCq1LFyaRIqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set problem parameters.\n",
        "h = 1\n",
        "p = 10\n",
        "mu = 5\n",
        "min_state = -15\n",
        "max_state = 20\n",
        "gamma = 0.95"
      ],
      "metadata": {
        "id": "5ZwIuLONRy5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pseudorandom number generator seed, for reproducibility when debugging.\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "_iU19daAPS50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MDP.\n",
        "mpnv_mdp = build_MPNV_MDP(h=h, p=p, mu=mu, min_state=min_state, max_state=max_state, gamma=gamma)"
      ],
      "metadata": {
        "id": "wmf8b3zYTJte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solving the MPNV MDP\n",
        "\n",
        "Here comes the main event -- optimization! This step should take a few seconds."
      ],
      "metadata": {
        "id": "8ugZA_ToB4xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize the order quantities using value iteration.\n",
        "pi = mpnv_mdp.value_iteration()\n",
        "\n",
        "# Alternately, use policy iteration.\n",
        "#_, pi = mpnv_mdp.policy_iteration()"
      ],
      "metadata": {
        "id": "Fw0uj5yHTPN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the results.\n",
        "\n",
        "First, let's plot the policy."
      ],
      "metadata": {
        "id": "eZnbhyA9CFfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the policy.\n",
        "plot_policy(pi)"
      ],
      "metadata": {
        "id": "DIti-Wn5YP0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all went well, the plot on the left (order quantity vs. inventory level) should decrease linearly with a slope of $-1$ until it flattens out at $y=0$; and the plot on the right (order-up-to level vs. inventory level) should be flat at first and then increase linearly with a slope of $1$. \n",
        "\n",
        "These shapes indicate that the optimal policy found by the MDP is a **base-stock policy:** We order up to a fixed value (the *base-stock level*), unless the inventory level is already greater than that value, in which case we order nothing. \n",
        "\n",
        "For this instance, the optimal base-stock level is 8. We can see this because the order-up-to level is 8 for any inventory levels less than or equal to 8; or because the order quantity is 0 when the inventory level is greater than or equal to 8."
      ],
      "metadata": {
        "id": "kzlqHdDqaBJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp_base_stock_level = list(pi[min_state].keys())[0] + min_state\n",
        "print(f\"Base-stock level found by MDP = {mdp_base_stock_level}\")"
      ],
      "metadata": {
        "id": "Q1zdv7m2FwcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's evaluate this policy by running a handful of episodes. The `play()` function of the `MDP` class makes this easy."
      ],
      "metadata": {
        "id": "PZD1vZ8wbJFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 100\n",
        "T = 1000 # number of time steps per episode\n",
        "avg_reward = 0\n",
        "pbar = tqdm(total=num_episodes) # progress bar\n",
        "for _ in range(num_episodes):\n",
        "    pbar.update()\n",
        "    total_reward, _ = mpnv_mdp.play(num_time_steps=T, policy=pi, messages=False)\n",
        "    avg_reward += total_reward / num_episodes\n",
        "\n",
        "print(f\"After {num_episodes} episodes, average total discounted reward per episode = {avg_reward}\")"
      ],
      "metadata": {
        "id": "H5iY-tjZYgFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Using `stockpyl`\n",
        "\n",
        "In this section, we'll validate the results of the MDP using `stockpyl`.\n"
      ],
      "metadata": {
        "id": "7DBqjy8AhOR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll validate the base-stock level and average reward produced by the MDP. The MPNV problem with Poisson demand has a simple analytical solution, which is implemented in the `newsvendor.newsvendor_poisson()` function in `stockpyl`. (Documentation for this function is available [here](https://stockpyl.readthedocs.io/en/latest/api/seio/newsvendor.html#stockpyl.newsvendor.newsvendor_poisson).)\n",
        "\n",
        "> **Note:** The `newsvendor_poisson()` function is actually solving the (single-period) newsvendor problem. However, the single-period and infinite-horizon versions of the problem have the same optimal solution and optimal expected cost per period (provided that either the discount factor or the purchase cost is 0; here, we are assuming the purchase cost is 0); see Snyder and Shen (2nd edition, 2019). Hence, we can use `newsvendor_poisson()` here.\n"
      ],
      "metadata": {
        "id": "JkoNb4xM1Sly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the stockpyl modules we'll need.\n",
        "from stockpyl.newsvendor import newsvendor_poisson\n",
        "from stockpyl import sim\n",
        "from stockpyl.supply_chain_network import single_stage_system"
      ],
      "metadata": {
        "id": "AHZztKJm1pjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve the newsvendor problem with Poisson demand.\n",
        "optimal_base_stock_level, optimal_cost = newsvendor_poisson(h, p, mu)\n",
        "print(f\"Optimal base-stock level is {optimal_base_stock_level}, with expected cost per period {optimal_cost}\")"
      ],
      "metadata": {
        "id": "PkL4G5qejoT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal base-stock level matches what the MDP found. The costs differ, though. That's because the MDP `play()` function reports the total discounted cost over the episode (which we set to 1000 periods), while the `newsvendor_poisson()` function reports the expected (undiscounted) cost per period. \n",
        "\n",
        "But if the expected cost per period is $g$, then the expected discounted cost over $T$ time periods is\n",
        "\n",
        "$$\\sum_{t=0}^{T-1} g\\gamma^i = g\\frac{1 - \\gamma^{T}}{1-\\gamma}$$\n",
        "\n",
        "using standard formulas for geometric series. So:"
      ],
      "metadata": {
        "id": "CE3GdF1_39or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_expected_discounted_cost_per_episode = optimal_cost * (1 - gamma**T) / (1 - gamma)\n",
        "print(f\"Expected discounted cost over {T} periods is {optimal_expected_discounted_cost_per_episode}\")"
      ],
      "metadata": {
        "id": "sDcw9qFv7TTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cost is close to the (negative of the) reward reported by the MDP `play()` function."
      ],
      "metadata": {
        "id": "_izF3AgS8Bkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll simulate the system using `stockpyl`'s simulation features. This is analogous to using the `MDP.play()` approach. \n",
        "\n",
        "First we have to build the supply chain network to be simulated. The network must use a data structure provided by `stockpyl` called `SupplyChainNetwork`. There are several ways to build such an object in `stockpyl`. Since our network consists of only a single node, we'll use a built-in `stockpyl` function that builds a single-node network. We'll tell it to use the base-stock level returned by the `newsvendor_poisson()` function."
      ],
      "metadata": {
        "id": "7hrx4rAR8ENM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build network to simulate.\n",
        "network = single_stage_system(\n",
        "    local_holding_cost=h,\n",
        "    stockout_cost=p,\n",
        "    demand_type='P',                    # Poisson demand\n",
        "    mean=mu,\n",
        "    policy_type='BS',                   # base-stock inventory policy\n",
        "    base_stock_level=optimal_base_stock_level,\n",
        "    shipment_lead_time=1\n",
        ")"
      ],
      "metadata": {
        "id": "8SqLGCwl8kGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we run the simulation. The `simulation()` function returns the average (undiscounted) cost per period, which should match the expected cost per period that we found above. (This cell will take 30 seconds or so to execute.)"
      ],
      "metadata": {
        "id": "kFwifkVzD7xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate system.\n",
        "mean_cost, _ = sim.run_multiple_trials(network, 10, 1000, progress_bar=False)\n",
        "print(f\"Average cost per period is {mean_cost}\")"
      ],
      "metadata": {
        "id": "et0jSRs0mErl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize:"
      ],
      "metadata": {
        "id": "yhRQ2hmLEMf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate([\n",
        "    [\"Base-stock level found by MDP\", mdp_base_stock_level],\n",
        "    [\"Optimal base-stock level\", optimal_base_stock_level],\n",
        "    [\"Average discounted reward per episode from MDP play()\", avg_reward],\n",
        "    [\"Optimal expected discounted reward per episode\", optimal_expected_discounted_cost_per_episode],\n",
        "    [\"Optimal expected cost per period\", optimal_cost],\n",
        "    [\"Simulated average cost per period\", mean_cost]\n",
        "]))"
      ],
      "metadata": {
        "id": "gtSPG04I81GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If You Have Extra Time\n",
        "\n",
        "We have assumed that there is no **fixed cost** to place an order, in which case a base-stock policy is optimal.\n",
        "\n",
        "Suppose now that we do have a fixed cost: We pay $K$ in each period in which we place a non-zero order, regardless of the size of the order. \n",
        "\n",
        "Write code to build an MDP with this cost structure. Solve the MDP using the same instance as before, plus $K=20$. Check whether the optimal actions follow an **$(s,S)$ policy**, in which the order quantity is given by\n",
        "\n",
        "$$Q = \\begin{cases} S-x, & x \\le s \\\\ 0, & x > s \\end{cases}$$\n",
        "\n",
        "where $x$ is the inventory level and $S$ and $s$ are policy parameters called the **order-up-to** level and the **reorder point**, respectively. (Note: here, $s$ does not mean \"state\"! Sorry for the confusing notation!)\n",
        "\n",
        "Also use `stockpyl` to find the optimal $s$ and $S$, via the `s_s_discrete_exact()` function (see documentation [here](https://stockpyl.readthedocs.io/en/latest/api/seio/ss.html#stockpyl.ss.s_s_discrete_exact)). In particular, the following code solves the $(s,S)$ problem for a Poisson distribution with mean `mu`:\n",
        "\n",
        "```python\n",
        "from stockpyl.ss import s_s_discrete_exact\n",
        "s, S, cost = s_s_discrete_exact(holding_cost=h, stockout_cost=p, fixed_cost=K, use_poisson=True, demand_mean=mu)\n",
        "```\n",
        "\n",
        "(Unfortunately, `stockpyl` cannot (yet) handle fixed costs in the simulation, so you can't simulate this system.)\n"
      ],
      "metadata": {
        "id": "9IYS7ZjSU707"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZK9a3XcqBRIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}