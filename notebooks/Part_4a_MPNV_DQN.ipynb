{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 4a: MPNV DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/P4zkMSv5rdLjsv1VuIcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/Part_4a_MPNV_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN for the Multi-Period Newsvendor Problem (MPNV)\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filenameâ€”not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University. The full repo is at https://github.com/LarrySnyder/RLforInventory.\n",
        "---"
      ],
      "metadata": {
        "id": "4REAg0yRBTmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll implement a simple DQN approach for solving the MPNV. We'll use the OpenAI `gym` package to define and manage our MPNV environment, and we'll use Tensorflow to do the deep RL.\n",
        "\n",
        "This notebook is just a demonstration; there are no exercises. Instead, you'll use it as a model to build a DQN approach for the beer game problem in the next notebook.\n",
        "\n",
        "The code used in this notebook is based on the approach outlined in the blog post \"[Building a Reinforcement Learning Environment using OpenAI Gym](https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/)\" by Lilian Tonia."
      ],
      "metadata": {
        "id": "mZgYaBvFBbYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff\n",
        "\n",
        "First we'll install the Python packages we need that are not pre-installed in Colab. The `pip install` commands below worked for me; I hope they work for you. I recommend not modifying the version numbers in the commands. Once you start tinkering with the dependencies, things can get messy. (Take my word for it.) "
      ],
      "metadata": {
        "id": "vL2mOvQrCZh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.2\n",
        "!pip install gym==0.23\n",
        "!pip install keras==2.8.0\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "id": "JijBa08URv4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "id": "Zpegaj6mXC3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import the packages we need."
      ],
      "metadata": {
        "id": "0NFSAGmCD48x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mqn9M8MZRym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stockpyl import sim\n",
        "from stockpyl.supply_chain_network import single_stage_system"
      ],
      "metadata": {
        "id": "16f2ytIQXl0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Environment\n",
        "\n",
        "The code below defines an environment for the MPNV. This class is identical to the `MPNVEnv` class we defined in the \"RL for MPNV\" notebook, except:\n",
        "\n",
        "* The class is subclassed from the `gym.Env` class.\n",
        "* There is no `gamma` parameter. (We'll assume no discounting.)\n",
        "* The `__init__()` method defines the `action_space` and `observation_space` (aka state space) using datatypes provided by `gym`. These are a little annoying to query, so we also define `action_space_list` and `observation_space_list` as simple lists that can be accessed when needed.\n",
        "* There is no `allowable_actions` attribute. All actions are considered allowable for every state, but actions that bring the inventory level above `state_max` just bring the IL to `state_max`. \n",
        "* There are no methods to get actions (e.g., `get_epsilon_greedy_action()`). Tensorflow will handle this for us."
      ],
      "metadata": {
        "id": "2mOI44FjEPkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNVEnv(Env):\n",
        "    \"\"\"Multi-period newsvendor (MPNV) problem environment. A state represents an inventory level. \n",
        "    An action is an order quantity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : SupplyChainNetwork\n",
        "        The network to simulate.\n",
        "    episode_length : int\n",
        "        The number of periods in one episode.\n",
        "    min_state : int\n",
        "        The minimum value of the state space to consider.\n",
        "    max_state : int\n",
        "        The maximum value of the state space to consider.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network, episode_length: int, min_state: int, max_state: int):\n",
        "\n",
        "        self.min_state = min_state\n",
        "        self.max_state = max_state\n",
        "        # State and action spaces, as gym spaces.\n",
        "\t\t# Note: I'm using the `start` parameter to tell gym that the Discrete\n",
        "\t\t# space should be numbered starting at something other than 0. This is\n",
        "\t\t# a new feature in gym (as of v0.23 or so) and will not be compatible\n",
        "\t\t# with earlier versions.\n",
        "        self.action_space = Discrete(max_state + 1)\n",
        "        self.observation_space = Discrete(max_state - min_state + 1, start=min_state)\n",
        "        # State and action spaces as lists, for convenience.\n",
        "        self.action_space_list = list(range(max_state + 1))\n",
        "        self.observation_space_list = list(range(min_state, max_state + 1))\n",
        "\n",
        "        # Store problem data.\n",
        "        self.network = network # the SupplyChainNetwork object\n",
        "        self.episode_length = episode_length\n",
        "\n",
        "        # Start the system in state 0.\n",
        "        self.initial_state = 0\n",
        "\n",
        "        # Initialize current state info.\n",
        "        self.state = None\n",
        "\n",
        "        # Get a shortcut to the (single) node in the network, for convenience.\n",
        "        self.node = network.nodes[0]\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment and the simulation. Set the state to the initial\n",
        "        state. Also set the corresponding attribute of the SupplyChainNode.\"\"\"\n",
        "\n",
        "        # Determine initial IL and store it in environment's state.\n",
        "        self.state = self.initial_state\n",
        "        \n",
        "        # Set node's initial IL attribute. (This will force the simulation to \n",
        "        # start with the node at this inventory level.)\n",
        "        self.node.initial_inventory_level = self.initial_state\n",
        "\n",
        "        # Reset the simulation environment.\n",
        "        sim.initialize(self.network, self.episode_length)\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Run one time step of the environment by taking the specified action.\n",
        "        Update the environment state to the new state. \n",
        "        Return a tuple (new_state, reward, done, info).\"\"\"\n",
        "\n",
        "        # Build dict specifying order quantity to use in this time period.\n",
        "        # (This will override the order quantities that the stockpyl simulation\n",
        "        # would choose on its own.) Make sure the order quantity does not bring\n",
        "        # the IL above its max value.\n",
        "        order_quantity = min(action, self.max_state - self.state)\n",
        "        order_quantity_override = {self.node: order_quantity}\n",
        "\n",
        "        # Simulate one time period.\n",
        "        sim.step(self.network, order_quantity_override=order_quantity_override)\n",
        "\n",
        "        # Determine reward by querying the simulation's state variables.\n",
        "        reward = -self.node.state_vars_current.total_cost_incurred\n",
        "\n",
        "        # If episode length has been reached, terminate.\n",
        "        done = self.network.period == self.episode_length - 1\n",
        "\n",
        "        # Get new inventory level from simulation. \n",
        "        # (Round to int -- should already be integer but sometimes there are small rounding errors.)\n",
        "        IL = int(self.node.state_vars_current.inventory_level)\n",
        "        # If new IL is outside the bounds of state space, truncate.\n",
        "        IL = int(np.clip(IL, self.min_state, self.max_state))\n",
        "\n",
        "        # Update state.\n",
        "        self.state = IL\n",
        "\n",
        "        # Fill the demand into the info dict.\n",
        "        info = {'demand': self.node.state_vars_current.inbound_order[None]}\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"This function can contain code for drawing the environment to\n",
        "        a graphics window, or printing it in ASCII format to the terminal.\n",
        "        But we'll just do something very simple and print the state.\n",
        "        (Feel free to add some nicer visualization code here if you want!)\"\"\"\n",
        "        print(self.state)\n",
        "\n",
        "    def play_episode(self, policy, messages=False):\n",
        "        \"\"\"Play one episode of the environment following the specified policy. \n",
        "        Return the total discounted reward over the episode.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize environment.\n",
        "        self.reset()\n",
        "        cumul_reward = 0\n",
        "\n",
        "        if messages:\n",
        "            print(f\"policy = {policy}\")\n",
        "            print(f\"Initial state = {self.state}, total reward = {cumul_reward}\")\n",
        "\n",
        "        # Step through until terminal state reached.\n",
        "        for t in range(self.episode_length):\n",
        "            \n",
        "            # Determine action.\n",
        "            action = policy[self.state]\n",
        "\n",
        "            if messages:\n",
        "                print(f\"timestep {t:6} state = {self.state:4} action = {action:4} \", end=\"\")\n",
        "\n",
        "            # Step.\n",
        "            new_state, reward, done, info = self.step(action)\n",
        "\n",
        "            # Update cumulative reward.\n",
        "            cumul_reward += reward\n",
        "\n",
        "            if messages:\n",
        "                print(f\"demand = {info['demand']:4} new_state = {new_state:4} reward = {reward:8.2f} cumulative reward = {cumul_reward:8.2f}\")\n",
        "   \n",
        "        return cumul_reward"
      ],
      "metadata": {
        "id": "fyiDsHkocwa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's our policy-plotting function:"
      ],
      "metadata": {
        "id": "MRMDP5RO6Mar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy(env: MPNVEnv, policy: dict, title: str = None):\n",
        "    \"\"\"Plot the policy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pi : \n",
        "        Probability distribution for a policy. A dict whose keys are states and\n",
        "        whose values are actions. (Note that this is a different structure\n",
        "        than what was used in the \"MPNV as MDP\" notebook.)\n",
        "    title : \n",
        "        Optional title for the figure.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=plt.figaspect(1/2))\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Order quantity plot.\n",
        "    ax = plt.subplot(121)\n",
        "    x_list = env.observation_space_list\n",
        "    y_list = [policy[x] for x in x_list]\n",
        "    plt.plot(x_list, y_list)\n",
        "    plt.xlabel('Starting Inventory Level')\n",
        "    plt.ylabel('Order Quantity')\n",
        "\n",
        "    # Order-up-to level plot.\n",
        "    ax = plt.subplot(122)\n",
        "    y_list = [x + policy[x] for x in x_list]\n",
        "    plt.plot(x_list, y_list)\n",
        "    plt.xlabel('Starting Inventory Level')\n",
        "    plt.ylabel('Order-Up-To Level')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "wy_uJzgqw80t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Instance\n",
        "\n",
        "Again we'll use the same MPNV instance:\n",
        "\n",
        "* $h = 1$\n",
        "* $p = 10$\n",
        "* $\\mu = 5$\n"
      ],
      "metadata": {
        "id": "v2WJKrrz6S8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build stockpyl SupplyChainNetwork object.\n",
        "network = single_stage_system(\n",
        "    local_holding_cost=1,\n",
        "    stockout_cost=10,\n",
        "    demand_type='P',\n",
        "    mean=5\n",
        ")"
      ],
      "metadata": {
        "id": "sJKUIo8TXiGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll use the same state space, $[-10,10]$. \n",
        "\n",
        "This time, we'll consider episodes of length 100 periods. Since we're working up to solving the beer game, we'll start to think of this inventory problem as a \"game\" with a fixed number of time periods, like the beer game has."
      ],
      "metadata": {
        "id": "SmIy2fO16b0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_state = -10\n",
        "max_state = 10\n",
        "episode_length = 100"
      ],
      "metadata": {
        "id": "nSUhScVD611O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's build our `MPNVEnv` environment.\n",
        "\n",
        "Remember: This is now a well-defined `gym` environment. It's possible to \"register\" a custom environment to take advantage of the full `gym` API, but we won't need to do that here."
      ],
      "metadata": {
        "id": "j87w-pJB67pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MPNVEnv object.\n",
        "env = MPNVEnv(\n",
        "    network=network,\n",
        "    episode_length=episode_length,\n",
        "    min_state=min_state,\n",
        "    max_state=max_state\n",
        ")"
      ],
      "metadata": {
        "id": "d39RKRwp6wa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give our new environment a quick spin. First, we'll create a base-stock policy with a base-stock level of 6. Then we'll ask our environment to play one episode of the MPNV \"game\". In each time period, it will print the starting state, the action (order quantity), the demand, the new state, and the reward.\n",
        "\n",
        "(I recommend that you think through a few periods to sanity-check the logic of the dynamics.)"
      ],
      "metadata": {
        "id": "cP2Y_QgE8jKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_stock_policy = {s: max(0, 6 - s) for s in env.observation_space_list}\n",
        "env.play_episode(base_stock_policy, messages=True)"
      ],
      "metadata": {
        "id": "ASsC_ztI3sWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up TensorFlow\n",
        "\n",
        "Next we'll set up our model in TensorFlow. First, some imports:"
      ],
      "metadata": {
        "id": "_HPX1ppg-TmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wSYk6yFwSZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy \n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "IcG-gFxhSqiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then a helper function to build the TF **model:**"
      ],
      "metadata": {
        "id": "RFO0_ZuC-sTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()    \n",
        "    model.add(Dense(24, activation='relu', input_shape=(1,))) \n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "5BNytuonShes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll build the model itself:"
      ],
      "metadata": {
        "id": "Eos3SOdM_KWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shortcut to size of observation and action spaces.\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "# Build the model.\n",
        "# NOTE: This must happen *after* the `from rl.x` imports.\n",
        "# (See https://stackoverflow.com/a/72438856/3453768)\n",
        "model = build_model(num_states, num_actions)"
      ],
      "metadata": {
        "id": "B4MEcBFtSndQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a summary of the model:"
      ],
      "metadata": {
        "id": "ZNol9lDX_cfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fizqVg6MSpQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need an RL **agent.** We'll use the `DQNAgent` class built into `keras` (part of TensorFlow). \n",
        "\n",
        "Our agent also needs a **policy.** We'll use the `EpsGreedyQPolicy`, again built into `keras`. (Feel free to play around with different policies. You'll have to `import` them like we did for `EpsGreedyQPolicy` above. I haven't been able to find good documentation for these policies, but you can find different policies to try by looking at the [source code](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).)"
      ],
      "metadata": {
        "id": "j3ziPVMQ_jI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = EpsGreedyQPolicy(eps=0.1) \n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "-1CEDkM4SyOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, build the DQN agent, store it in a variable called `dqn`, and \"compile\" it (a preprocessing step)."
      ],
      "metadata": {
        "id": "gmu1d7Z8BOoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, num_actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "id": "jJASpBtRS1RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DQN Agent\n",
        "\n",
        "Now we're finally ready for the main step: training the DQN agent. The command below trains it for 60,000 episodes, which should take about 10 minutes and produce medium-good results. Feel free to change this number to do more or less training."
      ],
      "metadata": {
        "id": "LzS1LLIYBgin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "id": "y51byZphBblU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most likely, you'll see the `episode_reward` get gradually better as the training progresses (though not necessarily monotonically so)."
      ],
      "metadata": {
        "id": "z6xdFPo6DlvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Results\n",
        "\n",
        "The DQN agent has a feature to test the learned policy by playing multiple episodes and print the results. Let's play 50 of them."
      ],
      "metadata": {
        "id": "aMOS9JTBCrvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = dqn.test(env, nb_episodes=50, visualize=False)\n",
        "print(f\"Average reward per episode = {np.mean(results.history['episode_reward'])}\")"
      ],
      "metadata": {
        "id": "0INhM5lxS-ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My results showed an average reward per episode of $-528$. Recall that this problem can be solved analytically; the optimal policy is a base-stock policy with base-stock level 8 and expected cost $4.34$ per period. So a 100-period episode has an optimal reward of $-434$. My agent's reward of $-528$ is so-so. Of course we could improve it by more training, different hyperparameters, etc."
      ],
      "metadata": {
        "id": "P2huQdP1EU-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a closer look at the policy that the DQN agent learned. The cells below parse the DQN's policy, print it as a dict, and then plot it."
      ],
      "metadata": {
        "id": "qv3tfG_JHmS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the learned policy.\n",
        "dqn_policy = {s: np.argmax(dqn.compute_q_values(s)) for s in env.observation_space_list}\n",
        "dqn_policy"
      ],
      "metadata": {
        "id": "E37MsswDqkyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_policy(env, dqn_policy)"
      ],
      "metadata": {
        "id": "y1Lp4S0-x8Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is nothing like a base-stock policy! It's possible for a non-optimal policy still to have good performance. This policy is only so-so; with better training, it remains to be seen how closely the learned policy will approximate a base-stock policy. \n",
        "\n",
        "(It's worth remembering that TD learning approximated the policy more closely. DQN is much more powerful but also requires much more extensive training.)"
      ],
      "metadata": {
        "id": "A6g3T8E_Ic8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Up\n",
        "\n",
        "Move on to notebook `4b: Beer Game DQN`!\n"
      ],
      "metadata": {
        "id": "HhHYPU8cKyIn"
      }
    }
  ]
}