{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 3: RL for MPNV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3nS3DJfLO0vGgCox/oV/8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/Part_3_RL_for_MPNV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL for the Multi-Period Newsvendor Problem (MPNV) \n",
        "\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filename—not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University.\n",
        "---\n",
        "\n",
        "Recall from the previous notebook that the multi-period newsvendor problem (MPNV) deciding, in each time period, how much to order in advance of observing a random demand. If we begin the period with an inventory level of $s$ (this is the **state**), place an order of size $a$ (this is the **action**), and experience a demand of $d$, then the cost in the period is\n",
        "\n",
        "$$h(s+a-d)^+ + p(d-(s+a))^+.$$\n",
        "\n",
        "The **Bellman equation** for the value function $v_\\pi$ is:\n",
        "\n",
        "$$v_\\pi(s) = {\\mathbb E}_D\\left[ -\\left(h(s + a -d)^+ + p(d - (s+a))^+\\right) + \\gamma v_\\pi(s+a-D)\\right],$$\n",
        "\n",
        "where ${\\mathbb E}_D$ denotes expectation over the random demand. For the optimal policy, the **Bellman optimality equation** is:\n",
        "\n",
        "$$v_*(s) = \\max_{a\\ge 0} {\\mathbb E}_D\\left[ -\\left(h(s+a-d)^+ + p(d-(s+a))^+\\right) + \\gamma v_*(s+a-D)\\right].$$\n",
        "\n"
      ],
      "metadata": {
        "id": "mQmitR3h8t3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the simulation features in `stockpyl` package (https://pypi.org/project/stockpyl/) to build our RL environment. "
      ],
      "metadata": {
        "id": "lHVbwBxRZ1f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff\n"
      ],
      "metadata": {
        "id": "XMUCsMuHnN2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll install `stockpyl`. You should only need to do this once. If you get a message like\n",
        "\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [sphinxcontrib]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```\n",
        "\n",
        "you can ignore it.\n",
        "\n"
      ],
      "metadata": {
        "id": "dXIIxdUDaQF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "id": "OeapNyxaaeYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytg9sReD8fRd"
      },
      "outputs": [],
      "source": [
        "# Import the packages we will need.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from tabulate import tabulate\n",
        "from stockpyl.supply_chain_network import single_stage_system\n",
        "from stockpyl.newsvendor import newsvendor_poisson\n",
        "from stockpyl import sim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Environment\n",
        "\n",
        "The code below creates an environment class for the MPNV. The class implements functions `reset()` and `step()` to initialize the environment and simulate one time step. \n",
        "\n",
        "* `reset()` returns the initial state of the environment\n",
        "* `step(action)` takes the action specified and returns a tuple `(new_state, reward, done)`, where `done` is a flag indicating whether the episode has terminated\n",
        "\n",
        "(If you are familiar with OpenAI `gym`, these functions will be familiar to you.)\n",
        "\n",
        "The code is missing some pieces. Your job is to fill in the missing pieces.\n",
        "\n",
        "---\n",
        "> **Note:** In the code below, the portions that you need to complete are marked with\n",
        "> \n",
        "> ```python\n",
        "> # #################\n",
        "> # TODO:\n",
        "> ```\n",
        "> \n",
        "> In place of the missing code is a line that says \n",
        "> \n",
        "> ```python\n",
        "> \traise NotImplementedError\n",
        "> ```\n",
        "> \n",
        "> This is a way of telling Python to raise an exception (error) because there's something missing here. You should **delete (or comment out) this line** after you write your code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V7Y48VMEa5SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNVEnv(object):\n",
        "    \"\"\"Multi-period newsvendor (MPNV) problem environment. A state represents an inventory level. \n",
        "    An action is an order quantity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : SupplyChainNetwork\n",
        "        The network to simulate.\n",
        "    episode_length : int\n",
        "        The number of periods in one episode.\n",
        "    min_state : int\n",
        "        The minimum value of the state space to consider.\n",
        "    max_state : int\n",
        "        The maximum value of the state space to consider.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network, episode_length: int, min_state: int, max_state: int, \\\n",
        "        gamma: float = 0.95):\n",
        "\n",
        "        # Store problem data.\n",
        "        self.network = network\n",
        "        self.episode_length = episode_length\n",
        "        self.min_state = min_state\n",
        "        self.max_state = max_state\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Build list of state and action spaces. State space is specified by min_state and max_state.\n",
        "        # Action space is the largest possible order quantity, i.e., starting with IL = min_state\n",
        "        # and ordering up to max_state.\n",
        "        self.state_space = list(range(min_state, max_state + 1))\n",
        "        self.action_space = list(range(max_state - min_state + 1))\n",
        "\n",
        "        # Start the system in state 0.\n",
        "        self.initial_state = 0\n",
        "\n",
        "        # Calculate allowable actions for each state. \n",
        "        # In state x, allowable actions are {0, ..., x_max - x}, where x_max is\n",
        "        # the upper range of the state space. \n",
        "        self.allowable_actions = {}\n",
        "        for s in self.state_space:\n",
        "            min_a = 0\n",
        "            max_a = max(self.state_space) - s\n",
        "            self.allowable_actions[s] = list(range(min_a, max_a + 1))\n",
        "\n",
        "        # Initialize current state info.\n",
        "        self.state = None\n",
        "\n",
        "        # Get a shortcut to the (single) node in the network, for convenience.\n",
        "        self.node = network.nodes[0]\n",
        "\n",
        "    def get_random_action(self, state):\n",
        "        \"\"\"Return a randomly selected action, with equal probability, for the\n",
        "        given state, chosen from the state's allowable actions.\"\"\"\n",
        "        # #################\n",
        "        # Set `action` to an action chosen randomly from among the allowable actions for the\n",
        "        # state. (Hint: self.allowable_actions[state] gives a list of allowable\n",
        "        # actions.)\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_greedy_action(self, state, Q):\n",
        "        \"\"\"Return a greedy action, i.e., an action `a` that maximizes `Q[state, a]`.\"\"\"\n",
        "        # #################\n",
        "        # Set `action` to an action that maximizes `Q[state, . ]`.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_epsilon_greedy_action(self, state, Q, epsilon):\n",
        "        \"\"\"Return an epsilon-greedy action, i.e., with probability epsilon choose a\n",
        "        random action from the allowable actions for the state, and with probability\n",
        "        1 - epsilon choose a greedy action for the state. i.e., an action `a` that \n",
        "        maximizes `Q[state, a]`.\"\"\"\n",
        "        # #################\n",
        "        # Set `action`: with probability epsilon, choose a random action, and\n",
        "        # with probability 1 - epsilon, choose a greedy action. \n",
        "        raise NotImplementedError\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_greedy_policy(self, Q):\n",
        "        \"\"\"Return a greedy policy, i.e., in which the action `a` for every state `s`\n",
        "        maximizes `Q[state, a]`.\"\"\"\n",
        "        return {s: self.get_greedy_action(s, Q) for s in self.state_space}\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment and the simulation. Set the state to the initial\n",
        "        state. Also set the corresponding attribute of the SupplyChainNode.\"\"\"\n",
        "\n",
        "        # Determine initial IL and store it in environment's state.\n",
        "        self.state = self.initial_state\n",
        "        \n",
        "        # Set node's initial IL attribute. (This will force the simulation to \n",
        "        # start with the node at this inventory level.)\n",
        "        self.node.initial_inventory_level = self.initial_state\n",
        "\n",
        "        # Reset the simulation environment.\n",
        "        sim.initialize(self.network, self.episode_length)\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Run one time step of the environment by taking the specified action.\n",
        "        Update the environment state to the new state.\n",
        "        Return a tuple (new_state, reward, terminated, info).\"\"\"\n",
        "\n",
        "        # #################\n",
        "        # Build a dict specifying the order quantity to be used in this time\n",
        "        # period. The dict should have a single key (equal to `self.node`)\n",
        "        # and a single value (equal to the order quantity). \n",
        "        # Build dict specifying order quantity to use in this time period.\n",
        "        # (This dict will be used to override the order quantity that the \n",
        "        # stockpyl simulation would choose on its own.)\n",
        "        # IMPORTANT: If the order quantity would make the inventory level \n",
        "        # greater than `max_state`, truncate it so the inventory level equals\n",
        "        # `max_state`.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # #################\n",
        "        # Simulate one time period by calling the stockpyl function \n",
        "        # `sim.step()`. The signature of this function is:\n",
        "        #\n",
        "        #  def step(network, order_quantity_override=None, consistency_checks='W')\n",
        "        #\n",
        "        #   * Set `network` to the network object to be simulated. (Hint: it's \n",
        "        # stored in the MPNVEnv object.)\n",
        "        #   * Set `order_quantity_override` to the dict that you defined above.\n",
        "        #   * Set `consistency_checks` to 'N' (including the single-quotes). \n",
        "        # (This just speeds up the simulation a bit.)\n",
        "        #   * The function does not return any values.    \n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Determine reward by querying the simulation's state variables.\n",
        "        reward = -self.node.state_vars_current.total_cost_incurred\n",
        "\n",
        "        # #################\n",
        "        # Determine whether the episode length has been reached, and if so,\n",
        "        # set `done` to True. (Hint: The current period of the simulation\n",
        "        # is stored in `self.network.period`.)\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Get the new inventory level from the simulation. (Also round it to\n",
        "        # an int -- it should already be integer but sometimes there are small \n",
        "        # numerical errors.)\n",
        "        IL = int(self.node.state_vars_current.inventory_level)\n",
        "  \n",
        "        # #################\n",
        "        # It is possible that the new IL is outside of the bounds of the\n",
        "        # state space. If it is, set it equal to the bound.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # #################\n",
        "        # Set the environment's state variable (self.inventory_level) equal to \n",
        "        # the new IL.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Fill the demand into the info dict.\n",
        "        info = {'demand': self.node.state_vars_current.inbound_order[None]}\n",
        "\n",
        "        # Return a tuple (new_state, reward, done, info).\n",
        "        return new_state, reward, done, info\n",
        "\n",
        "    def play_episode(self, policy, messages=False):\n",
        "        \"\"\"Play one episode of the environment following the specified policy. \n",
        "        Return the total discounted reward over the episode.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize environment.\n",
        "        self.reset()\n",
        "        cumul_reward = 0\n",
        "\n",
        "        if messages:\n",
        "            print(f\"policy = {policy}\")\n",
        "            print(f\"Initial state = {self.state}, total reward = {cumul_reward}\")\n",
        "\n",
        "        # Step through until terminal state reached.\n",
        "        for t in range(self.episode_length):\n",
        "            \n",
        "            # Determine action.\n",
        "            action = policy[self.state]\n",
        "\n",
        "            if messages:\n",
        "                print(f\"timestep {t:6} state = {self.state:4} action = {action:4} \", end=\"\")\n",
        "\n",
        "            # Step.\n",
        "            new_state, reward, done, info = self.step(action)\n",
        "\n",
        "            # Update cumulative reward.\n",
        "            cumul_reward += self.gamma**t * reward\n",
        "\n",
        "            if messages:\n",
        "                print(f\"demand = {info['demand']:4} new_state = {new_state:4} reward = {reward:8.2f} total reward = {cumul_reward:8.2f}\")\n",
        "\n",
        "        return cumul_reward\n",
        "\n",
        "    def play_episode_batch(self, policy, num_episodes):\n",
        "        \"\"\"Play `num_episodes` episode of the environment following the specified policy. \n",
        "        Return the average total discounted reward over all episodes.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\"\"\"\n",
        "\n",
        "        avg_total_reward = 0\n",
        "        for _ in range(num_episodes):\n",
        "            self.reset()\n",
        "            total_reward = self.play_episode(policy, messages=False)\n",
        "            avg_total_reward += total_reward / num_episodes\n",
        "\n",
        "        return avg_total_reward"
      ],
      "metadata": {
        "id": "fyiDsHkocwa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function plots a given policy in two ways: order quantity vs. inventory level and order-up-to level (= inventory level + order quantity) vs. inventory level."
      ],
      "metadata": {
        "id": "p09GZHyg_wuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy(env: MPNVEnv, policy: dict, title: str = None):\n",
        "\t\"\"\"Plot the policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tpi : \n",
        "\t\tProbability distribution for a policy. A dict whose keys are states and\n",
        "\t\twhose values are actions. (Note that this is a different structure\n",
        "\t\tthan what was used in the \"MPNV as MDP\" notebook.)\n",
        "\ttitle : \n",
        "\t\tOptional title for the figure.\n",
        "\t\"\"\"\n",
        "\n",
        "\tfig = plt.figure(figsize=plt.figaspect(1/2))\n",
        "\tfig.suptitle(title)\n",
        "\n",
        "\t# Order quantity plot.\n",
        "\tax = plt.subplot(121)\n",
        "\tx_list = env.state_space\n",
        "\ty_list = [policy[x] for x in x_list]\n",
        "\tplt.plot(x_list, y_list)\n",
        "\tplt.xlabel('Starting Inventory Level')\n",
        "\tplt.ylabel('Order Quantity')\n",
        "\n",
        "\t# Order-up-to level plot.\n",
        "\tax = plt.subplot(122)\n",
        "\ty_list = [x + policy[x] for x in x_list]\n",
        "\tplt.plot(x_list, y_list)\n",
        "\tplt.xlabel('Starting Inventory Level')\n",
        "\tplt.ylabel('Order-Up-To Level')\n",
        "\n",
        "\tplt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "P19CPSZkvbze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular RL\n",
        "\n",
        "Now our goal is to implement one or more tabular RL algorithms and apply them to the MPNV environment you built above.\n",
        "\n",
        "---\n",
        "> At this point you have three choices:\n",
        "> 1. Plug in tabular RL code that you wrote earlier in the workshop or at some other point.\n",
        "> 2. Use RL code from an external Python package or repo.\n",
        "> 3. Use my RL code that is already pasted below.\n",
        ">\n",
        "> If you go with option 1 or 2, your tabular algorithm must be able to interact with the `MPNVEnv` class you built above. In particular, it should call the `reset()` and `step()` functions to initialize the environment and to take one step through the simulation. (This is a pretty standard API structure, and many RL codes use it.) Your code can also access other features we built into the `MPNVEnv` class, such as `allowable_actions`, `get_epsilon_greedy_action()`, etc., but that is optional.\n",
        ">\n",
        "> Option 3 is probably the simplest, since the work is already done. However, my RL code is extremely no-frills; it is not as robust or efficient as many existing RL implementations. \n",
        "---\n",
        "\n",
        "Here is my tabular RL code:"
      ],
      "metadata": {
        "id": "ALSyN7pu_z2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TabularRL():\n",
        "    \"\"\"Class for tabular reinforcement learning for MPNV problem. (The code is fairly generic, but it assumes\n",
        "    data structures and state/action encodings that are specific to the MPNV.)\n",
        "    \"\"\"\n",
        "    def __init__(self, env: MPNVEnv = None):\n",
        "        self.env = env\n",
        "\n",
        "    def sarsa(self,\n",
        "            num_episodes: int,\n",
        "            initial_Q: dict = None,\n",
        "            epsilon: float = 0.1,\n",
        "            step_size: float = 0.5,     # alpha\n",
        "            progress_bar: bool = True,\n",
        "            messages: bool = False\n",
        "        ):\n",
        "        \"\"\"Sarsa (on-policy control) for estimating Q ~ q_* (Sutton and Barto, Section 6.5, p. 130.).\n",
        "        Returns Q and the final policy.\"\"\"\n",
        "        \n",
        "        # Initialize.\n",
        "        Q = initial_Q if initial_Q else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "\n",
        "        # Intialize progress bar.\n",
        "        pbar = tqdm(total=num_episodes, disable=not progress_bar)\n",
        "\n",
        "        # Loop through episodes.\n",
        "        for _ in range(num_episodes):\n",
        "\n",
        "            # Update progress bar.\n",
        "            pbar.update()\n",
        "\n",
        "            # Reset the environment and get first action.\n",
        "            S = self.env.reset()\n",
        "            A = self.env.get_epsilon_greedy_action(S, Q, epsilon)\n",
        "\n",
        "            # Loop through time steps.\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                # Take action.\n",
        "                S_prime, R, done, info = self.env.step(A)\n",
        "\n",
        "                # Choose A' from S'.\n",
        "                A_prime = self.env.get_epsilon_greedy_action(S_prime, Q, epsilon)\n",
        "\n",
        "                # Update Q.\n",
        "                Q[S, A] += step_size * (R + self.env.gamma * Q[S_prime, A_prime] - Q[S, A])\n",
        "\n",
        "                if messages:\n",
        "                    print(f\"S = {S} A = {A} R = {R} S' = {S_prime} A' = {A_prime}\")\n",
        "\n",
        "                # Update S and A.\n",
        "                S = S_prime\n",
        "                A = A_prime\n",
        "\n",
        "        return Q, self.env.get_greedy_policy(Q)\n",
        "\n",
        "    def Q_learning(self,\n",
        "            num_episodes: int,\n",
        "            initial_Q: dict = None,\n",
        "            epsilon: float = 0.1,\n",
        "            step_size: float = 0.5,     # alpha\n",
        "            progress_bar: bool = True,\n",
        "            messages: bool = False,\n",
        "            test_freq: int = None,\n",
        "            test_episodes: int = None\n",
        "        ):\n",
        "        \"\"\"Q-learning (off-policy control) for estimating pi ~ pi_* (Section 6.5, p. 130.).\n",
        "        Returns Q and the final policy.\n",
        "        \n",
        "        If ``test_freq`` is not None, the current policy is tested every ``test_freq`` episodes by\n",
        "        running ``test_epidodes`` episodes and printing the average reward. (This is mostly for \n",
        "        tracking the progress of the training while it's going on.)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize.\n",
        "        Q = initial_Q if initial_Q else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "\n",
        "        # Intialize progress bar.\n",
        "        pbar = tqdm(total=num_episodes, disable=not progress_bar)\n",
        "\n",
        "        # Loop through episodes.\n",
        "        for ep in range(num_episodes):\n",
        "\n",
        "            # Update progress bar.\n",
        "            pbar.update()\n",
        "\n",
        "            # Test.\n",
        "            if test_freq and ep % test_freq == 0:\n",
        "                pol = self.env.get_greedy_policy(Q)\n",
        "                avg_reward = self.env.play_episode_batch(policy=pol, num_episodes=test_episodes)\n",
        "                tqdm.write(f\"Training episode {ep:8d}, ran {test_episodes} episodes, average reward per episode = {avg_reward:8.4f}\")\n",
        "\n",
        "            # Reset the environment.\n",
        "            S = self.env.reset()\n",
        "\n",
        "            # Loop through time steps.\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                # Choose action.\n",
        "                A = self.env.get_epsilon_greedy_action(S, Q, epsilon)\n",
        "\n",
        "                # Take action.\n",
        "                S_prime, R, done, info = self.env.step(A)\n",
        "\n",
        "                # Update Q.\n",
        "                max_Q = max([Q[S_prime, a] for a in self.env.allowable_actions[S_prime]])\n",
        "                Q[S, A] += step_size * (R + self.env.gamma * max_Q - Q[S, A])\n",
        "\n",
        "                if messages:\n",
        "                    print(f\"S = {S} A = {A} R = {R} S' = {S_prime} max_Q = {max_Q}\")\n",
        "\n",
        "                # Update S.\n",
        "                S = S_prime\n",
        "\n",
        "        return Q, self.env.get_greedy_policy(Q)\n",
        "\n",
        "\n",
        "    def double_Q_learning(self,\n",
        "            num_episodes: int,\n",
        "            initial_Q1: dict = None,\n",
        "            initial_Q2: dict = None,\n",
        "            epsilon: float = 0.1,\n",
        "            step_size: float = 0.5,     # alpha\n",
        "            progress_bar: bool = True,\n",
        "            messages: bool = False\n",
        "        ):\n",
        "        \"\"\"Double Q-learning for estimating Q_1 ~ Q_2 ~ q_* (Section 6.7, p. 136.).\n",
        "        Returns Q and the final policy.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize.\n",
        "        Q1 = initial_Q1 if initial_Q1 else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "        Q2 = initial_Q2 if initial_Q2 else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "\n",
        "        # Intialize progress bar.\n",
        "        pbar = tqdm(total=num_episodes, disable=not progress_bar)\n",
        "\n",
        "        # Loop through episodes.\n",
        "        for _ in range(num_episodes):\n",
        "\n",
        "            # Update progress bar.\n",
        "            pbar.update()\n",
        "\n",
        "            # Choose initial state.\n",
        "            S = self.env.get_initial_state()\n",
        "\n",
        "            # Reset the environment.\n",
        "            self.env.reset()\n",
        "\n",
        "            # Loop through time steps.\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                # Choose action.\n",
        "                Q_sum = {(s, a): Q1[s, a] + Q2[s, a] for s, a in Q1.keys()}\n",
        "                A = self.env.get_epsilon_greedy_action(S, Q_sum, epsilon)\n",
        "\n",
        "                # Take action.\n",
        "                S_prime, R, done, info = self.env.step(A)\n",
        "\n",
        "                # Update Q.\n",
        "                if np.random.random() < 0.5:\n",
        "                    # Get a that maximizes Q1[S', a].\n",
        "                    Q_S_prime = {a: Q1[S_prime, a] for a in self.env.allowable_actions[S_prime]}\n",
        "                    argmax_Q = max(Q_S_prime, key=Q_S_prime.get)\n",
        "                    Q1[S, A] += step_size * (R + self.env.gamma * Q2[S_prime, argmax_Q] - Q1[S, A])\n",
        "                else:\n",
        "                    Q_S_prime = {a: Q2[S_prime, a] for a in self.env.allowable_actions[S_prime]}\n",
        "                    argmax_Q = max(Q_S_prime, key=Q_S_prime.get)\n",
        "                    Q2[S, A] += step_size * (R + self.env.gamma * Q1[S_prime, argmax_Q] - Q2[S, A])\n",
        "\n",
        "                if messages:\n",
        "                    print(f\"S = {S} A = {A} R = {R} S' = {S_prime} argmax_Q = {argmax_Q}\")\n",
        "\n",
        "                # Update S.\n",
        "                S = S_prime\n",
        "            \n",
        "        Q_sum = {(s, a): Q1[s, a] + Q2[s, a] for s, a in Q1.keys()}\n",
        "        return Q_sum, self.env.get_greedy_policy(Q_sum)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "NKnNvRQGvd_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Instance\n",
        "\n",
        "Let's use the same instance we used in the \"MPNV as MDP\" notebook:\n",
        "\n",
        "* $h = 1$\n",
        "* $p = 10$\n",
        "* $\\mu = 5$\n",
        "\n"
      ],
      "metadata": {
        "id": "zJj8KeyiVyn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build stockpyl SupplyChainNetwork object.\n",
        "network = single_stage_system(\n",
        "    local_holding_cost=1,\n",
        "    stockout_cost=10,\n",
        "    demand_type='P',\n",
        "    mean=5\n",
        ")"
      ],
      "metadata": {
        "id": "VoLvgiEuvyUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the MDP notebook, we set the state space limits to $[-15, 20]$. Here we'll use a narrow range in order to help the RL agent train more quickly:"
      ],
      "metadata": {
        "id": "cUcqFZ8FWagc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_state = -10\n",
        "max_state = 14"
      ],
      "metadata": {
        "id": "IRqXIvULW21d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and testing parameters."
      ],
      "metadata": {
        "id": "2u55g2hxXKmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_length = 500\n",
        "num_training_episodes = 2000\n",
        "num_testing_episodes = 100\n",
        "in_training_test_freq = 1000\n",
        "in_training_test_episodes = 100\n",
        "trl_epsilon = 0.05\n",
        "trl_step_size = 0.1"
      ],
      "metadata": {
        "id": "I0o-umG2vp-J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's build the `MPNVEnv` and `TDLearning` objects."
      ],
      "metadata": {
        "id": "XESXIFZMXbg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MPNVEnv object.\n",
        "env = MPNVEnv(\n",
        "    network=network,\n",
        "    episode_length=episode_length,\n",
        "    min_state=min_state,\n",
        "    max_state=max_state,\n",
        "    gamma=0.95\n",
        ")"
      ],
      "metadata": {
        "id": "t_V7n7b4v0Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build tabular RL object.\n",
        "trl = TabularRL(env)"
      ],
      "metadata": {
        "id": "TImGPz3iv2w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Let's train the RL agent! If you used my tabular RL code, you can choose either the `sarsa()`, `Q_learning()`, or `double_Q_learning()` functions. If you used your own code, plug in the appropriate call here.\n",
        "\n",
        "This will take several minutes (or more, depending on your settings) to execute."
      ],
      "metadata": {
        "id": "rv0Crh5VXygx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run tabular RL algorithm.\n",
        "#\tQ, pol = td.sarsa(\n",
        "Q, pol = trl.Q_learning(\n",
        "#\tQ, pol = td.double_Q_learning(\n",
        "    num_episodes=num_training_episodes,\n",
        "    epsilon=trl_epsilon,\n",
        "    step_size=trl_step_size,\n",
        "    progress_bar=True,\n",
        "    messages=False,\n",
        "    test_freq=in_training_test_freq,\n",
        "    test_episodes=in_training_test_episodes\n",
        ")"
      ],
      "metadata": {
        "id": "gp2Izr-Hv6MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Results\n",
        "\n",
        "We'll plot the policy below. Remember that we're hoping to see a **base-stock policy,** which will be evident if the plot on the left (order quantity vs. inventory level) should decrease linearly with a slope of $-1$ until it flattens out at $y=0$; and the plot on the right (order-up-to level vs. inventory level) should be flat at first and then increase linearly with a slope of $1$."
      ],
      "metadata": {
        "id": "U3CYnAB6Y_Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot policy.\n",
        "title=f\"{num_training_episodes} training episodes, {num_testing_episodes} testing episodes, {episode_length} periods per episode\"\n",
        "plot_policy(env=env, policy=pol, title=title)\n"
      ],
      "metadata": {
        "id": "AbzPygPXwI3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your RL agent performed similarly to mine, the plot is base-stock-ish, but not quite a base-stock policy. That's OK—we did a quicky and dirty training. More careful and longer training will result in a more accurate solution."
      ],
      "metadata": {
        "id": "58HFaSvlZbFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> If you wish, play around with the hyperparameters to try to improve the training process.\n",
        "---"
      ],
      "metadata": {
        "id": "NVHiY9S_ZrBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also test the policy by playing a batch of episodes."
      ],
      "metadata": {
        "id": "ckQmaMgAYr1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test policy.\n",
        "avg_reward = env.play_episode_batch(pol, num_testing_episodes)\n",
        "print(f\"Average reward per episode = {avg_reward}\")\n"
      ],
      "metadata": {
        "id": "XmS_k4gQwAZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that the problem can be solved analytically. Let's compare the RL agent's solution with the analytical solution. The code below reports both the optimal expected cost per period and the corresponding expected discount cost over the horizon. \n",
        "\n",
        "The expected discounted horizon cost calculation uses the fact that, as in the \"MPNV as MAB\" notebook, we can convert an expected cost per period, $g$, to an expected discounte cost over $T$ time periods, as follows:\n",
        "\n",
        "$$\\sum_{t=0}^{T-1} g\\gamma^i = g\\frac{1 - \\gamma^{T}}{1-\\gamma}$$\n"
      ],
      "metadata": {
        "id": "P0VjbU7kYyFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to analytical solution.\n",
        "n = network.nodes[0]\n",
        "opt_S, opt_cost = newsvendor_poisson(\n",
        "    holding_cost=n.local_holding_cost,\n",
        "    stockout_cost=n.stockout_cost,\n",
        "    demand_mean=n.demand_source.mean\n",
        ")\n",
        "exp_disc_cost = opt_cost * (1 - env.gamma**episode_length) / (1 - env.gamma)\n",
        "print(f\"Optimal base-stock level = {opt_S} with expected cost per period = {opt_cost:.4f} and expected discounted horizon cost {exp_disc_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "NKjIJ4nVwBMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also simulate the system using `stockpyl` to get an average cost per period."
      ],
      "metadata": {
        "id": "aUKL4fxz9FpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate system. \n",
        "# First we have to specify the inventory policy that the node\n",
        "# should use (base-stock with base-stock level 8).\n",
        "from stockpyl.policy import Policy\n",
        "network.nodes[0].inventory_policy = Policy(type='BS', base_stock_level=opt_S, node=network.nodes[0])\n",
        "# Next, we have to implement a small tweak to account for a difference between\n",
        "# our sequence of events and stockpyl's.\n",
        "network.nodes[0].shipment_lead_time = 1\n",
        "# Now do the simulation.\n",
        "avg_sim_cost, _ = sim.run_multiple_trials(network, 10, 1000, progress_bar=False)\n",
        "print(f\"Average cost per period is {avg_sim_cost:4f}\")"
      ],
      "metadata": {
        "id": "NZaoAPRy9E33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize:"
      ],
      "metadata": {
        "id": "yhRQ2hmLEMf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate([\n",
        "    [\"Optimal base-stock level\", opt_S],\n",
        "    [\"Average discounted reward per episode from environment play()\", avg_reward],\n",
        "    [\"Optimal expected discounted reward per episode\", exp_disc_cost],\n",
        "    [\"Optimal expected cost per period\", opt_cost],\n",
        "    [\"Simulated average cost per period\", avg_sim_cost]\n",
        "]))"
      ],
      "metadata": {
        "id": "gtSPG04I81GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If You Have Extra Time\n",
        "\n",
        "As in the \"MPNV as MDP\" notebook, try adding a fixed cost $K$ to the environment. Train the RL algorithm using the same instance as before, plus $K=20$. Check whether the RL agent learned to follow an $(s,S)$ policy. \n",
        "\n",
        "Also use `stockpyl` to find the optimal $s$ and $S$, via the `s_s_discrete_exact()` function (see documentation [here](https://stockpyl.readthedocs.io/en/latest/api/seio/ss.html#stockpyl.ss.s_s_discrete_exact)). In particular, the following code solves the $(s,S)$ problem for a Poisson distribution with mean `mu`:\n",
        "\n",
        "```python\n",
        "from stockpyl.ss import s_s_discrete_exact\n",
        "s, S, cost = s_s_discrete_exact(holding_cost=h, stockout_cost=p, fixed_cost=K, use_poisson=True, demand_mean=mu)\n",
        "```\n",
        "\n",
        "(Unfortunately, `stockpyl` cannot (yet) handle fixed costs in the simulation, so you can't simulate this system.)\n",
        "\n"
      ],
      "metadata": {
        "id": "wZjvMPx0AntA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Or: Suppose we have a finite time horizon, e.g., $T$ periods. We know that the optimal policy is a base-stock policy, and the base-stock levels are the parameters of that policy. Typically, those parameters are set using DP, but why not try out some policy gradient methods, like those that you implemented earlier in the workshop?"
      ],
      "metadata": {
        "id": "xv0Mmcr6RD07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PefhsuheAoL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}