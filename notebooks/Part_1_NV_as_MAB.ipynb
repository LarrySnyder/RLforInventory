{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 1: NV as MAB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYarzon333QU1IkON+95ov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/notebooks/Part_1_NV_as_MAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Newsvendor Problem as a Multi-Armed Bandit (MAB)\n",
        "\n",
        "This notebook contains code for an MAB implementation of the newsvendor problem.\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filenameâ€”not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ulij2zKcmrrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nwlZJftXUK89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **newsvendor problem**, the goal is to choose an order quantity $Q$ to use in each time period, in order to minimize the expected cost per period, given by\n",
        "\n",
        "$$g(Q) = {\\mathbb E}\\left[h(Q-D)^+ + p(D-Q)^+\\right],$$\n",
        "\n",
        "where $h$ is the **holding cost** (aka overage cost, the cost per unit left over at the end of the day), $p$ is the **stockout cost** (aka underage cost, the cost per unit of unmet demand), $D$ is the **demand** (a random variable), and $z^+ \\equiv \\max\\{0, z\\}$.\n",
        "\n",
        "We'll assume the demand has a discrete probability distribution with pmf $f(d)$ and cdf $F(d)$, in which case\n",
        "\n",
        "$$g(Q) = h\\sum_{d=0}^Q (Q-d)f(d) + p\\sum_{d=Q}^\\infty (d-Q)f(d).$$\n",
        "\n",
        "This is the objective function for the newsvendor problem, which we wish to minimize. Equivalently, we can maximize the expected reward function (to stay consistent with RL and MAB terminology), which is the negative of the cost function:\n",
        "\n",
        "$$r(Q) = -g(Q).$$\n",
        "\n",
        "The optimal order quantity $Q^*$, which minimizes the expected cost or maximizes the expected reward, is the smallest $Q$ such that\n",
        "\n",
        "$$F(Q) \\ge \\frac{p}{p+h}.$$\n",
        "\n",
        "In this notebook, we will model the newsvendor problem as a **multi-armed bandit (MAB)**."
      ],
      "metadata": {
        "id": "lJy8OExCbHMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff"
      ],
      "metadata": {
        "id": "Uf0iIJL1bM4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtETQocUlivA"
      },
      "outputs": [],
      "source": [
        "# Import the packages we will need.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm, poisson"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, we'll use the `stockpyl` Python package (https://pypi.org/project/stockpyl/) for inventory optimization stuff. We have to install the `stockpyl` package ourselves. (It doesn't come pre-installed on Colab like `numpy`, etc. do.) You should only need to do this once per notebook.\n",
        "\n",
        "If you get a message like\n",
        "\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [sphinxcontrib]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```\n",
        "\n",
        "you can ignore it.\n"
      ],
      "metadata": {
        "id": "mvYa1KQUnuVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "id": "0-KOixlOny1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stockpyl import newsvendor"
      ],
      "metadata": {
        "id": "tQq454GKoC7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bandit Class\n",
        "\n",
        "First, we'll define a `Bandit` class that implements a generic multi-armed bandit (MAB). \n",
        "\n",
        "The class is very simple. It has two attributes:\n",
        "\n",
        "* `k`: the number of arms\n",
        "* `mean`: a list of mean rewards, one per bandit\n",
        "* `sd`: a list of standard deviations of rewards, one per bandit\n",
        "\n",
        "And the class has two methods:\n",
        "\n",
        "* `__init__()` initializes the class\n",
        "* `pull()` takes an action and returns a randomly generated reward for that action\n",
        "\n",
        "At its default values, the bandit has $k=5$ arms whose rewards have mean $[1, \\ldots, 5]$ (respectively) and standard deviation 1."
      ],
      "metadata": {
        "id": "SslqScfqbukw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bandit(object):\n",
        "\n",
        "    def __init__(self, k: int = 5, mean: list = list(range(5)), sd: list = [1]*5):\n",
        "        \"\"\"Initialize the attributes.\"\"\"\n",
        "        self.k = k\n",
        "        self.mean = mean\n",
        "        self.sd = sd\n",
        "\n",
        "    def pull(self, action: int):\n",
        "        \"\"\"Get a random variate from a normal distribution with the mean and SD\n",
        "        corresponding to the action.\"\"\"\n",
        "        return norm.rvs(loc=self.mean[action], scale=self.sd[action])\n"
      ],
      "metadata": {
        "id": "436FfGBHdXqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give it a spin ðŸŽ°. \n",
        "\n",
        "(Sorryâ€”dad joke.) "
      ],
      "metadata": {
        "id": "zBFNKQJ-eFsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bandit = Bandit(k=3, mean=[5, 3, 1], sd=[1, 1, 0.5])\n",
        "for _ in range(10):\n",
        "    a = np.random.randint(3)\n",
        "    r = bandit.pull(a)\n",
        "    print(f\"Pulled arm {a}, got reward {r}\")"
      ],
      "metadata": {
        "id": "g-0IjixGePss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\epsilon$-Greedy Class\n",
        "\n",
        "Next we'll define an `EpsilonGreedyAgent` class that implements the $\\epsilon$-greedy algorithm for generic MABs. The algorithm implementation is based on the discussions in Sutton and Barto (2nd edition, 2018).\n",
        "\n",
        "In order to use the class, you need to provide it with an instance of the `Bandit` class defined above.\n",
        "\n",
        "Feel free to explore the code if you want, but all that's required is for you to execute the cell."
      ],
      "metadata": {
        "id": "totid_CXbVgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyAgent(object):\n",
        "\n",
        "    def __init__(self, bandit: Bandit, epsilon: float = 0.1):\n",
        "        # Initialize the attributes.\n",
        "        self.bandit = bandit\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def epsilon_greedy(self, num_time_steps: int = 1000, initial_Q: list = None):\n",
        "        # Initialize Q-value estimates (or use `initial_Q` if provided).\n",
        "        Q = initial_Q or [0] * self.bandit.k\n",
        "        # Initialize action counts.\n",
        "        N = [0] * self.bandit.k\n",
        "\n",
        "        # Initialize evaluation info.\n",
        "        \n",
        "        # Main loop.\n",
        "        for t in range(num_time_steps):\n",
        "\n",
        "            # Choose action.\n",
        "            if np.random.rand() < 1 - self.epsilon:\n",
        "                A = np.argmax(Q)\n",
        "            else:\n",
        "                A = np.random.randint(self.bandit.k)\n",
        "\n",
        "            # Get reward.\n",
        "            R = self.bandit.pull(A)\n",
        "\n",
        "            # Update stats.\n",
        "            N[A] += 1\n",
        "            Q[A] += (1 / N[A]) * (R - Q[A])\n",
        "\n",
        "        # Return Q estimates as well as best guess for optimal action.\n",
        "        return Q, np.argmax(Q)"
      ],
      "metadata": {
        "id": "FmUJRaMVbRtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it."
      ],
      "metadata": {
        "id": "kGibmGgOhVQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = EpsilonGreedyAgent(bandit)\n",
        "Q, A = agent.epsilon_greedy()\n",
        "print(f\"Best guess for optimal action is {A}\")\n",
        "print(f\"Estimates of action values:\")\n",
        "for a in range(bandit.k):\n",
        "    print(f\"  {a}: {Q[a]}\")"
      ],
      "metadata": {
        "id": "vMI4FlKHhTPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Newsvendor Bandit\n",
        "\n",
        "Now it's your turn. Your goal is to build a class called `NewsvendorBandit`. I started you off by building the structure of the class. (It's similar to the `Bandit` class declared earlier.) You need to fill in some details.\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "* The `NewsvendorBandit` class takes a parameter `k`, like the `Bandit` class, that indicates the number of \"arms\". The arms will be indexed $a=0,\\ldots,k-1$, and arm $a$ corresponds to using an order quantity of $a$.\n",
        "* The class takes three parameters specifying the newsvendor problem instance: \n",
        "\n",
        "    * `h` and `p` are the holding and stockout costs\n",
        "    * `mu` is the mean of the Poisson demand distribution\n",
        "    \n",
        "* \"Pulling\" an arm should return the **negative of the cost of one newsvendor period,** based on a randomly generated demand, rather than returning a random variate from a particular distribution.\n",
        "\n",
        "---\n",
        "> **Note:** In the code below, the portions that you need to complete are marked with\n",
        "> \n",
        "> ```python\n",
        "> # #################\n",
        "> # TODO:\n",
        "> ```\n",
        "> \n",
        "> In place of the missing code is a line that says \n",
        "> \n",
        "> ```python\n",
        "> \traise NotImplementedError\n",
        "> ```\n",
        "> \n",
        "> This is a way of telling Python to raise an exception (error) because there's something missing here. You should **delete (or comment out) this line** after you write your code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xqpmsNTGiS5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsvendorBandit(object):\n",
        "\n",
        "    def __init__(self, k: int = 10, h: float = 1, p: float = 10, mu: int = 5):\n",
        "        \"\"\"Initialize the attributes.\"\"\"\n",
        "        self.k = k\n",
        "\n",
        "        # #################\n",
        "        # TODO: store the attributes h, p, and mu in the object, too.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def pull(self, action: int):\n",
        "        \"\"\"Return a random newsvendor cost for the given action.\"\"\"\n",
        "\n",
        "        # Generate a Poisson(mu) random variate.\n",
        "        d = poisson.rvs(self.mu)\n",
        "\n",
        "        # #################\n",
        "        # TODO: Calculate the cost for the chosen action and the random demand.\n",
        "        # Set `reward` to the negative of this cost.\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Get a random variate from a normal distribution with the mean and SD\n",
        "        # corresponding to the action.\n",
        "        return reward"
      ],
      "metadata": {
        "id": "o3KQ7tiDj5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try out your `NewsvendorBandit` class on a newsvendor instance with:\n",
        "\n",
        "* $h=0.5$\n",
        "* $p=15$\n",
        "* $\\mu=4$\n",
        "\n",
        "We'll use 12 arms."
      ],
      "metadata": {
        "id": "iWGvFHxmpL7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the bandit.\n",
        "num_arms = 12\n",
        "bandit = NewsvendorBandit(k=num_arms, h=0.5, p=15, mu=4)"
      ],
      "metadata": {
        "id": "Ge8dh2PAsoXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull lever 5 a few times.\n",
        "a = 5\n",
        "for _ in range(10):\n",
        "    r = bandit.pull(a)\n",
        "    print(f\"Pulled arm {a}, got reward {r}\")"
      ],
      "metadata": {
        "id": "e5yLPe5rsqjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull it a lot of times and get the average reward.\n",
        "avg_reward = 0\n",
        "a = 5\n",
        "num_pulls = 10000\n",
        "for _ in range(num_pulls):\n",
        "    r = bandit.pull(a)\n",
        "    avg_reward += r / num_pulls\n",
        "print(f\"Pulled arm {a} {num_pulls} times, got average reward {avg_reward}\")"
      ],
      "metadata": {
        "id": "a8IlvDjppE-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's validate this using `stockpyl` by calculating the expected cost of using an order quantity of 5 (`stockpyl` calls this `base_stock_level`) for the given newsvendor instance."
      ],
      "metadata": {
        "id": "aR6s9gufqaRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, exp_cost = newsvendor.newsvendor_poisson(\n",
        "    bandit.h,\n",
        "    bandit.p,\n",
        "    bandit.mu,\n",
        "    base_stock_level=a\n",
        ")\n",
        "print(f\"Exact expected cost for order quantity 5 is {exp_cost}\")"
      ],
      "metadata": {
        "id": "gPs7W_bup1zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's get the *optimal* order quantity using `stockpyl`."
      ],
      "metadata": {
        "id": "XjX6zNg-tM0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_Q, opt_exp_cost = newsvendor.newsvendor_poisson(\n",
        "    bandit.h,\n",
        "    bandit.k,\n",
        "    bandit.mu\n",
        ")\n",
        "print(f\"Optimal order quantity is {opt_Q}, with expected cost {opt_exp_cost}\")"
      ],
      "metadata": {
        "id": "9KgwktR5tKSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Note:** Before proceeding, you should make sure that the results from your bandit are similar to those returned by `stockpyl`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KT-s9Y0Etfs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the $\\epsilon$-Greedy Agent for the Newsvendor MAB\n",
        "\n",
        "Now let's train the $\\epsilon$-greedy agent on the newsvendor MAB. The `EpsilonGreedyAgent` class does not need any modificationsâ€”we built it to be very genericâ€”so all you need to do is pass your newsvendor bandit to it."
      ],
      "metadata": {
        "id": "ZJ9vRzGsrhBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# TODO: Train the epsilon-greedy agent on your `NewsvendorBandit` object.\n",
        "# Print the agent's buess guess for the optimal action, as well as its\n",
        "# estimates of the action values. (Use the analogous cell above as a template.)\n",
        "raise NotImplementedError"
      ],
      "metadata": {
        "id": "jeCiauAFq9r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating the Results\n",
        "\n",
        "Let's compare your trained agent's action-value estimates (or really, their negatives) with the true expected costs of those actions as calculated by `stockpyl`. \n",
        "\n",
        "(The code below assumes that you have stored the agent's estimates of the action values in a variable called `Q` in the previous cell.)"
      ],
      "metadata": {
        "id": "tBkBYNwTuhGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_list = list(range(bandit.k))\n",
        "\n",
        "bandit_cost = [-Q[a] for a in action_list]\n",
        "exp_cost = [newsvendor.newsvendor_poisson(\n",
        "    bandit.h, \n",
        "    bandit.p, \n",
        "    bandit.mu, \n",
        "    base_stock_level=a\n",
        ")[1] for a in action_list]\n",
        "\n",
        "plt.scatter(action_list, bandit_cost, label='Bandit Cost Estimates')\n",
        "plt.scatter(action_list, exp_cost, label='True Expected Cost')\n",
        "plt.legend()\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Cost');"
      ],
      "metadata": {
        "id": "noxioRjzuaS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How good a job did your bandit and $\\epsilon$-greedy agent do of estimating the expected cost function for the newsvendor problem?\n",
        "\n",
        "(If you're not happy with the results, you can try increasing the `num_time_steps` parameter passed to the `epsilon_greedy()` method.)"
      ],
      "metadata": {
        "id": "cy5Wrkekw-BL"
      }
    }
  ]
}