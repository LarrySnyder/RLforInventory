{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOLUTIONS 4b: Beer Game DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfmT5HQuspCZSh1i+7zug2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/solutions/SOLUTIONS_4b_Beer_Game_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOLUTIONS: DQN for the Beer Game\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filename—not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University.\n",
        "---"
      ],
      "metadata": {
        "id": "4REAg0yRBTmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following Oroojlooyjadid, et al. (2022), we'll consider the following 4-node series system:\n",
        "\n",
        "![beer game system](https://raw.githubusercontent.com/LarrySnyder/RLforInventory/main/images/beer-game-schematic.png)\n",
        "\n",
        "The long-run systemwide expected cost is given by\n",
        "\n",
        "$$\\sum_{t=1}^T \\sum_{i=1}^4 h^i(IL_t^i)^+ + p^i(-IL_t^i)^+$$\n",
        "\n",
        "where $h^i$ and $p^i$ are the holding and stockout costs at node $i$, $IL_t^i$ is the inventory level at node $i$ at the end of period $t$, $T$ is the number of periods in one play of the game, and $z^+ \\equiv \\max\\{0,z\\}$.\n",
        "\n",
        "The inventory levels $IL_t^i$ are complicated random functions of the decision variables (i.e., the ordering policies), so this cost is difficult to formulate, let alone to optimize. Under certain assumptions (e.g., no fixed costs, stationary demands, etc.), and if there is a centralized decision maker who can make all of the ordering decisions, then a base-stock policy is optimal (Clark and Scarf 1960), and the optimal base-stock levels can be found relatively easily by optimizing a sequence of single-variable, convex problems (Chen and Zheng 1994).\n",
        "\n",
        "However, in the beer game, there is no centralized decision maker: Each node is controlled by a different player, each of whom make independent decisions about their ordering policies. Moreover, each player only knows the values of the state variables at their own node, not at the other nodes. The goal of our RL agent is to **choose order quantities at a single node to minimize the total systemwide cost under incomplete information.**"
      ],
      "metadata": {
        "id": "mZgYaBvFBbYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chess, Go, Atari, and other games that have successfully been solved by deep RL algorithms tend to have the following characteristics:\n",
        "\n",
        "* Competitive\n",
        "* Zero-sum\n",
        "* Full information\n",
        "* Instant reward signal (in some cases)\n",
        "\n",
        "But the beer game differs along all of these dimensions:\n",
        "\n",
        "* It is cooperative (the 4 players try to minimize their total cost)\n",
        "* It is not zero-sum (when one player succeeds, the whole team succeeds)\n",
        "* Players have only partial information (they have state information about only their own node)\n",
        "* The reward signal is delayed until the end of the game (since costs at other nodes are unknown during the game)\n",
        "\n",
        "Oroojlooyjadid, et al. (2022) propose an DQN-based algorithm they call the *shaped-reward DQN* (SRDQN). The SRDQN algorithm deals with the partial information by restricting the state variables that are available to the agent when making decisions. It deals with the delayed reward signal using **reward shaping,** which updates the reward information retroactively after the game ends. (We won't consider this in our simplified algorithm in this notebook, though.)"
      ],
      "metadata": {
        "id": "m5WGzhCaX3NJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff\n",
        "\n",
        "First we'll install the Python packages we need that are not pre-installed in Colab. The `pip install` commands below worked for me; I hope they work for you. I recommend not modifying the version numbers in the commands. Once you start tinkering with the dependencies, things can get messy. (Take my word for it.) "
      ],
      "metadata": {
        "id": "vL2mOvQrCZh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.2\n",
        "!pip install gym==0.23\n",
        "!pip install keras==2.8.0\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JijBa08URv4N",
        "outputId": "edc2f0e5-7174-4293-fbcd-9b96b60a13ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.2\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.2%2Bzzzcolab20220719082949-cp37-cp37m-linux_x86_64.whl (518.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 518.1 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.14.1)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (0.26.0)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.47.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.2) (1.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8.2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.1\n",
            "    Uninstalling tensorflow-2.9.1:\n",
            "      Successfully uninstalled tensorflow-2.9.1\n",
            "Successfully installed keras-2.8.0 tensorboard-2.8.0 tensorflow-2.8.2+zzzcolab20220719082949 tensorflow-estimator-2.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.23\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[K     |████████████████████████████████| 624 kB 4.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.23) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.23) (1.3.0)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.23) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym==0.23) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym==0.23) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697659 sha256=6d02c97e3decc1c8af8b5b2dea5c60c917b693fec0d9afc21839c6486dc57777\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/7e/16/4d727df048fdb96518ec5c02266e55b98bc398837353852a6a\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.23.0 gym-notices-0.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 580 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.47.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zpegaj6mXC3B",
        "outputId": "cfd7942a-05d4-4bb7-e566-81b0d79d1ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stockpyl\n",
            "  Downloading stockpyl-0.0.14-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=49.6 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (57.4.0)\n",
            "Collecting sphinx-toolbox>=3.1.2\n",
            "  Downloading sphinx_toolbox-3.2.0-py3-none-any.whl (523 kB)\n",
            "\u001b[K     |████████████████████████████████| 523 kB 25.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.21.6)\n",
            "Collecting jsonpickle>=1.0\n",
            "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (4.64.0)\n",
            "Collecting sphinx==4.5.0\n",
            "  Downloading Sphinx-4.5.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (2.6.3)\n",
            "Collecting sphinx-rtd-theme>=1.0.0\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting build>=0.0.2\n",
            "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.7.3)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 928 kB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.17.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.4.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.7.12)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.10.3)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (21.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (4.12.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx==4.5.0->stockpyl) (2022.2)\n",
            "Requirement already satisfied: pep517>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (0.13.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx==4.5.0->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0->stockpyl) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2022.6.15)\n",
            "Collecting autodocsumm>=0.2.0\n",
            "  Downloading autodocsumm-0.2.9-py3-none-any.whl (13 kB)\n",
            "Collecting html5lib>=1.1\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 51.7 MB/s \n",
            "\u001b[?25hCollecting dict2css>=0.2.3\n",
            "  Downloading dict2css-0.3.0-py3-none-any.whl (25 kB)\n",
            "Collecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.1-py3-none-any.whl (10.0 kB)\n",
            "Collecting sphinx-jinja2-compat>=0.1.0\n",
            "  Downloading sphinx_jinja2_compat-0.1.2-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.2-py3-none-any.whl (12 kB)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting beautifulsoup4>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 53.8 MB/s \n",
            "\u001b[?25hCollecting sphinx-prompt>=1.1.0\n",
            "  Downloading sphinx_prompt-1.5.0-py3-none-any.whl (4.5 kB)\n",
            "Collecting apeye>=0.4.0\n",
            "  Downloading apeye-1.2.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 67.8 MB/s \n",
            "\u001b[?25hCollecting domdf-python-tools>=2.9.0\n",
            "  Downloading domdf_python_tools-3.3.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 61.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachecontrol[filecache]>=0.12.6 in /usr/local/lib/python3.7/dist-packages (from sphinx-toolbox>=3.1.2->stockpyl) (0.12.11)\n",
            "Collecting ruamel.yaml>=0.16.12\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2.3.0\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting requests>=2.5.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from cachecontrol[filecache]>=0.12.6->sphinx-toolbox>=3.1.2->stockpyl) (1.0.4)\n",
            "Collecting cssutils>=2.2.0\n",
            "  Downloading cssutils-2.5.1-py3-none-any.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting natsort>=7.0.1\n",
            "  Downloading natsort-8.1.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.1->sphinx-toolbox>=3.1.2->stockpyl) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.1.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.1-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, requests, natsort, sphinx, soupsieve, ruamel.yaml.clib, platformdirs, mypy-extensions, lockfile, domdf-python-tools, cssutils, typing-inspect, sphinx-tabs, sphinx-prompt, sphinx-jinja2-compat, sphinx-autodoc-typehints, ruamel.yaml, html5lib, dict2css, beautifulsoup4, autodocsumm, apeye, sphinx-toolbox, sphinx-rtd-theme, jsonpickle, build, stockpyl\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: natsort\n",
            "    Found existing installation: natsort 5.5.0\n",
            "    Uninstalling natsort-5.5.0:\n",
            "      Successfully uninstalled natsort-5.5.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed apeye-1.2.0 autodocsumm-0.2.9 beautifulsoup4-4.11.1 build-0.8.0 cssutils-2.5.1 dict2css-0.3.0 domdf-python-tools-3.3.0 html5lib-1.1 jsonpickle-2.2.0 lockfile-0.12.2 mypy-extensions-0.4.3 natsort-8.1.0 platformdirs-2.5.2 requests-2.28.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 soupsieve-2.3.2.post1 sphinx-4.5.0 sphinx-autodoc-typehints-1.19.1 sphinx-jinja2-compat-0.1.2 sphinx-prompt-1.5.0 sphinx-rtd-theme-1.0.0 sphinx-tabs-3.4.0 sphinx-toolbox-3.2.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 stockpyl-0.0.14 typing-inspect-0.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import the packages we need."
      ],
      "metadata": {
        "id": "0NFSAGmCD48x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mqn9M8MZRym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stockpyl import sim\n",
        "from stockpyl.supply_chain_network import serial_system"
      ],
      "metadata": {
        "id": "16f2ytIQXl0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beer Game Environment\n",
        "\n",
        "#### States \n",
        "\n",
        "Assume that the RL agent is the decision maker at node $i\\in \\{1,\\ldots,4\\}$. (For example, if the RL is playing the role of the warehouse, then $i=2$.)\n",
        "As in OroojlooyJadid, et al., we assume that **state space** has 4 components:\n",
        "\n",
        "* $IL_t^i$, the inventory level at node $i$ in period $t$\n",
        "* $OO_t^i$, the on-order quantity at node $i$ in period $t$\n",
        "* $AO_t^i$, the arriving order (i.e., the demand received from the downstream neighbor) at node $i$ in period $t$\n",
        "* $AS_t^i$, the arriving shipment (i.e., the units received from the upstream neighbor) at node $i$ in period $t$\n",
        "\n",
        "In fact, the SRDQN algorithm assumes that we store the history of these state variables for the most recent 5 or 10 periods, but we will only use 1 period's worth of information in the algorithm below.\n",
        "\n",
        "It's natural to store the state as a tuple $(IL, OO, AO, AS)$, but it can be tricky to handle a tuple-based state in Tensorflow. Therefore, we convert the state tuple to a unique integer. The state space is therefore of type `Discrete` (using `gym` state classes). We'll never use the state integer directly; we'll covert the state tuple to an integer for storage and indexing, and convert back to a tuple when we need to know the individual state components. The `tuple_to_int()` and `int_to_tuple()` methods in the `BeerGameEnv` class do these conversions."
      ],
      "metadata": {
        "id": "2mOI44FjEPkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constants below provide the indices of the state-space components, so we don't have to remember them."
      ],
      "metadata": {
        "id": "0xWWeCzWdz1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortcuts to indices of the various states in the state space tuple.\n",
        "kIL = 0\n",
        "kOO = 1\n",
        "kAO = 2\n",
        "kAS = 3"
      ],
      "metadata": {
        "id": "AZ2aWem5FqVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actions\n",
        "\n",
        "Actions represent order quantities. In theory, any nonzero order quantity is allowed. However, to keep the state space manageable, we will require that the order quantity differs from the most recent demand by at most a fixed number (e.g., 5). In other words, if $AO$ is the most recent demand (arriving order), the order quantity is $AO+a$, where $a$ is constrained to be in some set such as $\\{-5,\\ldots,5\\}$. (This is sometimes called a \"$d+x$\" rule.)\n",
        "\n",
        "$a$ is the action, and can be different in different time periods."
      ],
      "metadata": {
        "id": "a8_J5eUXeWKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is the `BeerGameEnv` environment class. The code is missing some pieces. Your job is to fill in the missing pieces.\n",
        "\n",
        "---\n",
        "> **Note:** In the code below, the portions that you need to complete are marked with\n",
        "> \n",
        "> ```python\n",
        "> # #################\n",
        "> # TODO:\n",
        "> ```\n",
        "> \n",
        "> In place of the missing code is a line that says \n",
        "> \n",
        "> ```python\n",
        "> \traise NotImplementedError\n",
        "> ```\n",
        "> \n",
        "> This is a way of telling Python to raise an exception (error) because there's something missing here. You should **delete (or comment out) this line** after you write your code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PEzDFPXxd6xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BeerGameEnv(Env):\n",
        "    \"\"\"Beer game problem environment. A state represents a tuple (IL, OO, AO, AS),\n",
        "    where:\n",
        "    \n",
        "        * IL = inventory level at the agent at the end of the time period\n",
        "        * OO = on-order quantity at the end of the time period (items the agent \n",
        "            has ordered but not yet received)\n",
        "        * AO = arriving order, i.e., demand during the time period\n",
        "        * AS = arriving shipment, i.e., units received during the time period\n",
        "\n",
        "    However, this tuple is converted to an int via tuple_to_int() so that the\n",
        "    observation space is a 1-dimensional array.\n",
        "    \n",
        "    Actions represent differences from the demand observed in the time period.\n",
        "    That is, if the action is a, then the order quantity is AO + a. \n",
        "    a is restricted to be in a certain range, e.g., {-2, 1, 0, 1, 2}.\n",
        "    (This is sometimes called a \"d+x\" rule.)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : SupplyChainNetwork\n",
        "        The network to simulate.\n",
        "    episode_length : int\n",
        "        The number of periods in one episode.\n",
        "\tagent_node_index : int\n",
        "\t\tIndex of the node that the RL agent will play (e.g., 2 = wholesaler).\n",
        "    min_state : tuple\n",
        "        The minimum value of each state to consider: IL, OO, AO, AS.\n",
        "    max_state : tuple\n",
        "        The maximum value of each state to consider: IL, OO, AO, AS.\n",
        "    min_action : int\n",
        "        The minimum allowable action.\n",
        "    max_action : int\n",
        "        The maximum allowable action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network, episode_length: int, agent_node_index: int,\n",
        "\t            min_state: tuple, max_state: tuple, min_action: int, max_action: int):\n",
        "\n",
        "        # Store problem data.\n",
        "        self.network = network\n",
        "        self.episode_length = episode_length\n",
        "        self.agent_node_index = agent_node_index\n",
        "        self.min_state = min_state\n",
        "        self.max_state = max_state\n",
        "        self.min_action = min_action\n",
        "        self.max_action = max_action\n",
        "\n",
        "        # Set self.action_space to a gym Discrete space with elements\n",
        "        # min_action, min_action + 1, ..., max_action.\n",
        "        # (Hint: remember that you can use the `start` parameter; see \n",
        "\t\t# `MPNVEnv.__init__()` in the \"MPNV DQN\" notebook.)\n",
        "        # Also set self.action_space_list to a list with the same elements.\n",
        "        self.action_space = Discrete(max_action - min_action + 1, start=min_action)\n",
        "        self.action_space_list = list(range(min_action, max_action + 1))\n",
        "\n",
        "        # Determine the sizes of each component of the state space, and the\n",
        "        # total number of states in integer form.\n",
        "        self.state_size = [max_state[i] - min_state[i] + 1 for i in range(4)]\n",
        "        self.num_int_states = self.tuple_to_int(tuple(max_state[i] for i in range(4))) + 1\n",
        "        # Set the observation space as a Discrete space, as well as a list version.\n",
        "        self.observation_space = Discrete(self.num_int_states)\n",
        "        self.observation_space_list = list(range(self.num_int_states))\n",
        "\n",
        "        # Set self.initial_state assuming all components start at 0. That is,\n",
        "\t\t# use tuple_to_int() to set it to the integer version of the tuple (0, 0, 0, 0).\n",
        "        self.initial_state = self.tuple_to_int((0, 0, 0, 0))\n",
        "\n",
        "        # Initialize current state info.\n",
        "        self.state = None\n",
        "\n",
        "        # Get shortcuts to the RL agent node (as a SupplyChainNode object) \n",
        "\t\t# and its predecessor and successor node indices.\n",
        "        self.agent_node = network.get_node_from_index(self.agent_node_index)\n",
        "        self.predecessor_index = self.agent_node.predecessor_indices(include_external=True)[0]\n",
        "        self.successor_index = self.agent_node.successor_indices(include_external=True)[0]\n",
        "\n",
        "    def tuple_to_int(self, the_tuple: tuple):\n",
        "        \"\"\"Convert a tuple (n_0, ..., n_{m-1}) to a unique integer, where element\n",
        "        n_i can take one of self.state_sizes[i] values beginning at self.min_state[i]; that is, \n",
        "        n_i can be in {self.min_state[i], self.min_state[i] + 1, ..., self.min_state[i] + self.state_size[i] - 1}.\n",
        "        \"\"\"\n",
        "        # Get length of tuple/lists.\n",
        "        m = len(self.state_size)\n",
        "        # Convert tuple to a tuple in which each element starts at 0.\n",
        "        new_tuple = tuple(the_tuple[i] - self.min_state[i] for i in range(m))\n",
        "        # Convert new_tuple to int.\n",
        "        the_int = 0\n",
        "        for i in range(m):\n",
        "            the_int += int(np.prod([self.state_size[j] for j in range(i + 1, m)]) * new_tuple[i])\n",
        "        return the_int\n",
        "\n",
        "    def int_to_tuple(self, the_int: int):\n",
        "        \"\"\"Convert an integer to a unique tuple (n_0, ..., n_{m-1}), where element\n",
        "        n_i can take one of self.state_size[i] values beginning at self.min_state[i]; that is, \n",
        "        n_i can be in {self.min_state[i], self.min_state[i] + 1, ..., self.min_state[i] + self.state_size[i] - 1}.\n",
        "        \"\"\"\n",
        "        # Get length of tuple/lists.\n",
        "        m = len(self.state_size)\n",
        "        # Convert int to a tuple assuming each element starts at 0.\n",
        "        the_list = []\n",
        "        for i in range(m):\n",
        "            base = int(np.prod([self.state_size[j] for j in range(i + 1, m)]))\n",
        "            the_list.append(the_int // base)\n",
        "            the_int = the_int % base\n",
        "        # Convert list to new list accounting for min values.\n",
        "        new_list = [the_list[i] + self.min_state[i] for i in range(m)]\n",
        "        return tuple(new_list)\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment and the simulation. Choose an initial state randomly from\n",
        "        the list of possible initial states. Return it and set it in self.inventory_level.\"\"\"\n",
        "\n",
        "        # Reset the environment, following the same steps as in the reset()\n",
        "\t\t# method of the `MPNVEnv` class in the \"MPNV DQN\" notebook.)\n",
        "        # Determine initial IL and store it in environment's state.\n",
        "        self.state = self.initial_state\n",
        "        # Set node's initial IL attribute. (This will force the simulation to start with\n",
        "        # the node at this inventory level.)\n",
        "        self.agent_node.initial_inventory_level = self.int_to_tuple(self.initial_state)[kIL]\n",
        "        # Reset the simulation environment.\n",
        "        sim.initialize(self.network, self.episode_length)\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Run one time step of the environment by taking the specified action.\n",
        "        Update the environment state to the new state. \n",
        "        Return a tuple (new_state, reward, done).\"\"\"\n",
        "\n",
        "        # Convert self.state to a tuple.\n",
        "        # Convert state int to tuple.\n",
        "        state_tuple = self.int_to_tuple(self.state)\n",
        "\n",
        "        # Determine the order quantity.\n",
        "\t\t# Note: remember that the order quantity equals the most recent AO\n",
        "\t\t# (which is already stored in the state) plus the action.\n",
        "\t\t# Also: make sure to clip the order quantity so that it does not bring\n",
        "\t\t# the IL above its max value.\n",
        "        # Get order quantity (= most recent AO plus action).\n",
        "        order_quantity = state_tuple[kAO] + action\n",
        "        # Make sure the order quantity does not bring the IL above its max value.\n",
        "        order_quantity = int(np.clip(order_quantity, self.min_action, self.max_action))\n",
        "\n",
        "        # Build dict specifying order quantity to use in this time period.\n",
        "        # (This will override the order quantities that the stockpyl simulation\n",
        "        # would choose on its own.) \n",
        "\t\t# Note: the dict should contain only one entry, for the RL agent's\n",
        "\t\t# node; the other nodes are not included because we are not overriding\n",
        "\t\t# their order quantities.\n",
        "        order_quantity_override = {self.agent_node: order_quantity}\n",
        "# TODO: replace this after update package\n",
        "#         order_quantity_override = {n: order_quantity if n == self.agent_node \\\n",
        "#                                    else None for n in self.network.nodes}\n",
        "\n",
        "        # Simulate one time period.\n",
        "        sim.step(self.network, order_quantity_override=order_quantity_override)\n",
        "\n",
        "        # Determine reward by querying the simulation's state variables.\n",
        "        # NOTE: reward includes ALL nodes even though the agent only knows\n",
        "        # its own information. This is a simplification of the assumptions in\n",
        "        # Oroojlooyjadid et al (2021).\n",
        "        reward = -np.sum([n.state_vars_current.total_cost_incurred for \\\n",
        "                          n in self.network.nodes])\n",
        "\n",
        "        # If episode length has been reached, terminate.\n",
        "        done = self.network.period == self.episode_length - 1\n",
        "\n",
        "        # Get new state variables from simulation. (Round to int -- should \n",
        "        # already be integer but sometimes there are small rounding errors.)\n",
        "        # Clip states to state-space bounds.\n",
        "        IL = int(np.clip(self.agent_node.state_vars_current.inventory_level, \\\n",
        "                self.min_state[kIL], self.max_state[kIL]))\n",
        "        OO = int(np.clip(self.agent_node.state_vars_current.on_order, \\\n",
        "                self.min_state[kOO], self.max_state[kOO]))\n",
        "        AO = int(np.clip(self.agent_node.state_vars_current.inbound_order[self.successor_index], \\\n",
        "                self.min_state[kAO], self.max_state[kAO]))\n",
        "        AS = int(np.clip(self.agent_node.state_vars_current.inbound_shipment[self.predecessor_index], \\\n",
        "                self.min_state[kAS], self.max_state[kAS]))\n",
        "\n",
        "        # Update the state: first determine the new state tuple, then convert\n",
        "\t\t# it to an integer and store it in self.state.\n",
        "        state_tuple = (IL, OO, AO, AS)\n",
        "        self.state = self.tuple_to_int(state_tuple)\n",
        "\n",
        "        # Fill the demand into the info dict. (This repeats what's already in AO.)\n",
        "        info = {'demand': self.agent_node.state_vars_current.inbound_order[self.successor_index]}\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"This function can contain code for drawing the environment to\n",
        "        a graphics window, or printing it in ASCII format to the terminal.\n",
        "        But we'll just do something very simple and print the state.\n",
        "        (Feel free to add some nicer visualization code here if you want!)\"\"\"\n",
        "        print(self.state)\n",
        "\n",
        "    def play_episode(self, policy, messages=False):\n",
        "        \"\"\"Play one episode of the environment following the specified policy. \n",
        "        Return the total discounted reward over the episode.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize environment.\n",
        "        self.reset()\n",
        "        cumul_reward = 0\n",
        "\n",
        "        if messages:\n",
        "            print(f\"policy = {policy}\")\n",
        "            print(f\"Initial state = {self.int_to_tuple(self.state)}, total reward = {cumul_reward}\")\n",
        "\n",
        "        # Step through until terminal state reached.\n",
        "        for t in range(self.episode_length):\n",
        "            \n",
        "            # Determine action.\n",
        "            action = policy[self.state]\n",
        "\n",
        "            if messages:\n",
        "                print(f\"timestep {t:6} state = {self.int_to_tuple(self.state)} action = {action:4} \", end=\"\")\n",
        "\n",
        "            # Step.\n",
        "            new_state, reward, done, info = self.step(action)\n",
        "\n",
        "            # Update cumulative reward.\n",
        "            cumul_reward += reward\n",
        "\n",
        "            if messages:\n",
        "                print(f\"demand = {info['demand']:4} new_state = {self.int_to_tuple(new_state)} reward = {reward:8.2f} cumulative reward = {cumul_reward:8.2f}\")\n",
        "   \n",
        "        return cumul_reward"
      ],
      "metadata": {
        "id": "fyiDsHkocwa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beer Game Instance\n",
        "\n",
        "We'll use the following beer game instance. (This is similar to the \"simple instance\" in §4.1 of Oroojlooyjadid, et al. (2021).) The vectors below give the values for stages $1, ..., 4$, respectively. (Node 4 is upstream, node 1 is downstream.)\n",
        "\n",
        "* $h = [2, 2, 2, 2]$ \n",
        "* $p = [2, 0, 0, 0]$\n",
        "* $l^{tr} = [2, 2, 2, 2]$ (shipment lead time)\n",
        "* $l^{in} = [2, 2, 2, 2]$ (order lead time)\n",
        "* $D \\sim \\text{Poisson}(1)$ (demand uniformly drawn from Poisson distribution with mean 1)\n",
        "* Coplayers use base-stock policies with base-stock level 2\n",
        "\n",
        "We'll restrict the spaces as follows:\n",
        "\n",
        "* Action space: ${\\mathcal A} = \\{-2, -1, 0, 1, 2\\}$ (remember that the order quantity equals the action plus the observed demand)\n",
        "* State space: \n",
        "    * ${\\mathcal S}_{IL} = \\{-4, -3, ..., 4\\}$\n",
        "    * ${\\mathcal S}_{OO} = \\{0, 1, ..., 8\\}$\n",
        "    * ${\\mathcal S}_{AO} = \\{0, 1, ..., 4\\}$\n",
        "    * ${\\mathcal S}_{AS} = \\{0, 1, ..., 4\\}$\n",
        "\n",
        "And we'll use episodes of length 100.\n",
        "    \n"
      ],
      "metadata": {
        "id": "v2WJKrrz6S8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the network as a SupplyChainNetwork object.\n",
        "network = serial_system(\n",
        "    num_nodes=4,\n",
        "    node_order_in_system=[4, 3, 2, 1],  # in the network, nodes go 4 > 3 > 2 > 1\n",
        "    node_order_in_lists=[1, 2, 3, 4],   # in the lists below, nodes go 1 > 2 > 3 > 4\n",
        "    local_holding_cost=[2, 2, 2, 2],\n",
        "    stockout_cost=[2, 0, 0, 0],\n",
        "    shipment_lead_time=[2, 2, 2, 2],\n",
        "    order_lead_time=[2, 2, 2, 2],\n",
        "    demand_type='P', \n",
        "    mean=1,                         \n",
        "    policy_type='BS',                   \n",
        "    base_stock_level=2             \n",
        ")"
      ],
      "metadata": {
        "id": "sJKUIo8TXiGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_state = (-4, 0, 0, 0)\n",
        "max_state = (4, 8, 8, 8)\n",
        "min_action = -2\n",
        "max_action = 2\n",
        "episode_length = 100"
      ],
      "metadata": {
        "id": "nSUhScVD611O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's build our `BeerGameEnv` environment.\n",
        "\n",
        "Remember: This is now a well-defined `gym` environment. It's possible to \"register\" a custom environment to take advantage of the full `gym` API, but we won't need to do that here."
      ],
      "metadata": {
        "id": "j87w-pJB67pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build BeerGameEnv object.\n",
        "env = BeerGameEnv(\n",
        "    network=network,\n",
        "    episode_length=episode_length,\n",
        "    agent_node_index=2, # wholesaler\n",
        "    min_state=min_state,\n",
        "    max_state=max_state,\n",
        "    min_action=min_action,\n",
        "    max_action=max_action\n",
        ")"
      ],
      "metadata": {
        "id": "d39RKRwp6wa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give our new environment a quick spin. First, we'll create a base-stock policy with a base-stock level of 2 at every node. Then we'll ask our environment to play one episode of the beer game. In each time period, it will print the starting state, the action (order quantity), the demand, the new state, and the reward."
      ],
      "metadata": {
        "id": "cP2Y_QgE8jKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_stock_policy = {}\n",
        "for s in env.observation_space_list:\n",
        "    state_tuple = env.int_to_tuple(s)\n",
        "    base_stock_policy[s] = max(0, 2 - state_tuple[kIL])\n",
        "\n",
        "env.play_episode(base_stock_policy, messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASsC_ztI3sWP",
        "outputId": "ca248909-683f-470c-c2c6-899c74539bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy = {0: 6, 1: 6, 2: 6, 3: 6, 4: 6, 5: 6, 6: 6, 7: 6, 8: 6, 9: 6, 10: 6, 11: 6, 12: 6, 13: 6, 14: 6, 15: 6, 16: 6, 17: 6, 18: 6, 19: 6, 20: 6, 21: 6, 22: 6, 23: 6, 24: 6, 25: 6, 26: 6, 27: 6, 28: 6, 29: 6, 30: 6, 31: 6, 32: 6, 33: 6, 34: 6, 35: 6, 36: 6, 37: 6, 38: 6, 39: 6, 40: 6, 41: 6, 42: 6, 43: 6, 44: 6, 45: 6, 46: 6, 47: 6, 48: 6, 49: 6, 50: 6, 51: 6, 52: 6, 53: 6, 54: 6, 55: 6, 56: 6, 57: 6, 58: 6, 59: 6, 60: 6, 61: 6, 62: 6, 63: 6, 64: 6, 65: 6, 66: 6, 67: 6, 68: 6, 69: 6, 70: 6, 71: 6, 72: 6, 73: 6, 74: 6, 75: 6, 76: 6, 77: 6, 78: 6, 79: 6, 80: 6, 81: 6, 82: 6, 83: 6, 84: 6, 85: 6, 86: 6, 87: 6, 88: 6, 89: 6, 90: 6, 91: 6, 92: 6, 93: 6, 94: 6, 95: 6, 96: 6, 97: 6, 98: 6, 99: 6, 100: 6, 101: 6, 102: 6, 103: 6, 104: 6, 105: 6, 106: 6, 107: 6, 108: 6, 109: 6, 110: 6, 111: 6, 112: 6, 113: 6, 114: 6, 115: 6, 116: 6, 117: 6, 118: 6, 119: 6, 120: 6, 121: 6, 122: 6, 123: 6, 124: 6, 125: 6, 126: 6, 127: 6, 128: 6, 129: 6, 130: 6, 131: 6, 132: 6, 133: 6, 134: 6, 135: 6, 136: 6, 137: 6, 138: 6, 139: 6, 140: 6, 141: 6, 142: 6, 143: 6, 144: 6, 145: 6, 146: 6, 147: 6, 148: 6, 149: 6, 150: 6, 151: 6, 152: 6, 153: 6, 154: 6, 155: 6, 156: 6, 157: 6, 158: 6, 159: 6, 160: 6, 161: 6, 162: 6, 163: 6, 164: 6, 165: 6, 166: 6, 167: 6, 168: 6, 169: 6, 170: 6, 171: 6, 172: 6, 173: 6, 174: 6, 175: 6, 176: 6, 177: 6, 178: 6, 179: 6, 180: 6, 181: 6, 182: 6, 183: 6, 184: 6, 185: 6, 186: 6, 187: 6, 188: 6, 189: 6, 190: 6, 191: 6, 192: 6, 193: 6, 194: 6, 195: 6, 196: 6, 197: 6, 198: 6, 199: 6, 200: 6, 201: 6, 202: 6, 203: 6, 204: 6, 205: 6, 206: 6, 207: 6, 208: 6, 209: 6, 210: 6, 211: 6, 212: 6, 213: 6, 214: 6, 215: 6, 216: 6, 217: 6, 218: 6, 219: 6, 220: 6, 221: 6, 222: 6, 223: 6, 224: 6, 225: 6, 226: 6, 227: 6, 228: 6, 229: 6, 230: 6, 231: 6, 232: 6, 233: 6, 234: 6, 235: 6, 236: 6, 237: 6, 238: 6, 239: 6, 240: 6, 241: 6, 242: 6, 243: 6, 244: 6, 245: 6, 246: 6, 247: 6, 248: 6, 249: 6, 250: 6, 251: 6, 252: 6, 253: 6, 254: 6, 255: 6, 256: 6, 257: 6, 258: 6, 259: 6, 260: 6, 261: 6, 262: 6, 263: 6, 264: 6, 265: 6, 266: 6, 267: 6, 268: 6, 269: 6, 270: 6, 271: 6, 272: 6, 273: 6, 274: 6, 275: 6, 276: 6, 277: 6, 278: 6, 279: 6, 280: 6, 281: 6, 282: 6, 283: 6, 284: 6, 285: 6, 286: 6, 287: 6, 288: 6, 289: 6, 290: 6, 291: 6, 292: 6, 293: 6, 294: 6, 295: 6, 296: 6, 297: 6, 298: 6, 299: 6, 300: 6, 301: 6, 302: 6, 303: 6, 304: 6, 305: 6, 306: 6, 307: 6, 308: 6, 309: 6, 310: 6, 311: 6, 312: 6, 313: 6, 314: 6, 315: 6, 316: 6, 317: 6, 318: 6, 319: 6, 320: 6, 321: 6, 322: 6, 323: 6, 324: 6, 325: 6, 326: 6, 327: 6, 328: 6, 329: 6, 330: 6, 331: 6, 332: 6, 333: 6, 334: 6, 335: 6, 336: 6, 337: 6, 338: 6, 339: 6, 340: 6, 341: 6, 342: 6, 343: 6, 344: 6, 345: 6, 346: 6, 347: 6, 348: 6, 349: 6, 350: 6, 351: 6, 352: 6, 353: 6, 354: 6, 355: 6, 356: 6, 357: 6, 358: 6, 359: 6, 360: 6, 361: 6, 362: 6, 363: 6, 364: 6, 365: 6, 366: 6, 367: 6, 368: 6, 369: 6, 370: 6, 371: 6, 372: 6, 373: 6, 374: 6, 375: 6, 376: 6, 377: 6, 378: 6, 379: 6, 380: 6, 381: 6, 382: 6, 383: 6, 384: 6, 385: 6, 386: 6, 387: 6, 388: 6, 389: 6, 390: 6, 391: 6, 392: 6, 393: 6, 394: 6, 395: 6, 396: 6, 397: 6, 398: 6, 399: 6, 400: 6, 401: 6, 402: 6, 403: 6, 404: 6, 405: 6, 406: 6, 407: 6, 408: 6, 409: 6, 410: 6, 411: 6, 412: 6, 413: 6, 414: 6, 415: 6, 416: 6, 417: 6, 418: 6, 419: 6, 420: 6, 421: 6, 422: 6, 423: 6, 424: 6, 425: 6, 426: 6, 427: 6, 428: 6, 429: 6, 430: 6, 431: 6, 432: 6, 433: 6, 434: 6, 435: 6, 436: 6, 437: 6, 438: 6, 439: 6, 440: 6, 441: 6, 442: 6, 443: 6, 444: 6, 445: 6, 446: 6, 447: 6, 448: 6, 449: 6, 450: 6, 451: 6, 452: 6, 453: 6, 454: 6, 455: 6, 456: 6, 457: 6, 458: 6, 459: 6, 460: 6, 461: 6, 462: 6, 463: 6, 464: 6, 465: 6, 466: 6, 467: 6, 468: 6, 469: 6, 470: 6, 471: 6, 472: 6, 473: 6, 474: 6, 475: 6, 476: 6, 477: 6, 478: 6, 479: 6, 480: 6, 481: 6, 482: 6, 483: 6, 484: 6, 485: 6, 486: 6, 487: 6, 488: 6, 489: 6, 490: 6, 491: 6, 492: 6, 493: 6, 494: 6, 495: 6, 496: 6, 497: 6, 498: 6, 499: 6, 500: 6, 501: 6, 502: 6, 503: 6, 504: 6, 505: 6, 506: 6, 507: 6, 508: 6, 509: 6, 510: 6, 511: 6, 512: 6, 513: 6, 514: 6, 515: 6, 516: 6, 517: 6, 518: 6, 519: 6, 520: 6, 521: 6, 522: 6, 523: 6, 524: 6, 525: 6, 526: 6, 527: 6, 528: 6, 529: 6, 530: 6, 531: 6, 532: 6, 533: 6, 534: 6, 535: 6, 536: 6, 537: 6, 538: 6, 539: 6, 540: 6, 541: 6, 542: 6, 543: 6, 544: 6, 545: 6, 546: 6, 547: 6, 548: 6, 549: 6, 550: 6, 551: 6, 552: 6, 553: 6, 554: 6, 555: 6, 556: 6, 557: 6, 558: 6, 559: 6, 560: 6, 561: 6, 562: 6, 563: 6, 564: 6, 565: 6, 566: 6, 567: 6, 568: 6, 569: 6, 570: 6, 571: 6, 572: 6, 573: 6, 574: 6, 575: 6, 576: 6, 577: 6, 578: 6, 579: 6, 580: 6, 581: 6, 582: 6, 583: 6, 584: 6, 585: 6, 586: 6, 587: 6, 588: 6, 589: 6, 590: 6, 591: 6, 592: 6, 593: 6, 594: 6, 595: 6, 596: 6, 597: 6, 598: 6, 599: 6, 600: 6, 601: 6, 602: 6, 603: 6, 604: 6, 605: 6, 606: 6, 607: 6, 608: 6, 609: 6, 610: 6, 611: 6, 612: 6, 613: 6, 614: 6, 615: 6, 616: 6, 617: 6, 618: 6, 619: 6, 620: 6, 621: 6, 622: 6, 623: 6, 624: 6, 625: 6, 626: 6, 627: 6, 628: 6, 629: 6, 630: 6, 631: 6, 632: 6, 633: 6, 634: 6, 635: 6, 636: 6, 637: 6, 638: 6, 639: 6, 640: 6, 641: 6, 642: 6, 643: 6, 644: 6, 645: 6, 646: 6, 647: 6, 648: 6, 649: 6, 650: 6, 651: 6, 652: 6, 653: 6, 654: 6, 655: 6, 656: 6, 657: 6, 658: 6, 659: 6, 660: 6, 661: 6, 662: 6, 663: 6, 664: 6, 665: 6, 666: 6, 667: 6, 668: 6, 669: 6, 670: 6, 671: 6, 672: 6, 673: 6, 674: 6, 675: 6, 676: 6, 677: 6, 678: 6, 679: 6, 680: 6, 681: 6, 682: 6, 683: 6, 684: 6, 685: 6, 686: 6, 687: 6, 688: 6, 689: 6, 690: 6, 691: 6, 692: 6, 693: 6, 694: 6, 695: 6, 696: 6, 697: 6, 698: 6, 699: 6, 700: 6, 701: 6, 702: 6, 703: 6, 704: 6, 705: 6, 706: 6, 707: 6, 708: 6, 709: 6, 710: 6, 711: 6, 712: 6, 713: 6, 714: 6, 715: 6, 716: 6, 717: 6, 718: 6, 719: 6, 720: 6, 721: 6, 722: 6, 723: 6, 724: 6, 725: 6, 726: 6, 727: 6, 728: 6, 729: 5, 730: 5, 731: 5, 732: 5, 733: 5, 734: 5, 735: 5, 736: 5, 737: 5, 738: 5, 739: 5, 740: 5, 741: 5, 742: 5, 743: 5, 744: 5, 745: 5, 746: 5, 747: 5, 748: 5, 749: 5, 750: 5, 751: 5, 752: 5, 753: 5, 754: 5, 755: 5, 756: 5, 757: 5, 758: 5, 759: 5, 760: 5, 761: 5, 762: 5, 763: 5, 764: 5, 765: 5, 766: 5, 767: 5, 768: 5, 769: 5, 770: 5, 771: 5, 772: 5, 773: 5, 774: 5, 775: 5, 776: 5, 777: 5, 778: 5, 779: 5, 780: 5, 781: 5, 782: 5, 783: 5, 784: 5, 785: 5, 786: 5, 787: 5, 788: 5, 789: 5, 790: 5, 791: 5, 792: 5, 793: 5, 794: 5, 795: 5, 796: 5, 797: 5, 798: 5, 799: 5, 800: 5, 801: 5, 802: 5, 803: 5, 804: 5, 805: 5, 806: 5, 807: 5, 808: 5, 809: 5, 810: 5, 811: 5, 812: 5, 813: 5, 814: 5, 815: 5, 816: 5, 817: 5, 818: 5, 819: 5, 820: 5, 821: 5, 822: 5, 823: 5, 824: 5, 825: 5, 826: 5, 827: 5, 828: 5, 829: 5, 830: 5, 831: 5, 832: 5, 833: 5, 834: 5, 835: 5, 836: 5, 837: 5, 838: 5, 839: 5, 840: 5, 841: 5, 842: 5, 843: 5, 844: 5, 845: 5, 846: 5, 847: 5, 848: 5, 849: 5, 850: 5, 851: 5, 852: 5, 853: 5, 854: 5, 855: 5, 856: 5, 857: 5, 858: 5, 859: 5, 860: 5, 861: 5, 862: 5, 863: 5, 864: 5, 865: 5, 866: 5, 867: 5, 868: 5, 869: 5, 870: 5, 871: 5, 872: 5, 873: 5, 874: 5, 875: 5, 876: 5, 877: 5, 878: 5, 879: 5, 880: 5, 881: 5, 882: 5, 883: 5, 884: 5, 885: 5, 886: 5, 887: 5, 888: 5, 889: 5, 890: 5, 891: 5, 892: 5, 893: 5, 894: 5, 895: 5, 896: 5, 897: 5, 898: 5, 899: 5, 900: 5, 901: 5, 902: 5, 903: 5, 904: 5, 905: 5, 906: 5, 907: 5, 908: 5, 909: 5, 910: 5, 911: 5, 912: 5, 913: 5, 914: 5, 915: 5, 916: 5, 917: 5, 918: 5, 919: 5, 920: 5, 921: 5, 922: 5, 923: 5, 924: 5, 925: 5, 926: 5, 927: 5, 928: 5, 929: 5, 930: 5, 931: 5, 932: 5, 933: 5, 934: 5, 935: 5, 936: 5, 937: 5, 938: 5, 939: 5, 940: 5, 941: 5, 942: 5, 943: 5, 944: 5, 945: 5, 946: 5, 947: 5, 948: 5, 949: 5, 950: 5, 951: 5, 952: 5, 953: 5, 954: 5, 955: 5, 956: 5, 957: 5, 958: 5, 959: 5, 960: 5, 961: 5, 962: 5, 963: 5, 964: 5, 965: 5, 966: 5, 967: 5, 968: 5, 969: 5, 970: 5, 971: 5, 972: 5, 973: 5, 974: 5, 975: 5, 976: 5, 977: 5, 978: 5, 979: 5, 980: 5, 981: 5, 982: 5, 983: 5, 984: 5, 985: 5, 986: 5, 987: 5, 988: 5, 989: 5, 990: 5, 991: 5, 992: 5, 993: 5, 994: 5, 995: 5, 996: 5, 997: 5, 998: 5, 999: 5, 1000: 5, 1001: 5, 1002: 5, 1003: 5, 1004: 5, 1005: 5, 1006: 5, 1007: 5, 1008: 5, 1009: 5, 1010: 5, 1011: 5, 1012: 5, 1013: 5, 1014: 5, 1015: 5, 1016: 5, 1017: 5, 1018: 5, 1019: 5, 1020: 5, 1021: 5, 1022: 5, 1023: 5, 1024: 5, 1025: 5, 1026: 5, 1027: 5, 1028: 5, 1029: 5, 1030: 5, 1031: 5, 1032: 5, 1033: 5, 1034: 5, 1035: 5, 1036: 5, 1037: 5, 1038: 5, 1039: 5, 1040: 5, 1041: 5, 1042: 5, 1043: 5, 1044: 5, 1045: 5, 1046: 5, 1047: 5, 1048: 5, 1049: 5, 1050: 5, 1051: 5, 1052: 5, 1053: 5, 1054: 5, 1055: 5, 1056: 5, 1057: 5, 1058: 5, 1059: 5, 1060: 5, 1061: 5, 1062: 5, 1063: 5, 1064: 5, 1065: 5, 1066: 5, 1067: 5, 1068: 5, 1069: 5, 1070: 5, 1071: 5, 1072: 5, 1073: 5, 1074: 5, 1075: 5, 1076: 5, 1077: 5, 1078: 5, 1079: 5, 1080: 5, 1081: 5, 1082: 5, 1083: 5, 1084: 5, 1085: 5, 1086: 5, 1087: 5, 1088: 5, 1089: 5, 1090: 5, 1091: 5, 1092: 5, 1093: 5, 1094: 5, 1095: 5, 1096: 5, 1097: 5, 1098: 5, 1099: 5, 1100: 5, 1101: 5, 1102: 5, 1103: 5, 1104: 5, 1105: 5, 1106: 5, 1107: 5, 1108: 5, 1109: 5, 1110: 5, 1111: 5, 1112: 5, 1113: 5, 1114: 5, 1115: 5, 1116: 5, 1117: 5, 1118: 5, 1119: 5, 1120: 5, 1121: 5, 1122: 5, 1123: 5, 1124: 5, 1125: 5, 1126: 5, 1127: 5, 1128: 5, 1129: 5, 1130: 5, 1131: 5, 1132: 5, 1133: 5, 1134: 5, 1135: 5, 1136: 5, 1137: 5, 1138: 5, 1139: 5, 1140: 5, 1141: 5, 1142: 5, 1143: 5, 1144: 5, 1145: 5, 1146: 5, 1147: 5, 1148: 5, 1149: 5, 1150: 5, 1151: 5, 1152: 5, 1153: 5, 1154: 5, 1155: 5, 1156: 5, 1157: 5, 1158: 5, 1159: 5, 1160: 5, 1161: 5, 1162: 5, 1163: 5, 1164: 5, 1165: 5, 1166: 5, 1167: 5, 1168: 5, 1169: 5, 1170: 5, 1171: 5, 1172: 5, 1173: 5, 1174: 5, 1175: 5, 1176: 5, 1177: 5, 1178: 5, 1179: 5, 1180: 5, 1181: 5, 1182: 5, 1183: 5, 1184: 5, 1185: 5, 1186: 5, 1187: 5, 1188: 5, 1189: 5, 1190: 5, 1191: 5, 1192: 5, 1193: 5, 1194: 5, 1195: 5, 1196: 5, 1197: 5, 1198: 5, 1199: 5, 1200: 5, 1201: 5, 1202: 5, 1203: 5, 1204: 5, 1205: 5, 1206: 5, 1207: 5, 1208: 5, 1209: 5, 1210: 5, 1211: 5, 1212: 5, 1213: 5, 1214: 5, 1215: 5, 1216: 5, 1217: 5, 1218: 5, 1219: 5, 1220: 5, 1221: 5, 1222: 5, 1223: 5, 1224: 5, 1225: 5, 1226: 5, 1227: 5, 1228: 5, 1229: 5, 1230: 5, 1231: 5, 1232: 5, 1233: 5, 1234: 5, 1235: 5, 1236: 5, 1237: 5, 1238: 5, 1239: 5, 1240: 5, 1241: 5, 1242: 5, 1243: 5, 1244: 5, 1245: 5, 1246: 5, 1247: 5, 1248: 5, 1249: 5, 1250: 5, 1251: 5, 1252: 5, 1253: 5, 1254: 5, 1255: 5, 1256: 5, 1257: 5, 1258: 5, 1259: 5, 1260: 5, 1261: 5, 1262: 5, 1263: 5, 1264: 5, 1265: 5, 1266: 5, 1267: 5, 1268: 5, 1269: 5, 1270: 5, 1271: 5, 1272: 5, 1273: 5, 1274: 5, 1275: 5, 1276: 5, 1277: 5, 1278: 5, 1279: 5, 1280: 5, 1281: 5, 1282: 5, 1283: 5, 1284: 5, 1285: 5, 1286: 5, 1287: 5, 1288: 5, 1289: 5, 1290: 5, 1291: 5, 1292: 5, 1293: 5, 1294: 5, 1295: 5, 1296: 5, 1297: 5, 1298: 5, 1299: 5, 1300: 5, 1301: 5, 1302: 5, 1303: 5, 1304: 5, 1305: 5, 1306: 5, 1307: 5, 1308: 5, 1309: 5, 1310: 5, 1311: 5, 1312: 5, 1313: 5, 1314: 5, 1315: 5, 1316: 5, 1317: 5, 1318: 5, 1319: 5, 1320: 5, 1321: 5, 1322: 5, 1323: 5, 1324: 5, 1325: 5, 1326: 5, 1327: 5, 1328: 5, 1329: 5, 1330: 5, 1331: 5, 1332: 5, 1333: 5, 1334: 5, 1335: 5, 1336: 5, 1337: 5, 1338: 5, 1339: 5, 1340: 5, 1341: 5, 1342: 5, 1343: 5, 1344: 5, 1345: 5, 1346: 5, 1347: 5, 1348: 5, 1349: 5, 1350: 5, 1351: 5, 1352: 5, 1353: 5, 1354: 5, 1355: 5, 1356: 5, 1357: 5, 1358: 5, 1359: 5, 1360: 5, 1361: 5, 1362: 5, 1363: 5, 1364: 5, 1365: 5, 1366: 5, 1367: 5, 1368: 5, 1369: 5, 1370: 5, 1371: 5, 1372: 5, 1373: 5, 1374: 5, 1375: 5, 1376: 5, 1377: 5, 1378: 5, 1379: 5, 1380: 5, 1381: 5, 1382: 5, 1383: 5, 1384: 5, 1385: 5, 1386: 5, 1387: 5, 1388: 5, 1389: 5, 1390: 5, 1391: 5, 1392: 5, 1393: 5, 1394: 5, 1395: 5, 1396: 5, 1397: 5, 1398: 5, 1399: 5, 1400: 5, 1401: 5, 1402: 5, 1403: 5, 1404: 5, 1405: 5, 1406: 5, 1407: 5, 1408: 5, 1409: 5, 1410: 5, 1411: 5, 1412: 5, 1413: 5, 1414: 5, 1415: 5, 1416: 5, 1417: 5, 1418: 5, 1419: 5, 1420: 5, 1421: 5, 1422: 5, 1423: 5, 1424: 5, 1425: 5, 1426: 5, 1427: 5, 1428: 5, 1429: 5, 1430: 5, 1431: 5, 1432: 5, 1433: 5, 1434: 5, 1435: 5, 1436: 5, 1437: 5, 1438: 5, 1439: 5, 1440: 5, 1441: 5, 1442: 5, 1443: 5, 1444: 5, 1445: 5, 1446: 5, 1447: 5, 1448: 5, 1449: 5, 1450: 5, 1451: 5, 1452: 5, 1453: 5, 1454: 5, 1455: 5, 1456: 5, 1457: 5, 1458: 4, 1459: 4, 1460: 4, 1461: 4, 1462: 4, 1463: 4, 1464: 4, 1465: 4, 1466: 4, 1467: 4, 1468: 4, 1469: 4, 1470: 4, 1471: 4, 1472: 4, 1473: 4, 1474: 4, 1475: 4, 1476: 4, 1477: 4, 1478: 4, 1479: 4, 1480: 4, 1481: 4, 1482: 4, 1483: 4, 1484: 4, 1485: 4, 1486: 4, 1487: 4, 1488: 4, 1489: 4, 1490: 4, 1491: 4, 1492: 4, 1493: 4, 1494: 4, 1495: 4, 1496: 4, 1497: 4, 1498: 4, 1499: 4, 1500: 4, 1501: 4, 1502: 4, 1503: 4, 1504: 4, 1505: 4, 1506: 4, 1507: 4, 1508: 4, 1509: 4, 1510: 4, 1511: 4, 1512: 4, 1513: 4, 1514: 4, 1515: 4, 1516: 4, 1517: 4, 1518: 4, 1519: 4, 1520: 4, 1521: 4, 1522: 4, 1523: 4, 1524: 4, 1525: 4, 1526: 4, 1527: 4, 1528: 4, 1529: 4, 1530: 4, 1531: 4, 1532: 4, 1533: 4, 1534: 4, 1535: 4, 1536: 4, 1537: 4, 1538: 4, 1539: 4, 1540: 4, 1541: 4, 1542: 4, 1543: 4, 1544: 4, 1545: 4, 1546: 4, 1547: 4, 1548: 4, 1549: 4, 1550: 4, 1551: 4, 1552: 4, 1553: 4, 1554: 4, 1555: 4, 1556: 4, 1557: 4, 1558: 4, 1559: 4, 1560: 4, 1561: 4, 1562: 4, 1563: 4, 1564: 4, 1565: 4, 1566: 4, 1567: 4, 1568: 4, 1569: 4, 1570: 4, 1571: 4, 1572: 4, 1573: 4, 1574: 4, 1575: 4, 1576: 4, 1577: 4, 1578: 4, 1579: 4, 1580: 4, 1581: 4, 1582: 4, 1583: 4, 1584: 4, 1585: 4, 1586: 4, 1587: 4, 1588: 4, 1589: 4, 1590: 4, 1591: 4, 1592: 4, 1593: 4, 1594: 4, 1595: 4, 1596: 4, 1597: 4, 1598: 4, 1599: 4, 1600: 4, 1601: 4, 1602: 4, 1603: 4, 1604: 4, 1605: 4, 1606: 4, 1607: 4, 1608: 4, 1609: 4, 1610: 4, 1611: 4, 1612: 4, 1613: 4, 1614: 4, 1615: 4, 1616: 4, 1617: 4, 1618: 4, 1619: 4, 1620: 4, 1621: 4, 1622: 4, 1623: 4, 1624: 4, 1625: 4, 1626: 4, 1627: 4, 1628: 4, 1629: 4, 1630: 4, 1631: 4, 1632: 4, 1633: 4, 1634: 4, 1635: 4, 1636: 4, 1637: 4, 1638: 4, 1639: 4, 1640: 4, 1641: 4, 1642: 4, 1643: 4, 1644: 4, 1645: 4, 1646: 4, 1647: 4, 1648: 4, 1649: 4, 1650: 4, 1651: 4, 1652: 4, 1653: 4, 1654: 4, 1655: 4, 1656: 4, 1657: 4, 1658: 4, 1659: 4, 1660: 4, 1661: 4, 1662: 4, 1663: 4, 1664: 4, 1665: 4, 1666: 4, 1667: 4, 1668: 4, 1669: 4, 1670: 4, 1671: 4, 1672: 4, 1673: 4, 1674: 4, 1675: 4, 1676: 4, 1677: 4, 1678: 4, 1679: 4, 1680: 4, 1681: 4, 1682: 4, 1683: 4, 1684: 4, 1685: 4, 1686: 4, 1687: 4, 1688: 4, 1689: 4, 1690: 4, 1691: 4, 1692: 4, 1693: 4, 1694: 4, 1695: 4, 1696: 4, 1697: 4, 1698: 4, 1699: 4, 1700: 4, 1701: 4, 1702: 4, 1703: 4, 1704: 4, 1705: 4, 1706: 4, 1707: 4, 1708: 4, 1709: 4, 1710: 4, 1711: 4, 1712: 4, 1713: 4, 1714: 4, 1715: 4, 1716: 4, 1717: 4, 1718: 4, 1719: 4, 1720: 4, 1721: 4, 1722: 4, 1723: 4, 1724: 4, 1725: 4, 1726: 4, 1727: 4, 1728: 4, 1729: 4, 1730: 4, 1731: 4, 1732: 4, 1733: 4, 1734: 4, 1735: 4, 1736: 4, 1737: 4, 1738: 4, 1739: 4, 1740: 4, 1741: 4, 1742: 4, 1743: 4, 1744: 4, 1745: 4, 1746: 4, 1747: 4, 1748: 4, 1749: 4, 1750: 4, 1751: 4, 1752: 4, 1753: 4, 1754: 4, 1755: 4, 1756: 4, 1757: 4, 1758: 4, 1759: 4, 1760: 4, 1761: 4, 1762: 4, 1763: 4, 1764: 4, 1765: 4, 1766: 4, 1767: 4, 1768: 4, 1769: 4, 1770: 4, 1771: 4, 1772: 4, 1773: 4, 1774: 4, 1775: 4, 1776: 4, 1777: 4, 1778: 4, 1779: 4, 1780: 4, 1781: 4, 1782: 4, 1783: 4, 1784: 4, 1785: 4, 1786: 4, 1787: 4, 1788: 4, 1789: 4, 1790: 4, 1791: 4, 1792: 4, 1793: 4, 1794: 4, 1795: 4, 1796: 4, 1797: 4, 1798: 4, 1799: 4, 1800: 4, 1801: 4, 1802: 4, 1803: 4, 1804: 4, 1805: 4, 1806: 4, 1807: 4, 1808: 4, 1809: 4, 1810: 4, 1811: 4, 1812: 4, 1813: 4, 1814: 4, 1815: 4, 1816: 4, 1817: 4, 1818: 4, 1819: 4, 1820: 4, 1821: 4, 1822: 4, 1823: 4, 1824: 4, 1825: 4, 1826: 4, 1827: 4, 1828: 4, 1829: 4, 1830: 4, 1831: 4, 1832: 4, 1833: 4, 1834: 4, 1835: 4, 1836: 4, 1837: 4, 1838: 4, 1839: 4, 1840: 4, 1841: 4, 1842: 4, 1843: 4, 1844: 4, 1845: 4, 1846: 4, 1847: 4, 1848: 4, 1849: 4, 1850: 4, 1851: 4, 1852: 4, 1853: 4, 1854: 4, 1855: 4, 1856: 4, 1857: 4, 1858: 4, 1859: 4, 1860: 4, 1861: 4, 1862: 4, 1863: 4, 1864: 4, 1865: 4, 1866: 4, 1867: 4, 1868: 4, 1869: 4, 1870: 4, 1871: 4, 1872: 4, 1873: 4, 1874: 4, 1875: 4, 1876: 4, 1877: 4, 1878: 4, 1879: 4, 1880: 4, 1881: 4, 1882: 4, 1883: 4, 1884: 4, 1885: 4, 1886: 4, 1887: 4, 1888: 4, 1889: 4, 1890: 4, 1891: 4, 1892: 4, 1893: 4, 1894: 4, 1895: 4, 1896: 4, 1897: 4, 1898: 4, 1899: 4, 1900: 4, 1901: 4, 1902: 4, 1903: 4, 1904: 4, 1905: 4, 1906: 4, 1907: 4, 1908: 4, 1909: 4, 1910: 4, 1911: 4, 1912: 4, 1913: 4, 1914: 4, 1915: 4, 1916: 4, 1917: 4, 1918: 4, 1919: 4, 1920: 4, 1921: 4, 1922: 4, 1923: 4, 1924: 4, 1925: 4, 1926: 4, 1927: 4, 1928: 4, 1929: 4, 1930: 4, 1931: 4, 1932: 4, 1933: 4, 1934: 4, 1935: 4, 1936: 4, 1937: 4, 1938: 4, 1939: 4, 1940: 4, 1941: 4, 1942: 4, 1943: 4, 1944: 4, 1945: 4, 1946: 4, 1947: 4, 1948: 4, 1949: 4, 1950: 4, 1951: 4, 1952: 4, 1953: 4, 1954: 4, 1955: 4, 1956: 4, 1957: 4, 1958: 4, 1959: 4, 1960: 4, 1961: 4, 1962: 4, 1963: 4, 1964: 4, 1965: 4, 1966: 4, 1967: 4, 1968: 4, 1969: 4, 1970: 4, 1971: 4, 1972: 4, 1973: 4, 1974: 4, 1975: 4, 1976: 4, 1977: 4, 1978: 4, 1979: 4, 1980: 4, 1981: 4, 1982: 4, 1983: 4, 1984: 4, 1985: 4, 1986: 4, 1987: 4, 1988: 4, 1989: 4, 1990: 4, 1991: 4, 1992: 4, 1993: 4, 1994: 4, 1995: 4, 1996: 4, 1997: 4, 1998: 4, 1999: 4, 2000: 4, 2001: 4, 2002: 4, 2003: 4, 2004: 4, 2005: 4, 2006: 4, 2007: 4, 2008: 4, 2009: 4, 2010: 4, 2011: 4, 2012: 4, 2013: 4, 2014: 4, 2015: 4, 2016: 4, 2017: 4, 2018: 4, 2019: 4, 2020: 4, 2021: 4, 2022: 4, 2023: 4, 2024: 4, 2025: 4, 2026: 4, 2027: 4, 2028: 4, 2029: 4, 2030: 4, 2031: 4, 2032: 4, 2033: 4, 2034: 4, 2035: 4, 2036: 4, 2037: 4, 2038: 4, 2039: 4, 2040: 4, 2041: 4, 2042: 4, 2043: 4, 2044: 4, 2045: 4, 2046: 4, 2047: 4, 2048: 4, 2049: 4, 2050: 4, 2051: 4, 2052: 4, 2053: 4, 2054: 4, 2055: 4, 2056: 4, 2057: 4, 2058: 4, 2059: 4, 2060: 4, 2061: 4, 2062: 4, 2063: 4, 2064: 4, 2065: 4, 2066: 4, 2067: 4, 2068: 4, 2069: 4, 2070: 4, 2071: 4, 2072: 4, 2073: 4, 2074: 4, 2075: 4, 2076: 4, 2077: 4, 2078: 4, 2079: 4, 2080: 4, 2081: 4, 2082: 4, 2083: 4, 2084: 4, 2085: 4, 2086: 4, 2087: 4, 2088: 4, 2089: 4, 2090: 4, 2091: 4, 2092: 4, 2093: 4, 2094: 4, 2095: 4, 2096: 4, 2097: 4, 2098: 4, 2099: 4, 2100: 4, 2101: 4, 2102: 4, 2103: 4, 2104: 4, 2105: 4, 2106: 4, 2107: 4, 2108: 4, 2109: 4, 2110: 4, 2111: 4, 2112: 4, 2113: 4, 2114: 4, 2115: 4, 2116: 4, 2117: 4, 2118: 4, 2119: 4, 2120: 4, 2121: 4, 2122: 4, 2123: 4, 2124: 4, 2125: 4, 2126: 4, 2127: 4, 2128: 4, 2129: 4, 2130: 4, 2131: 4, 2132: 4, 2133: 4, 2134: 4, 2135: 4, 2136: 4, 2137: 4, 2138: 4, 2139: 4, 2140: 4, 2141: 4, 2142: 4, 2143: 4, 2144: 4, 2145: 4, 2146: 4, 2147: 4, 2148: 4, 2149: 4, 2150: 4, 2151: 4, 2152: 4, 2153: 4, 2154: 4, 2155: 4, 2156: 4, 2157: 4, 2158: 4, 2159: 4, 2160: 4, 2161: 4, 2162: 4, 2163: 4, 2164: 4, 2165: 4, 2166: 4, 2167: 4, 2168: 4, 2169: 4, 2170: 4, 2171: 4, 2172: 4, 2173: 4, 2174: 4, 2175: 4, 2176: 4, 2177: 4, 2178: 4, 2179: 4, 2180: 4, 2181: 4, 2182: 4, 2183: 4, 2184: 4, 2185: 4, 2186: 4, 2187: 3, 2188: 3, 2189: 3, 2190: 3, 2191: 3, 2192: 3, 2193: 3, 2194: 3, 2195: 3, 2196: 3, 2197: 3, 2198: 3, 2199: 3, 2200: 3, 2201: 3, 2202: 3, 2203: 3, 2204: 3, 2205: 3, 2206: 3, 2207: 3, 2208: 3, 2209: 3, 2210: 3, 2211: 3, 2212: 3, 2213: 3, 2214: 3, 2215: 3, 2216: 3, 2217: 3, 2218: 3, 2219: 3, 2220: 3, 2221: 3, 2222: 3, 2223: 3, 2224: 3, 2225: 3, 2226: 3, 2227: 3, 2228: 3, 2229: 3, 2230: 3, 2231: 3, 2232: 3, 2233: 3, 2234: 3, 2235: 3, 2236: 3, 2237: 3, 2238: 3, 2239: 3, 2240: 3, 2241: 3, 2242: 3, 2243: 3, 2244: 3, 2245: 3, 2246: 3, 2247: 3, 2248: 3, 2249: 3, 2250: 3, 2251: 3, 2252: 3, 2253: 3, 2254: 3, 2255: 3, 2256: 3, 2257: 3, 2258: 3, 2259: 3, 2260: 3, 2261: 3, 2262: 3, 2263: 3, 2264: 3, 2265: 3, 2266: 3, 2267: 3, 2268: 3, 2269: 3, 2270: 3, 2271: 3, 2272: 3, 2273: 3, 2274: 3, 2275: 3, 2276: 3, 2277: 3, 2278: 3, 2279: 3, 2280: 3, 2281: 3, 2282: 3, 2283: 3, 2284: 3, 2285: 3, 2286: 3, 2287: 3, 2288: 3, 2289: 3, 2290: 3, 2291: 3, 2292: 3, 2293: 3, 2294: 3, 2295: 3, 2296: 3, 2297: 3, 2298: 3, 2299: 3, 2300: 3, 2301: 3, 2302: 3, 2303: 3, 2304: 3, 2305: 3, 2306: 3, 2307: 3, 2308: 3, 2309: 3, 2310: 3, 2311: 3, 2312: 3, 2313: 3, 2314: 3, 2315: 3, 2316: 3, 2317: 3, 2318: 3, 2319: 3, 2320: 3, 2321: 3, 2322: 3, 2323: 3, 2324: 3, 2325: 3, 2326: 3, 2327: 3, 2328: 3, 2329: 3, 2330: 3, 2331: 3, 2332: 3, 2333: 3, 2334: 3, 2335: 3, 2336: 3, 2337: 3, 2338: 3, 2339: 3, 2340: 3, 2341: 3, 2342: 3, 2343: 3, 2344: 3, 2345: 3, 2346: 3, 2347: 3, 2348: 3, 2349: 3, 2350: 3, 2351: 3, 2352: 3, 2353: 3, 2354: 3, 2355: 3, 2356: 3, 2357: 3, 2358: 3, 2359: 3, 2360: 3, 2361: 3, 2362: 3, 2363: 3, 2364: 3, 2365: 3, 2366: 3, 2367: 3, 2368: 3, 2369: 3, 2370: 3, 2371: 3, 2372: 3, 2373: 3, 2374: 3, 2375: 3, 2376: 3, 2377: 3, 2378: 3, 2379: 3, 2380: 3, 2381: 3, 2382: 3, 2383: 3, 2384: 3, 2385: 3, 2386: 3, 2387: 3, 2388: 3, 2389: 3, 2390: 3, 2391: 3, 2392: 3, 2393: 3, 2394: 3, 2395: 3, 2396: 3, 2397: 3, 2398: 3, 2399: 3, 2400: 3, 2401: 3, 2402: 3, 2403: 3, 2404: 3, 2405: 3, 2406: 3, 2407: 3, 2408: 3, 2409: 3, 2410: 3, 2411: 3, 2412: 3, 2413: 3, 2414: 3, 2415: 3, 2416: 3, 2417: 3, 2418: 3, 2419: 3, 2420: 3, 2421: 3, 2422: 3, 2423: 3, 2424: 3, 2425: 3, 2426: 3, 2427: 3, 2428: 3, 2429: 3, 2430: 3, 2431: 3, 2432: 3, 2433: 3, 2434: 3, 2435: 3, 2436: 3, 2437: 3, 2438: 3, 2439: 3, 2440: 3, 2441: 3, 2442: 3, 2443: 3, 2444: 3, 2445: 3, 2446: 3, 2447: 3, 2448: 3, 2449: 3, 2450: 3, 2451: 3, 2452: 3, 2453: 3, 2454: 3, 2455: 3, 2456: 3, 2457: 3, 2458: 3, 2459: 3, 2460: 3, 2461: 3, 2462: 3, 2463: 3, 2464: 3, 2465: 3, 2466: 3, 2467: 3, 2468: 3, 2469: 3, 2470: 3, 2471: 3, 2472: 3, 2473: 3, 2474: 3, 2475: 3, 2476: 3, 2477: 3, 2478: 3, 2479: 3, 2480: 3, 2481: 3, 2482: 3, 2483: 3, 2484: 3, 2485: 3, 2486: 3, 2487: 3, 2488: 3, 2489: 3, 2490: 3, 2491: 3, 2492: 3, 2493: 3, 2494: 3, 2495: 3, 2496: 3, 2497: 3, 2498: 3, 2499: 3, 2500: 3, 2501: 3, 2502: 3, 2503: 3, 2504: 3, 2505: 3, 2506: 3, 2507: 3, 2508: 3, 2509: 3, 2510: 3, 2511: 3, 2512: 3, 2513: 3, 2514: 3, 2515: 3, 2516: 3, 2517: 3, 2518: 3, 2519: 3, 2520: 3, 2521: 3, 2522: 3, 2523: 3, 2524: 3, 2525: 3, 2526: 3, 2527: 3, 2528: 3, 2529: 3, 2530: 3, 2531: 3, 2532: 3, 2533: 3, 2534: 3, 2535: 3, 2536: 3, 2537: 3, 2538: 3, 2539: 3, 2540: 3, 2541: 3, 2542: 3, 2543: 3, 2544: 3, 2545: 3, 2546: 3, 2547: 3, 2548: 3, 2549: 3, 2550: 3, 2551: 3, 2552: 3, 2553: 3, 2554: 3, 2555: 3, 2556: 3, 2557: 3, 2558: 3, 2559: 3, 2560: 3, 2561: 3, 2562: 3, 2563: 3, 2564: 3, 2565: 3, 2566: 3, 2567: 3, 2568: 3, 2569: 3, 2570: 3, 2571: 3, 2572: 3, 2573: 3, 2574: 3, 2575: 3, 2576: 3, 2577: 3, 2578: 3, 2579: 3, 2580: 3, 2581: 3, 2582: 3, 2583: 3, 2584: 3, 2585: 3, 2586: 3, 2587: 3, 2588: 3, 2589: 3, 2590: 3, 2591: 3, 2592: 3, 2593: 3, 2594: 3, 2595: 3, 2596: 3, 2597: 3, 2598: 3, 2599: 3, 2600: 3, 2601: 3, 2602: 3, 2603: 3, 2604: 3, 2605: 3, 2606: 3, 2607: 3, 2608: 3, 2609: 3, 2610: 3, 2611: 3, 2612: 3, 2613: 3, 2614: 3, 2615: 3, 2616: 3, 2617: 3, 2618: 3, 2619: 3, 2620: 3, 2621: 3, 2622: 3, 2623: 3, 2624: 3, 2625: 3, 2626: 3, 2627: 3, 2628: 3, 2629: 3, 2630: 3, 2631: 3, 2632: 3, 2633: 3, 2634: 3, 2635: 3, 2636: 3, 2637: 3, 2638: 3, 2639: 3, 2640: 3, 2641: 3, 2642: 3, 2643: 3, 2644: 3, 2645: 3, 2646: 3, 2647: 3, 2648: 3, 2649: 3, 2650: 3, 2651: 3, 2652: 3, 2653: 3, 2654: 3, 2655: 3, 2656: 3, 2657: 3, 2658: 3, 2659: 3, 2660: 3, 2661: 3, 2662: 3, 2663: 3, 2664: 3, 2665: 3, 2666: 3, 2667: 3, 2668: 3, 2669: 3, 2670: 3, 2671: 3, 2672: 3, 2673: 3, 2674: 3, 2675: 3, 2676: 3, 2677: 3, 2678: 3, 2679: 3, 2680: 3, 2681: 3, 2682: 3, 2683: 3, 2684: 3, 2685: 3, 2686: 3, 2687: 3, 2688: 3, 2689: 3, 2690: 3, 2691: 3, 2692: 3, 2693: 3, 2694: 3, 2695: 3, 2696: 3, 2697: 3, 2698: 3, 2699: 3, 2700: 3, 2701: 3, 2702: 3, 2703: 3, 2704: 3, 2705: 3, 2706: 3, 2707: 3, 2708: 3, 2709: 3, 2710: 3, 2711: 3, 2712: 3, 2713: 3, 2714: 3, 2715: 3, 2716: 3, 2717: 3, 2718: 3, 2719: 3, 2720: 3, 2721: 3, 2722: 3, 2723: 3, 2724: 3, 2725: 3, 2726: 3, 2727: 3, 2728: 3, 2729: 3, 2730: 3, 2731: 3, 2732: 3, 2733: 3, 2734: 3, 2735: 3, 2736: 3, 2737: 3, 2738: 3, 2739: 3, 2740: 3, 2741: 3, 2742: 3, 2743: 3, 2744: 3, 2745: 3, 2746: 3, 2747: 3, 2748: 3, 2749: 3, 2750: 3, 2751: 3, 2752: 3, 2753: 3, 2754: 3, 2755: 3, 2756: 3, 2757: 3, 2758: 3, 2759: 3, 2760: 3, 2761: 3, 2762: 3, 2763: 3, 2764: 3, 2765: 3, 2766: 3, 2767: 3, 2768: 3, 2769: 3, 2770: 3, 2771: 3, 2772: 3, 2773: 3, 2774: 3, 2775: 3, 2776: 3, 2777: 3, 2778: 3, 2779: 3, 2780: 3, 2781: 3, 2782: 3, 2783: 3, 2784: 3, 2785: 3, 2786: 3, 2787: 3, 2788: 3, 2789: 3, 2790: 3, 2791: 3, 2792: 3, 2793: 3, 2794: 3, 2795: 3, 2796: 3, 2797: 3, 2798: 3, 2799: 3, 2800: 3, 2801: 3, 2802: 3, 2803: 3, 2804: 3, 2805: 3, 2806: 3, 2807: 3, 2808: 3, 2809: 3, 2810: 3, 2811: 3, 2812: 3, 2813: 3, 2814: 3, 2815: 3, 2816: 3, 2817: 3, 2818: 3, 2819: 3, 2820: 3, 2821: 3, 2822: 3, 2823: 3, 2824: 3, 2825: 3, 2826: 3, 2827: 3, 2828: 3, 2829: 3, 2830: 3, 2831: 3, 2832: 3, 2833: 3, 2834: 3, 2835: 3, 2836: 3, 2837: 3, 2838: 3, 2839: 3, 2840: 3, 2841: 3, 2842: 3, 2843: 3, 2844: 3, 2845: 3, 2846: 3, 2847: 3, 2848: 3, 2849: 3, 2850: 3, 2851: 3, 2852: 3, 2853: 3, 2854: 3, 2855: 3, 2856: 3, 2857: 3, 2858: 3, 2859: 3, 2860: 3, 2861: 3, 2862: 3, 2863: 3, 2864: 3, 2865: 3, 2866: 3, 2867: 3, 2868: 3, 2869: 3, 2870: 3, 2871: 3, 2872: 3, 2873: 3, 2874: 3, 2875: 3, 2876: 3, 2877: 3, 2878: 3, 2879: 3, 2880: 3, 2881: 3, 2882: 3, 2883: 3, 2884: 3, 2885: 3, 2886: 3, 2887: 3, 2888: 3, 2889: 3, 2890: 3, 2891: 3, 2892: 3, 2893: 3, 2894: 3, 2895: 3, 2896: 3, 2897: 3, 2898: 3, 2899: 3, 2900: 3, 2901: 3, 2902: 3, 2903: 3, 2904: 3, 2905: 3, 2906: 3, 2907: 3, 2908: 3, 2909: 3, 2910: 3, 2911: 3, 2912: 3, 2913: 3, 2914: 3, 2915: 3, 2916: 2, 2917: 2, 2918: 2, 2919: 2, 2920: 2, 2921: 2, 2922: 2, 2923: 2, 2924: 2, 2925: 2, 2926: 2, 2927: 2, 2928: 2, 2929: 2, 2930: 2, 2931: 2, 2932: 2, 2933: 2, 2934: 2, 2935: 2, 2936: 2, 2937: 2, 2938: 2, 2939: 2, 2940: 2, 2941: 2, 2942: 2, 2943: 2, 2944: 2, 2945: 2, 2946: 2, 2947: 2, 2948: 2, 2949: 2, 2950: 2, 2951: 2, 2952: 2, 2953: 2, 2954: 2, 2955: 2, 2956: 2, 2957: 2, 2958: 2, 2959: 2, 2960: 2, 2961: 2, 2962: 2, 2963: 2, 2964: 2, 2965: 2, 2966: 2, 2967: 2, 2968: 2, 2969: 2, 2970: 2, 2971: 2, 2972: 2, 2973: 2, 2974: 2, 2975: 2, 2976: 2, 2977: 2, 2978: 2, 2979: 2, 2980: 2, 2981: 2, 2982: 2, 2983: 2, 2984: 2, 2985: 2, 2986: 2, 2987: 2, 2988: 2, 2989: 2, 2990: 2, 2991: 2, 2992: 2, 2993: 2, 2994: 2, 2995: 2, 2996: 2, 2997: 2, 2998: 2, 2999: 2, 3000: 2, 3001: 2, 3002: 2, 3003: 2, 3004: 2, 3005: 2, 3006: 2, 3007: 2, 3008: 2, 3009: 2, 3010: 2, 3011: 2, 3012: 2, 3013: 2, 3014: 2, 3015: 2, 3016: 2, 3017: 2, 3018: 2, 3019: 2, 3020: 2, 3021: 2, 3022: 2, 3023: 2, 3024: 2, 3025: 2, 3026: 2, 3027: 2, 3028: 2, 3029: 2, 3030: 2, 3031: 2, 3032: 2, 3033: 2, 3034: 2, 3035: 2, 3036: 2, 3037: 2, 3038: 2, 3039: 2, 3040: 2, 3041: 2, 3042: 2, 3043: 2, 3044: 2, 3045: 2, 3046: 2, 3047: 2, 3048: 2, 3049: 2, 3050: 2, 3051: 2, 3052: 2, 3053: 2, 3054: 2, 3055: 2, 3056: 2, 3057: 2, 3058: 2, 3059: 2, 3060: 2, 3061: 2, 3062: 2, 3063: 2, 3064: 2, 3065: 2, 3066: 2, 3067: 2, 3068: 2, 3069: 2, 3070: 2, 3071: 2, 3072: 2, 3073: 2, 3074: 2, 3075: 2, 3076: 2, 3077: 2, 3078: 2, 3079: 2, 3080: 2, 3081: 2, 3082: 2, 3083: 2, 3084: 2, 3085: 2, 3086: 2, 3087: 2, 3088: 2, 3089: 2, 3090: 2, 3091: 2, 3092: 2, 3093: 2, 3094: 2, 3095: 2, 3096: 2, 3097: 2, 3098: 2, 3099: 2, 3100: 2, 3101: 2, 3102: 2, 3103: 2, 3104: 2, 3105: 2, 3106: 2, 3107: 2, 3108: 2, 3109: 2, 3110: 2, 3111: 2, 3112: 2, 3113: 2, 3114: 2, 3115: 2, 3116: 2, 3117: 2, 3118: 2, 3119: 2, 3120: 2, 3121: 2, 3122: 2, 3123: 2, 3124: 2, 3125: 2, 3126: 2, 3127: 2, 3128: 2, 3129: 2, 3130: 2, 3131: 2, 3132: 2, 3133: 2, 3134: 2, 3135: 2, 3136: 2, 3137: 2, 3138: 2, 3139: 2, 3140: 2, 3141: 2, 3142: 2, 3143: 2, 3144: 2, 3145: 2, 3146: 2, 3147: 2, 3148: 2, 3149: 2, 3150: 2, 3151: 2, 3152: 2, 3153: 2, 3154: 2, 3155: 2, 3156: 2, 3157: 2, 3158: 2, 3159: 2, 3160: 2, 3161: 2, 3162: 2, 3163: 2, 3164: 2, 3165: 2, 3166: 2, 3167: 2, 3168: 2, 3169: 2, 3170: 2, 3171: 2, 3172: 2, 3173: 2, 3174: 2, 3175: 2, 3176: 2, 3177: 2, 3178: 2, 3179: 2, 3180: 2, 3181: 2, 3182: 2, 3183: 2, 3184: 2, 3185: 2, 3186: 2, 3187: 2, 3188: 2, 3189: 2, 3190: 2, 3191: 2, 3192: 2, 3193: 2, 3194: 2, 3195: 2, 3196: 2, 3197: 2, 3198: 2, 3199: 2, 3200: 2, 3201: 2, 3202: 2, 3203: 2, 3204: 2, 3205: 2, 3206: 2, 3207: 2, 3208: 2, 3209: 2, 3210: 2, 3211: 2, 3212: 2, 3213: 2, 3214: 2, 3215: 2, 3216: 2, 3217: 2, 3218: 2, 3219: 2, 3220: 2, 3221: 2, 3222: 2, 3223: 2, 3224: 2, 3225: 2, 3226: 2, 3227: 2, 3228: 2, 3229: 2, 3230: 2, 3231: 2, 3232: 2, 3233: 2, 3234: 2, 3235: 2, 3236: 2, 3237: 2, 3238: 2, 3239: 2, 3240: 2, 3241: 2, 3242: 2, 3243: 2, 3244: 2, 3245: 2, 3246: 2, 3247: 2, 3248: 2, 3249: 2, 3250: 2, 3251: 2, 3252: 2, 3253: 2, 3254: 2, 3255: 2, 3256: 2, 3257: 2, 3258: 2, 3259: 2, 3260: 2, 3261: 2, 3262: 2, 3263: 2, 3264: 2, 3265: 2, 3266: 2, 3267: 2, 3268: 2, 3269: 2, 3270: 2, 3271: 2, 3272: 2, 3273: 2, 3274: 2, 3275: 2, 3276: 2, 3277: 2, 3278: 2, 3279: 2, 3280: 2, 3281: 2, 3282: 2, 3283: 2, 3284: 2, 3285: 2, 3286: 2, 3287: 2, 3288: 2, 3289: 2, 3290: 2, 3291: 2, 3292: 2, 3293: 2, 3294: 2, 3295: 2, 3296: 2, 3297: 2, 3298: 2, 3299: 2, 3300: 2, 3301: 2, 3302: 2, 3303: 2, 3304: 2, 3305: 2, 3306: 2, 3307: 2, 3308: 2, 3309: 2, 3310: 2, 3311: 2, 3312: 2, 3313: 2, 3314: 2, 3315: 2, 3316: 2, 3317: 2, 3318: 2, 3319: 2, 3320: 2, 3321: 2, 3322: 2, 3323: 2, 3324: 2, 3325: 2, 3326: 2, 3327: 2, 3328: 2, 3329: 2, 3330: 2, 3331: 2, 3332: 2, 3333: 2, 3334: 2, 3335: 2, 3336: 2, 3337: 2, 3338: 2, 3339: 2, 3340: 2, 3341: 2, 3342: 2, 3343: 2, 3344: 2, 3345: 2, 3346: 2, 3347: 2, 3348: 2, 3349: 2, 3350: 2, 3351: 2, 3352: 2, 3353: 2, 3354: 2, 3355: 2, 3356: 2, 3357: 2, 3358: 2, 3359: 2, 3360: 2, 3361: 2, 3362: 2, 3363: 2, 3364: 2, 3365: 2, 3366: 2, 3367: 2, 3368: 2, 3369: 2, 3370: 2, 3371: 2, 3372: 2, 3373: 2, 3374: 2, 3375: 2, 3376: 2, 3377: 2, 3378: 2, 3379: 2, 3380: 2, 3381: 2, 3382: 2, 3383: 2, 3384: 2, 3385: 2, 3386: 2, 3387: 2, 3388: 2, 3389: 2, 3390: 2, 3391: 2, 3392: 2, 3393: 2, 3394: 2, 3395: 2, 3396: 2, 3397: 2, 3398: 2, 3399: 2, 3400: 2, 3401: 2, 3402: 2, 3403: 2, 3404: 2, 3405: 2, 3406: 2, 3407: 2, 3408: 2, 3409: 2, 3410: 2, 3411: 2, 3412: 2, 3413: 2, 3414: 2, 3415: 2, 3416: 2, 3417: 2, 3418: 2, 3419: 2, 3420: 2, 3421: 2, 3422: 2, 3423: 2, 3424: 2, 3425: 2, 3426: 2, 3427: 2, 3428: 2, 3429: 2, 3430: 2, 3431: 2, 3432: 2, 3433: 2, 3434: 2, 3435: 2, 3436: 2, 3437: 2, 3438: 2, 3439: 2, 3440: 2, 3441: 2, 3442: 2, 3443: 2, 3444: 2, 3445: 2, 3446: 2, 3447: 2, 3448: 2, 3449: 2, 3450: 2, 3451: 2, 3452: 2, 3453: 2, 3454: 2, 3455: 2, 3456: 2, 3457: 2, 3458: 2, 3459: 2, 3460: 2, 3461: 2, 3462: 2, 3463: 2, 3464: 2, 3465: 2, 3466: 2, 3467: 2, 3468: 2, 3469: 2, 3470: 2, 3471: 2, 3472: 2, 3473: 2, 3474: 2, 3475: 2, 3476: 2, 3477: 2, 3478: 2, 3479: 2, 3480: 2, 3481: 2, 3482: 2, 3483: 2, 3484: 2, 3485: 2, 3486: 2, 3487: 2, 3488: 2, 3489: 2, 3490: 2, 3491: 2, 3492: 2, 3493: 2, 3494: 2, 3495: 2, 3496: 2, 3497: 2, 3498: 2, 3499: 2, 3500: 2, 3501: 2, 3502: 2, 3503: 2, 3504: 2, 3505: 2, 3506: 2, 3507: 2, 3508: 2, 3509: 2, 3510: 2, 3511: 2, 3512: 2, 3513: 2, 3514: 2, 3515: 2, 3516: 2, 3517: 2, 3518: 2, 3519: 2, 3520: 2, 3521: 2, 3522: 2, 3523: 2, 3524: 2, 3525: 2, 3526: 2, 3527: 2, 3528: 2, 3529: 2, 3530: 2, 3531: 2, 3532: 2, 3533: 2, 3534: 2, 3535: 2, 3536: 2, 3537: 2, 3538: 2, 3539: 2, 3540: 2, 3541: 2, 3542: 2, 3543: 2, 3544: 2, 3545: 2, 3546: 2, 3547: 2, 3548: 2, 3549: 2, 3550: 2, 3551: 2, 3552: 2, 3553: 2, 3554: 2, 3555: 2, 3556: 2, 3557: 2, 3558: 2, 3559: 2, 3560: 2, 3561: 2, 3562: 2, 3563: 2, 3564: 2, 3565: 2, 3566: 2, 3567: 2, 3568: 2, 3569: 2, 3570: 2, 3571: 2, 3572: 2, 3573: 2, 3574: 2, 3575: 2, 3576: 2, 3577: 2, 3578: 2, 3579: 2, 3580: 2, 3581: 2, 3582: 2, 3583: 2, 3584: 2, 3585: 2, 3586: 2, 3587: 2, 3588: 2, 3589: 2, 3590: 2, 3591: 2, 3592: 2, 3593: 2, 3594: 2, 3595: 2, 3596: 2, 3597: 2, 3598: 2, 3599: 2, 3600: 2, 3601: 2, 3602: 2, 3603: 2, 3604: 2, 3605: 2, 3606: 2, 3607: 2, 3608: 2, 3609: 2, 3610: 2, 3611: 2, 3612: 2, 3613: 2, 3614: 2, 3615: 2, 3616: 2, 3617: 2, 3618: 2, 3619: 2, 3620: 2, 3621: 2, 3622: 2, 3623: 2, 3624: 2, 3625: 2, 3626: 2, 3627: 2, 3628: 2, 3629: 2, 3630: 2, 3631: 2, 3632: 2, 3633: 2, 3634: 2, 3635: 2, 3636: 2, 3637: 2, 3638: 2, 3639: 2, 3640: 2, 3641: 2, 3642: 2, 3643: 2, 3644: 2, 3645: 1, 3646: 1, 3647: 1, 3648: 1, 3649: 1, 3650: 1, 3651: 1, 3652: 1, 3653: 1, 3654: 1, 3655: 1, 3656: 1, 3657: 1, 3658: 1, 3659: 1, 3660: 1, 3661: 1, 3662: 1, 3663: 1, 3664: 1, 3665: 1, 3666: 1, 3667: 1, 3668: 1, 3669: 1, 3670: 1, 3671: 1, 3672: 1, 3673: 1, 3674: 1, 3675: 1, 3676: 1, 3677: 1, 3678: 1, 3679: 1, 3680: 1, 3681: 1, 3682: 1, 3683: 1, 3684: 1, 3685: 1, 3686: 1, 3687: 1, 3688: 1, 3689: 1, 3690: 1, 3691: 1, 3692: 1, 3693: 1, 3694: 1, 3695: 1, 3696: 1, 3697: 1, 3698: 1, 3699: 1, 3700: 1, 3701: 1, 3702: 1, 3703: 1, 3704: 1, 3705: 1, 3706: 1, 3707: 1, 3708: 1, 3709: 1, 3710: 1, 3711: 1, 3712: 1, 3713: 1, 3714: 1, 3715: 1, 3716: 1, 3717: 1, 3718: 1, 3719: 1, 3720: 1, 3721: 1, 3722: 1, 3723: 1, 3724: 1, 3725: 1, 3726: 1, 3727: 1, 3728: 1, 3729: 1, 3730: 1, 3731: 1, 3732: 1, 3733: 1, 3734: 1, 3735: 1, 3736: 1, 3737: 1, 3738: 1, 3739: 1, 3740: 1, 3741: 1, 3742: 1, 3743: 1, 3744: 1, 3745: 1, 3746: 1, 3747: 1, 3748: 1, 3749: 1, 3750: 1, 3751: 1, 3752: 1, 3753: 1, 3754: 1, 3755: 1, 3756: 1, 3757: 1, 3758: 1, 3759: 1, 3760: 1, 3761: 1, 3762: 1, 3763: 1, 3764: 1, 3765: 1, 3766: 1, 3767: 1, 3768: 1, 3769: 1, 3770: 1, 3771: 1, 3772: 1, 3773: 1, 3774: 1, 3775: 1, 3776: 1, 3777: 1, 3778: 1, 3779: 1, 3780: 1, 3781: 1, 3782: 1, 3783: 1, 3784: 1, 3785: 1, 3786: 1, 3787: 1, 3788: 1, 3789: 1, 3790: 1, 3791: 1, 3792: 1, 3793: 1, 3794: 1, 3795: 1, 3796: 1, 3797: 1, 3798: 1, 3799: 1, 3800: 1, 3801: 1, 3802: 1, 3803: 1, 3804: 1, 3805: 1, 3806: 1, 3807: 1, 3808: 1, 3809: 1, 3810: 1, 3811: 1, 3812: 1, 3813: 1, 3814: 1, 3815: 1, 3816: 1, 3817: 1, 3818: 1, 3819: 1, 3820: 1, 3821: 1, 3822: 1, 3823: 1, 3824: 1, 3825: 1, 3826: 1, 3827: 1, 3828: 1, 3829: 1, 3830: 1, 3831: 1, 3832: 1, 3833: 1, 3834: 1, 3835: 1, 3836: 1, 3837: 1, 3838: 1, 3839: 1, 3840: 1, 3841: 1, 3842: 1, 3843: 1, 3844: 1, 3845: 1, 3846: 1, 3847: 1, 3848: 1, 3849: 1, 3850: 1, 3851: 1, 3852: 1, 3853: 1, 3854: 1, 3855: 1, 3856: 1, 3857: 1, 3858: 1, 3859: 1, 3860: 1, 3861: 1, 3862: 1, 3863: 1, 3864: 1, 3865: 1, 3866: 1, 3867: 1, 3868: 1, 3869: 1, 3870: 1, 3871: 1, 3872: 1, 3873: 1, 3874: 1, 3875: 1, 3876: 1, 3877: 1, 3878: 1, 3879: 1, 3880: 1, 3881: 1, 3882: 1, 3883: 1, 3884: 1, 3885: 1, 3886: 1, 3887: 1, 3888: 1, 3889: 1, 3890: 1, 3891: 1, 3892: 1, 3893: 1, 3894: 1, 3895: 1, 3896: 1, 3897: 1, 3898: 1, 3899: 1, 3900: 1, 3901: 1, 3902: 1, 3903: 1, 3904: 1, 3905: 1, 3906: 1, 3907: 1, 3908: 1, 3909: 1, 3910: 1, 3911: 1, 3912: 1, 3913: 1, 3914: 1, 3915: 1, 3916: 1, 3917: 1, 3918: 1, 3919: 1, 3920: 1, 3921: 1, 3922: 1, 3923: 1, 3924: 1, 3925: 1, 3926: 1, 3927: 1, 3928: 1, 3929: 1, 3930: 1, 3931: 1, 3932: 1, 3933: 1, 3934: 1, 3935: 1, 3936: 1, 3937: 1, 3938: 1, 3939: 1, 3940: 1, 3941: 1, 3942: 1, 3943: 1, 3944: 1, 3945: 1, 3946: 1, 3947: 1, 3948: 1, 3949: 1, 3950: 1, 3951: 1, 3952: 1, 3953: 1, 3954: 1, 3955: 1, 3956: 1, 3957: 1, 3958: 1, 3959: 1, 3960: 1, 3961: 1, 3962: 1, 3963: 1, 3964: 1, 3965: 1, 3966: 1, 3967: 1, 3968: 1, 3969: 1, 3970: 1, 3971: 1, 3972: 1, 3973: 1, 3974: 1, 3975: 1, 3976: 1, 3977: 1, 3978: 1, 3979: 1, 3980: 1, 3981: 1, 3982: 1, 3983: 1, 3984: 1, 3985: 1, 3986: 1, 3987: 1, 3988: 1, 3989: 1, 3990: 1, 3991: 1, 3992: 1, 3993: 1, 3994: 1, 3995: 1, 3996: 1, 3997: 1, 3998: 1, 3999: 1, 4000: 1, 4001: 1, 4002: 1, 4003: 1, 4004: 1, 4005: 1, 4006: 1, 4007: 1, 4008: 1, 4009: 1, 4010: 1, 4011: 1, 4012: 1, 4013: 1, 4014: 1, 4015: 1, 4016: 1, 4017: 1, 4018: 1, 4019: 1, 4020: 1, 4021: 1, 4022: 1, 4023: 1, 4024: 1, 4025: 1, 4026: 1, 4027: 1, 4028: 1, 4029: 1, 4030: 1, 4031: 1, 4032: 1, 4033: 1, 4034: 1, 4035: 1, 4036: 1, 4037: 1, 4038: 1, 4039: 1, 4040: 1, 4041: 1, 4042: 1, 4043: 1, 4044: 1, 4045: 1, 4046: 1, 4047: 1, 4048: 1, 4049: 1, 4050: 1, 4051: 1, 4052: 1, 4053: 1, 4054: 1, 4055: 1, 4056: 1, 4057: 1, 4058: 1, 4059: 1, 4060: 1, 4061: 1, 4062: 1, 4063: 1, 4064: 1, 4065: 1, 4066: 1, 4067: 1, 4068: 1, 4069: 1, 4070: 1, 4071: 1, 4072: 1, 4073: 1, 4074: 1, 4075: 1, 4076: 1, 4077: 1, 4078: 1, 4079: 1, 4080: 1, 4081: 1, 4082: 1, 4083: 1, 4084: 1, 4085: 1, 4086: 1, 4087: 1, 4088: 1, 4089: 1, 4090: 1, 4091: 1, 4092: 1, 4093: 1, 4094: 1, 4095: 1, 4096: 1, 4097: 1, 4098: 1, 4099: 1, 4100: 1, 4101: 1, 4102: 1, 4103: 1, 4104: 1, 4105: 1, 4106: 1, 4107: 1, 4108: 1, 4109: 1, 4110: 1, 4111: 1, 4112: 1, 4113: 1, 4114: 1, 4115: 1, 4116: 1, 4117: 1, 4118: 1, 4119: 1, 4120: 1, 4121: 1, 4122: 1, 4123: 1, 4124: 1, 4125: 1, 4126: 1, 4127: 1, 4128: 1, 4129: 1, 4130: 1, 4131: 1, 4132: 1, 4133: 1, 4134: 1, 4135: 1, 4136: 1, 4137: 1, 4138: 1, 4139: 1, 4140: 1, 4141: 1, 4142: 1, 4143: 1, 4144: 1, 4145: 1, 4146: 1, 4147: 1, 4148: 1, 4149: 1, 4150: 1, 4151: 1, 4152: 1, 4153: 1, 4154: 1, 4155: 1, 4156: 1, 4157: 1, 4158: 1, 4159: 1, 4160: 1, 4161: 1, 4162: 1, 4163: 1, 4164: 1, 4165: 1, 4166: 1, 4167: 1, 4168: 1, 4169: 1, 4170: 1, 4171: 1, 4172: 1, 4173: 1, 4174: 1, 4175: 1, 4176: 1, 4177: 1, 4178: 1, 4179: 1, 4180: 1, 4181: 1, 4182: 1, 4183: 1, 4184: 1, 4185: 1, 4186: 1, 4187: 1, 4188: 1, 4189: 1, 4190: 1, 4191: 1, 4192: 1, 4193: 1, 4194: 1, 4195: 1, 4196: 1, 4197: 1, 4198: 1, 4199: 1, 4200: 1, 4201: 1, 4202: 1, 4203: 1, 4204: 1, 4205: 1, 4206: 1, 4207: 1, 4208: 1, 4209: 1, 4210: 1, 4211: 1, 4212: 1, 4213: 1, 4214: 1, 4215: 1, 4216: 1, 4217: 1, 4218: 1, 4219: 1, 4220: 1, 4221: 1, 4222: 1, 4223: 1, 4224: 1, 4225: 1, 4226: 1, 4227: 1, 4228: 1, 4229: 1, 4230: 1, 4231: 1, 4232: 1, 4233: 1, 4234: 1, 4235: 1, 4236: 1, 4237: 1, 4238: 1, 4239: 1, 4240: 1, 4241: 1, 4242: 1, 4243: 1, 4244: 1, 4245: 1, 4246: 1, 4247: 1, 4248: 1, 4249: 1, 4250: 1, 4251: 1, 4252: 1, 4253: 1, 4254: 1, 4255: 1, 4256: 1, 4257: 1, 4258: 1, 4259: 1, 4260: 1, 4261: 1, 4262: 1, 4263: 1, 4264: 1, 4265: 1, 4266: 1, 4267: 1, 4268: 1, 4269: 1, 4270: 1, 4271: 1, 4272: 1, 4273: 1, 4274: 1, 4275: 1, 4276: 1, 4277: 1, 4278: 1, 4279: 1, 4280: 1, 4281: 1, 4282: 1, 4283: 1, 4284: 1, 4285: 1, 4286: 1, 4287: 1, 4288: 1, 4289: 1, 4290: 1, 4291: 1, 4292: 1, 4293: 1, 4294: 1, 4295: 1, 4296: 1, 4297: 1, 4298: 1, 4299: 1, 4300: 1, 4301: 1, 4302: 1, 4303: 1, 4304: 1, 4305: 1, 4306: 1, 4307: 1, 4308: 1, 4309: 1, 4310: 1, 4311: 1, 4312: 1, 4313: 1, 4314: 1, 4315: 1, 4316: 1, 4317: 1, 4318: 1, 4319: 1, 4320: 1, 4321: 1, 4322: 1, 4323: 1, 4324: 1, 4325: 1, 4326: 1, 4327: 1, 4328: 1, 4329: 1, 4330: 1, 4331: 1, 4332: 1, 4333: 1, 4334: 1, 4335: 1, 4336: 1, 4337: 1, 4338: 1, 4339: 1, 4340: 1, 4341: 1, 4342: 1, 4343: 1, 4344: 1, 4345: 1, 4346: 1, 4347: 1, 4348: 1, 4349: 1, 4350: 1, 4351: 1, 4352: 1, 4353: 1, 4354: 1, 4355: 1, 4356: 1, 4357: 1, 4358: 1, 4359: 1, 4360: 1, 4361: 1, 4362: 1, 4363: 1, 4364: 1, 4365: 1, 4366: 1, 4367: 1, 4368: 1, 4369: 1, 4370: 1, 4371: 1, 4372: 1, 4373: 1, 4374: 0, 4375: 0, 4376: 0, 4377: 0, 4378: 0, 4379: 0, 4380: 0, 4381: 0, 4382: 0, 4383: 0, 4384: 0, 4385: 0, 4386: 0, 4387: 0, 4388: 0, 4389: 0, 4390: 0, 4391: 0, 4392: 0, 4393: 0, 4394: 0, 4395: 0, 4396: 0, 4397: 0, 4398: 0, 4399: 0, 4400: 0, 4401: 0, 4402: 0, 4403: 0, 4404: 0, 4405: 0, 4406: 0, 4407: 0, 4408: 0, 4409: 0, 4410: 0, 4411: 0, 4412: 0, 4413: 0, 4414: 0, 4415: 0, 4416: 0, 4417: 0, 4418: 0, 4419: 0, 4420: 0, 4421: 0, 4422: 0, 4423: 0, 4424: 0, 4425: 0, 4426: 0, 4427: 0, 4428: 0, 4429: 0, 4430: 0, 4431: 0, 4432: 0, 4433: 0, 4434: 0, 4435: 0, 4436: 0, 4437: 0, 4438: 0, 4439: 0, 4440: 0, 4441: 0, 4442: 0, 4443: 0, 4444: 0, 4445: 0, 4446: 0, 4447: 0, 4448: 0, 4449: 0, 4450: 0, 4451: 0, 4452: 0, 4453: 0, 4454: 0, 4455: 0, 4456: 0, 4457: 0, 4458: 0, 4459: 0, 4460: 0, 4461: 0, 4462: 0, 4463: 0, 4464: 0, 4465: 0, 4466: 0, 4467: 0, 4468: 0, 4469: 0, 4470: 0, 4471: 0, 4472: 0, 4473: 0, 4474: 0, 4475: 0, 4476: 0, 4477: 0, 4478: 0, 4479: 0, 4480: 0, 4481: 0, 4482: 0, 4483: 0, 4484: 0, 4485: 0, 4486: 0, 4487: 0, 4488: 0, 4489: 0, 4490: 0, 4491: 0, 4492: 0, 4493: 0, 4494: 0, 4495: 0, 4496: 0, 4497: 0, 4498: 0, 4499: 0, 4500: 0, 4501: 0, 4502: 0, 4503: 0, 4504: 0, 4505: 0, 4506: 0, 4507: 0, 4508: 0, 4509: 0, 4510: 0, 4511: 0, 4512: 0, 4513: 0, 4514: 0, 4515: 0, 4516: 0, 4517: 0, 4518: 0, 4519: 0, 4520: 0, 4521: 0, 4522: 0, 4523: 0, 4524: 0, 4525: 0, 4526: 0, 4527: 0, 4528: 0, 4529: 0, 4530: 0, 4531: 0, 4532: 0, 4533: 0, 4534: 0, 4535: 0, 4536: 0, 4537: 0, 4538: 0, 4539: 0, 4540: 0, 4541: 0, 4542: 0, 4543: 0, 4544: 0, 4545: 0, 4546: 0, 4547: 0, 4548: 0, 4549: 0, 4550: 0, 4551: 0, 4552: 0, 4553: 0, 4554: 0, 4555: 0, 4556: 0, 4557: 0, 4558: 0, 4559: 0, 4560: 0, 4561: 0, 4562: 0, 4563: 0, 4564: 0, 4565: 0, 4566: 0, 4567: 0, 4568: 0, 4569: 0, 4570: 0, 4571: 0, 4572: 0, 4573: 0, 4574: 0, 4575: 0, 4576: 0, 4577: 0, 4578: 0, 4579: 0, 4580: 0, 4581: 0, 4582: 0, 4583: 0, 4584: 0, 4585: 0, 4586: 0, 4587: 0, 4588: 0, 4589: 0, 4590: 0, 4591: 0, 4592: 0, 4593: 0, 4594: 0, 4595: 0, 4596: 0, 4597: 0, 4598: 0, 4599: 0, 4600: 0, 4601: 0, 4602: 0, 4603: 0, 4604: 0, 4605: 0, 4606: 0, 4607: 0, 4608: 0, 4609: 0, 4610: 0, 4611: 0, 4612: 0, 4613: 0, 4614: 0, 4615: 0, 4616: 0, 4617: 0, 4618: 0, 4619: 0, 4620: 0, 4621: 0, 4622: 0, 4623: 0, 4624: 0, 4625: 0, 4626: 0, 4627: 0, 4628: 0, 4629: 0, 4630: 0, 4631: 0, 4632: 0, 4633: 0, 4634: 0, 4635: 0, 4636: 0, 4637: 0, 4638: 0, 4639: 0, 4640: 0, 4641: 0, 4642: 0, 4643: 0, 4644: 0, 4645: 0, 4646: 0, 4647: 0, 4648: 0, 4649: 0, 4650: 0, 4651: 0, 4652: 0, 4653: 0, 4654: 0, 4655: 0, 4656: 0, 4657: 0, 4658: 0, 4659: 0, 4660: 0, 4661: 0, 4662: 0, 4663: 0, 4664: 0, 4665: 0, 4666: 0, 4667: 0, 4668: 0, 4669: 0, 4670: 0, 4671: 0, 4672: 0, 4673: 0, 4674: 0, 4675: 0, 4676: 0, 4677: 0, 4678: 0, 4679: 0, 4680: 0, 4681: 0, 4682: 0, 4683: 0, 4684: 0, 4685: 0, 4686: 0, 4687: 0, 4688: 0, 4689: 0, 4690: 0, 4691: 0, 4692: 0, 4693: 0, 4694: 0, 4695: 0, 4696: 0, 4697: 0, 4698: 0, 4699: 0, 4700: 0, 4701: 0, 4702: 0, 4703: 0, 4704: 0, 4705: 0, 4706: 0, 4707: 0, 4708: 0, 4709: 0, 4710: 0, 4711: 0, 4712: 0, 4713: 0, 4714: 0, 4715: 0, 4716: 0, 4717: 0, 4718: 0, 4719: 0, 4720: 0, 4721: 0, 4722: 0, 4723: 0, 4724: 0, 4725: 0, 4726: 0, 4727: 0, 4728: 0, 4729: 0, 4730: 0, 4731: 0, 4732: 0, 4733: 0, 4734: 0, 4735: 0, 4736: 0, 4737: 0, 4738: 0, 4739: 0, 4740: 0, 4741: 0, 4742: 0, 4743: 0, 4744: 0, 4745: 0, 4746: 0, 4747: 0, 4748: 0, 4749: 0, 4750: 0, 4751: 0, 4752: 0, 4753: 0, 4754: 0, 4755: 0, 4756: 0, 4757: 0, 4758: 0, 4759: 0, 4760: 0, 4761: 0, 4762: 0, 4763: 0, 4764: 0, 4765: 0, 4766: 0, 4767: 0, 4768: 0, 4769: 0, 4770: 0, 4771: 0, 4772: 0, 4773: 0, 4774: 0, 4775: 0, 4776: 0, 4777: 0, 4778: 0, 4779: 0, 4780: 0, 4781: 0, 4782: 0, 4783: 0, 4784: 0, 4785: 0, 4786: 0, 4787: 0, 4788: 0, 4789: 0, 4790: 0, 4791: 0, 4792: 0, 4793: 0, 4794: 0, 4795: 0, 4796: 0, 4797: 0, 4798: 0, 4799: 0, 4800: 0, 4801: 0, 4802: 0, 4803: 0, 4804: 0, 4805: 0, 4806: 0, 4807: 0, 4808: 0, 4809: 0, 4810: 0, 4811: 0, 4812: 0, 4813: 0, 4814: 0, 4815: 0, 4816: 0, 4817: 0, 4818: 0, 4819: 0, 4820: 0, 4821: 0, 4822: 0, 4823: 0, 4824: 0, 4825: 0, 4826: 0, 4827: 0, 4828: 0, 4829: 0, 4830: 0, 4831: 0, 4832: 0, 4833: 0, 4834: 0, 4835: 0, 4836: 0, 4837: 0, 4838: 0, 4839: 0, 4840: 0, 4841: 0, 4842: 0, 4843: 0, 4844: 0, 4845: 0, 4846: 0, 4847: 0, 4848: 0, 4849: 0, 4850: 0, 4851: 0, 4852: 0, 4853: 0, 4854: 0, 4855: 0, 4856: 0, 4857: 0, 4858: 0, 4859: 0, 4860: 0, 4861: 0, 4862: 0, 4863: 0, 4864: 0, 4865: 0, 4866: 0, 4867: 0, 4868: 0, 4869: 0, 4870: 0, 4871: 0, 4872: 0, 4873: 0, 4874: 0, 4875: 0, 4876: 0, 4877: 0, 4878: 0, 4879: 0, 4880: 0, 4881: 0, 4882: 0, 4883: 0, 4884: 0, 4885: 0, 4886: 0, 4887: 0, 4888: 0, 4889: 0, 4890: 0, 4891: 0, 4892: 0, 4893: 0, 4894: 0, 4895: 0, 4896: 0, 4897: 0, 4898: 0, 4899: 0, 4900: 0, 4901: 0, 4902: 0, 4903: 0, 4904: 0, 4905: 0, 4906: 0, 4907: 0, 4908: 0, 4909: 0, 4910: 0, 4911: 0, 4912: 0, 4913: 0, 4914: 0, 4915: 0, 4916: 0, 4917: 0, 4918: 0, 4919: 0, 4920: 0, 4921: 0, 4922: 0, 4923: 0, 4924: 0, 4925: 0, 4926: 0, 4927: 0, 4928: 0, 4929: 0, 4930: 0, 4931: 0, 4932: 0, 4933: 0, 4934: 0, 4935: 0, 4936: 0, 4937: 0, 4938: 0, 4939: 0, 4940: 0, 4941: 0, 4942: 0, 4943: 0, 4944: 0, 4945: 0, 4946: 0, 4947: 0, 4948: 0, 4949: 0, 4950: 0, 4951: 0, 4952: 0, 4953: 0, 4954: 0, 4955: 0, 4956: 0, 4957: 0, 4958: 0, 4959: 0, 4960: 0, 4961: 0, 4962: 0, 4963: 0, 4964: 0, 4965: 0, 4966: 0, 4967: 0, 4968: 0, 4969: 0, 4970: 0, 4971: 0, 4972: 0, 4973: 0, 4974: 0, 4975: 0, 4976: 0, 4977: 0, 4978: 0, 4979: 0, 4980: 0, 4981: 0, 4982: 0, 4983: 0, 4984: 0, 4985: 0, 4986: 0, 4987: 0, 4988: 0, 4989: 0, 4990: 0, 4991: 0, 4992: 0, 4993: 0, 4994: 0, 4995: 0, 4996: 0, 4997: 0, 4998: 0, 4999: 0, 5000: 0, 5001: 0, 5002: 0, 5003: 0, 5004: 0, 5005: 0, 5006: 0, 5007: 0, 5008: 0, 5009: 0, 5010: 0, 5011: 0, 5012: 0, 5013: 0, 5014: 0, 5015: 0, 5016: 0, 5017: 0, 5018: 0, 5019: 0, 5020: 0, 5021: 0, 5022: 0, 5023: 0, 5024: 0, 5025: 0, 5026: 0, 5027: 0, 5028: 0, 5029: 0, 5030: 0, 5031: 0, 5032: 0, 5033: 0, 5034: 0, 5035: 0, 5036: 0, 5037: 0, 5038: 0, 5039: 0, 5040: 0, 5041: 0, 5042: 0, 5043: 0, 5044: 0, 5045: 0, 5046: 0, 5047: 0, 5048: 0, 5049: 0, 5050: 0, 5051: 0, 5052: 0, 5053: 0, 5054: 0, 5055: 0, 5056: 0, 5057: 0, 5058: 0, 5059: 0, 5060: 0, 5061: 0, 5062: 0, 5063: 0, 5064: 0, 5065: 0, 5066: 0, 5067: 0, 5068: 0, 5069: 0, 5070: 0, 5071: 0, 5072: 0, 5073: 0, 5074: 0, 5075: 0, 5076: 0, 5077: 0, 5078: 0, 5079: 0, 5080: 0, 5081: 0, 5082: 0, 5083: 0, 5084: 0, 5085: 0, 5086: 0, 5087: 0, 5088: 0, 5089: 0, 5090: 0, 5091: 0, 5092: 0, 5093: 0, 5094: 0, 5095: 0, 5096: 0, 5097: 0, 5098: 0, 5099: 0, 5100: 0, 5101: 0, 5102: 0, 5103: 0, 5104: 0, 5105: 0, 5106: 0, 5107: 0, 5108: 0, 5109: 0, 5110: 0, 5111: 0, 5112: 0, 5113: 0, 5114: 0, 5115: 0, 5116: 0, 5117: 0, 5118: 0, 5119: 0, 5120: 0, 5121: 0, 5122: 0, 5123: 0, 5124: 0, 5125: 0, 5126: 0, 5127: 0, 5128: 0, 5129: 0, 5130: 0, 5131: 0, 5132: 0, 5133: 0, 5134: 0, 5135: 0, 5136: 0, 5137: 0, 5138: 0, 5139: 0, 5140: 0, 5141: 0, 5142: 0, 5143: 0, 5144: 0, 5145: 0, 5146: 0, 5147: 0, 5148: 0, 5149: 0, 5150: 0, 5151: 0, 5152: 0, 5153: 0, 5154: 0, 5155: 0, 5156: 0, 5157: 0, 5158: 0, 5159: 0, 5160: 0, 5161: 0, 5162: 0, 5163: 0, 5164: 0, 5165: 0, 5166: 0, 5167: 0, 5168: 0, 5169: 0, 5170: 0, 5171: 0, 5172: 0, 5173: 0, 5174: 0, 5175: 0, 5176: 0, 5177: 0, 5178: 0, 5179: 0, 5180: 0, 5181: 0, 5182: 0, 5183: 0, 5184: 0, 5185: 0, 5186: 0, 5187: 0, 5188: 0, 5189: 0, 5190: 0, 5191: 0, 5192: 0, 5193: 0, 5194: 0, 5195: 0, 5196: 0, 5197: 0, 5198: 0, 5199: 0, 5200: 0, 5201: 0, 5202: 0, 5203: 0, 5204: 0, 5205: 0, 5206: 0, 5207: 0, 5208: 0, 5209: 0, 5210: 0, 5211: 0, 5212: 0, 5213: 0, 5214: 0, 5215: 0, 5216: 0, 5217: 0, 5218: 0, 5219: 0, 5220: 0, 5221: 0, 5222: 0, 5223: 0, 5224: 0, 5225: 0, 5226: 0, 5227: 0, 5228: 0, 5229: 0, 5230: 0, 5231: 0, 5232: 0, 5233: 0, 5234: 0, 5235: 0, 5236: 0, 5237: 0, 5238: 0, 5239: 0, 5240: 0, 5241: 0, 5242: 0, 5243: 0, 5244: 0, 5245: 0, 5246: 0, 5247: 0, 5248: 0, 5249: 0, 5250: 0, 5251: 0, 5252: 0, 5253: 0, 5254: 0, 5255: 0, 5256: 0, 5257: 0, 5258: 0, 5259: 0, 5260: 0, 5261: 0, 5262: 0, 5263: 0, 5264: 0, 5265: 0, 5266: 0, 5267: 0, 5268: 0, 5269: 0, 5270: 0, 5271: 0, 5272: 0, 5273: 0, 5274: 0, 5275: 0, 5276: 0, 5277: 0, 5278: 0, 5279: 0, 5280: 0, 5281: 0, 5282: 0, 5283: 0, 5284: 0, 5285: 0, 5286: 0, 5287: 0, 5288: 0, 5289: 0, 5290: 0, 5291: 0, 5292: 0, 5293: 0, 5294: 0, 5295: 0, 5296: 0, 5297: 0, 5298: 0, 5299: 0, 5300: 0, 5301: 0, 5302: 0, 5303: 0, 5304: 0, 5305: 0, 5306: 0, 5307: 0, 5308: 0, 5309: 0, 5310: 0, 5311: 0, 5312: 0, 5313: 0, 5314: 0, 5315: 0, 5316: 0, 5317: 0, 5318: 0, 5319: 0, 5320: 0, 5321: 0, 5322: 0, 5323: 0, 5324: 0, 5325: 0, 5326: 0, 5327: 0, 5328: 0, 5329: 0, 5330: 0, 5331: 0, 5332: 0, 5333: 0, 5334: 0, 5335: 0, 5336: 0, 5337: 0, 5338: 0, 5339: 0, 5340: 0, 5341: 0, 5342: 0, 5343: 0, 5344: 0, 5345: 0, 5346: 0, 5347: 0, 5348: 0, 5349: 0, 5350: 0, 5351: 0, 5352: 0, 5353: 0, 5354: 0, 5355: 0, 5356: 0, 5357: 0, 5358: 0, 5359: 0, 5360: 0, 5361: 0, 5362: 0, 5363: 0, 5364: 0, 5365: 0, 5366: 0, 5367: 0, 5368: 0, 5369: 0, 5370: 0, 5371: 0, 5372: 0, 5373: 0, 5374: 0, 5375: 0, 5376: 0, 5377: 0, 5378: 0, 5379: 0, 5380: 0, 5381: 0, 5382: 0, 5383: 0, 5384: 0, 5385: 0, 5386: 0, 5387: 0, 5388: 0, 5389: 0, 5390: 0, 5391: 0, 5392: 0, 5393: 0, 5394: 0, 5395: 0, 5396: 0, 5397: 0, 5398: 0, 5399: 0, 5400: 0, 5401: 0, 5402: 0, 5403: 0, 5404: 0, 5405: 0, 5406: 0, 5407: 0, 5408: 0, 5409: 0, 5410: 0, 5411: 0, 5412: 0, 5413: 0, 5414: 0, 5415: 0, 5416: 0, 5417: 0, 5418: 0, 5419: 0, 5420: 0, 5421: 0, 5422: 0, 5423: 0, 5424: 0, 5425: 0, 5426: 0, 5427: 0, 5428: 0, 5429: 0, 5430: 0, 5431: 0, 5432: 0, 5433: 0, 5434: 0, 5435: 0, 5436: 0, 5437: 0, 5438: 0, 5439: 0, 5440: 0, 5441: 0, 5442: 0, 5443: 0, 5444: 0, 5445: 0, 5446: 0, 5447: 0, 5448: 0, 5449: 0, 5450: 0, 5451: 0, 5452: 0, 5453: 0, 5454: 0, 5455: 0, 5456: 0, 5457: 0, 5458: 0, 5459: 0, 5460: 0, 5461: 0, 5462: 0, 5463: 0, 5464: 0, 5465: 0, 5466: 0, 5467: 0, 5468: 0, 5469: 0, 5470: 0, 5471: 0, 5472: 0, 5473: 0, 5474: 0, 5475: 0, 5476: 0, 5477: 0, 5478: 0, 5479: 0, 5480: 0, 5481: 0, 5482: 0, 5483: 0, 5484: 0, 5485: 0, 5486: 0, 5487: 0, 5488: 0, 5489: 0, 5490: 0, 5491: 0, 5492: 0, 5493: 0, 5494: 0, 5495: 0, 5496: 0, 5497: 0, 5498: 0, 5499: 0, 5500: 0, 5501: 0, 5502: 0, 5503: 0, 5504: 0, 5505: 0, 5506: 0, 5507: 0, 5508: 0, 5509: 0, 5510: 0, 5511: 0, 5512: 0, 5513: 0, 5514: 0, 5515: 0, 5516: 0, 5517: 0, 5518: 0, 5519: 0, 5520: 0, 5521: 0, 5522: 0, 5523: 0, 5524: 0, 5525: 0, 5526: 0, 5527: 0, 5528: 0, 5529: 0, 5530: 0, 5531: 0, 5532: 0, 5533: 0, 5534: 0, 5535: 0, 5536: 0, 5537: 0, 5538: 0, 5539: 0, 5540: 0, 5541: 0, 5542: 0, 5543: 0, 5544: 0, 5545: 0, 5546: 0, 5547: 0, 5548: 0, 5549: 0, 5550: 0, 5551: 0, 5552: 0, 5553: 0, 5554: 0, 5555: 0, 5556: 0, 5557: 0, 5558: 0, 5559: 0, 5560: 0, 5561: 0, 5562: 0, 5563: 0, 5564: 0, 5565: 0, 5566: 0, 5567: 0, 5568: 0, 5569: 0, 5570: 0, 5571: 0, 5572: 0, 5573: 0, 5574: 0, 5575: 0, 5576: 0, 5577: 0, 5578: 0, 5579: 0, 5580: 0, 5581: 0, 5582: 0, 5583: 0, 5584: 0, 5585: 0, 5586: 0, 5587: 0, 5588: 0, 5589: 0, 5590: 0, 5591: 0, 5592: 0, 5593: 0, 5594: 0, 5595: 0, 5596: 0, 5597: 0, 5598: 0, 5599: 0, 5600: 0, 5601: 0, 5602: 0, 5603: 0, 5604: 0, 5605: 0, 5606: 0, 5607: 0, 5608: 0, 5609: 0, 5610: 0, 5611: 0, 5612: 0, 5613: 0, 5614: 0, 5615: 0, 5616: 0, 5617: 0, 5618: 0, 5619: 0, 5620: 0, 5621: 0, 5622: 0, 5623: 0, 5624: 0, 5625: 0, 5626: 0, 5627: 0, 5628: 0, 5629: 0, 5630: 0, 5631: 0, 5632: 0, 5633: 0, 5634: 0, 5635: 0, 5636: 0, 5637: 0, 5638: 0, 5639: 0, 5640: 0, 5641: 0, 5642: 0, 5643: 0, 5644: 0, 5645: 0, 5646: 0, 5647: 0, 5648: 0, 5649: 0, 5650: 0, 5651: 0, 5652: 0, 5653: 0, 5654: 0, 5655: 0, 5656: 0, 5657: 0, 5658: 0, 5659: 0, 5660: 0, 5661: 0, 5662: 0, 5663: 0, 5664: 0, 5665: 0, 5666: 0, 5667: 0, 5668: 0, 5669: 0, 5670: 0, 5671: 0, 5672: 0, 5673: 0, 5674: 0, 5675: 0, 5676: 0, 5677: 0, 5678: 0, 5679: 0, 5680: 0, 5681: 0, 5682: 0, 5683: 0, 5684: 0, 5685: 0, 5686: 0, 5687: 0, 5688: 0, 5689: 0, 5690: 0, 5691: 0, 5692: 0, 5693: 0, 5694: 0, 5695: 0, 5696: 0, 5697: 0, 5698: 0, 5699: 0, 5700: 0, 5701: 0, 5702: 0, 5703: 0, 5704: 0, 5705: 0, 5706: 0, 5707: 0, 5708: 0, 5709: 0, 5710: 0, 5711: 0, 5712: 0, 5713: 0, 5714: 0, 5715: 0, 5716: 0, 5717: 0, 5718: 0, 5719: 0, 5720: 0, 5721: 0, 5722: 0, 5723: 0, 5724: 0, 5725: 0, 5726: 0, 5727: 0, 5728: 0, 5729: 0, 5730: 0, 5731: 0, 5732: 0, 5733: 0, 5734: 0, 5735: 0, 5736: 0, 5737: 0, 5738: 0, 5739: 0, 5740: 0, 5741: 0, 5742: 0, 5743: 0, 5744: 0, 5745: 0, 5746: 0, 5747: 0, 5748: 0, 5749: 0, 5750: 0, 5751: 0, 5752: 0, 5753: 0, 5754: 0, 5755: 0, 5756: 0, 5757: 0, 5758: 0, 5759: 0, 5760: 0, 5761: 0, 5762: 0, 5763: 0, 5764: 0, 5765: 0, 5766: 0, 5767: 0, 5768: 0, 5769: 0, 5770: 0, 5771: 0, 5772: 0, 5773: 0, 5774: 0, 5775: 0, 5776: 0, 5777: 0, 5778: 0, 5779: 0, 5780: 0, 5781: 0, 5782: 0, 5783: 0, 5784: 0, 5785: 0, 5786: 0, 5787: 0, 5788: 0, 5789: 0, 5790: 0, 5791: 0, 5792: 0, 5793: 0, 5794: 0, 5795: 0, 5796: 0, 5797: 0, 5798: 0, 5799: 0, 5800: 0, 5801: 0, 5802: 0, 5803: 0, 5804: 0, 5805: 0, 5806: 0, 5807: 0, 5808: 0, 5809: 0, 5810: 0, 5811: 0, 5812: 0, 5813: 0, 5814: 0, 5815: 0, 5816: 0, 5817: 0, 5818: 0, 5819: 0, 5820: 0, 5821: 0, 5822: 0, 5823: 0, 5824: 0, 5825: 0, 5826: 0, 5827: 0, 5828: 0, 5829: 0, 5830: 0, 5831: 0, 5832: 0, 5833: 0, 5834: 0, 5835: 0, 5836: 0, 5837: 0, 5838: 0, 5839: 0, 5840: 0, 5841: 0, 5842: 0, 5843: 0, 5844: 0, 5845: 0, 5846: 0, 5847: 0, 5848: 0, 5849: 0, 5850: 0, 5851: 0, 5852: 0, 5853: 0, 5854: 0, 5855: 0, 5856: 0, 5857: 0, 5858: 0, 5859: 0, 5860: 0, 5861: 0, 5862: 0, 5863: 0, 5864: 0, 5865: 0, 5866: 0, 5867: 0, 5868: 0, 5869: 0, 5870: 0, 5871: 0, 5872: 0, 5873: 0, 5874: 0, 5875: 0, 5876: 0, 5877: 0, 5878: 0, 5879: 0, 5880: 0, 5881: 0, 5882: 0, 5883: 0, 5884: 0, 5885: 0, 5886: 0, 5887: 0, 5888: 0, 5889: 0, 5890: 0, 5891: 0, 5892: 0, 5893: 0, 5894: 0, 5895: 0, 5896: 0, 5897: 0, 5898: 0, 5899: 0, 5900: 0, 5901: 0, 5902: 0, 5903: 0, 5904: 0, 5905: 0, 5906: 0, 5907: 0, 5908: 0, 5909: 0, 5910: 0, 5911: 0, 5912: 0, 5913: 0, 5914: 0, 5915: 0, 5916: 0, 5917: 0, 5918: 0, 5919: 0, 5920: 0, 5921: 0, 5922: 0, 5923: 0, 5924: 0, 5925: 0, 5926: 0, 5927: 0, 5928: 0, 5929: 0, 5930: 0, 5931: 0, 5932: 0, 5933: 0, 5934: 0, 5935: 0, 5936: 0, 5937: 0, 5938: 0, 5939: 0, 5940: 0, 5941: 0, 5942: 0, 5943: 0, 5944: 0, 5945: 0, 5946: 0, 5947: 0, 5948: 0, 5949: 0, 5950: 0, 5951: 0, 5952: 0, 5953: 0, 5954: 0, 5955: 0, 5956: 0, 5957: 0, 5958: 0, 5959: 0, 5960: 0, 5961: 0, 5962: 0, 5963: 0, 5964: 0, 5965: 0, 5966: 0, 5967: 0, 5968: 0, 5969: 0, 5970: 0, 5971: 0, 5972: 0, 5973: 0, 5974: 0, 5975: 0, 5976: 0, 5977: 0, 5978: 0, 5979: 0, 5980: 0, 5981: 0, 5982: 0, 5983: 0, 5984: 0, 5985: 0, 5986: 0, 5987: 0, 5988: 0, 5989: 0, 5990: 0, 5991: 0, 5992: 0, 5993: 0, 5994: 0, 5995: 0, 5996: 0, 5997: 0, 5998: 0, 5999: 0, 6000: 0, 6001: 0, 6002: 0, 6003: 0, 6004: 0, 6005: 0, 6006: 0, 6007: 0, 6008: 0, 6009: 0, 6010: 0, 6011: 0, 6012: 0, 6013: 0, 6014: 0, 6015: 0, 6016: 0, 6017: 0, 6018: 0, 6019: 0, 6020: 0, 6021: 0, 6022: 0, 6023: 0, 6024: 0, 6025: 0, 6026: 0, 6027: 0, 6028: 0, 6029: 0, 6030: 0, 6031: 0, 6032: 0, 6033: 0, 6034: 0, 6035: 0, 6036: 0, 6037: 0, 6038: 0, 6039: 0, 6040: 0, 6041: 0, 6042: 0, 6043: 0, 6044: 0, 6045: 0, 6046: 0, 6047: 0, 6048: 0, 6049: 0, 6050: 0, 6051: 0, 6052: 0, 6053: 0, 6054: 0, 6055: 0, 6056: 0, 6057: 0, 6058: 0, 6059: 0, 6060: 0, 6061: 0, 6062: 0, 6063: 0, 6064: 0, 6065: 0, 6066: 0, 6067: 0, 6068: 0, 6069: 0, 6070: 0, 6071: 0, 6072: 0, 6073: 0, 6074: 0, 6075: 0, 6076: 0, 6077: 0, 6078: 0, 6079: 0, 6080: 0, 6081: 0, 6082: 0, 6083: 0, 6084: 0, 6085: 0, 6086: 0, 6087: 0, 6088: 0, 6089: 0, 6090: 0, 6091: 0, 6092: 0, 6093: 0, 6094: 0, 6095: 0, 6096: 0, 6097: 0, 6098: 0, 6099: 0, 6100: 0, 6101: 0, 6102: 0, 6103: 0, 6104: 0, 6105: 0, 6106: 0, 6107: 0, 6108: 0, 6109: 0, 6110: 0, 6111: 0, 6112: 0, 6113: 0, 6114: 0, 6115: 0, 6116: 0, 6117: 0, 6118: 0, 6119: 0, 6120: 0, 6121: 0, 6122: 0, 6123: 0, 6124: 0, 6125: 0, 6126: 0, 6127: 0, 6128: 0, 6129: 0, 6130: 0, 6131: 0, 6132: 0, 6133: 0, 6134: 0, 6135: 0, 6136: 0, 6137: 0, 6138: 0, 6139: 0, 6140: 0, 6141: 0, 6142: 0, 6143: 0, 6144: 0, 6145: 0, 6146: 0, 6147: 0, 6148: 0, 6149: 0, 6150: 0, 6151: 0, 6152: 0, 6153: 0, 6154: 0, 6155: 0, 6156: 0, 6157: 0, 6158: 0, 6159: 0, 6160: 0, 6161: 0, 6162: 0, 6163: 0, 6164: 0, 6165: 0, 6166: 0, 6167: 0, 6168: 0, 6169: 0, 6170: 0, 6171: 0, 6172: 0, 6173: 0, 6174: 0, 6175: 0, 6176: 0, 6177: 0, 6178: 0, 6179: 0, 6180: 0, 6181: 0, 6182: 0, 6183: 0, 6184: 0, 6185: 0, 6186: 0, 6187: 0, 6188: 0, 6189: 0, 6190: 0, 6191: 0, 6192: 0, 6193: 0, 6194: 0, 6195: 0, 6196: 0, 6197: 0, 6198: 0, 6199: 0, 6200: 0, 6201: 0, 6202: 0, 6203: 0, 6204: 0, 6205: 0, 6206: 0, 6207: 0, 6208: 0, 6209: 0, 6210: 0, 6211: 0, 6212: 0, 6213: 0, 6214: 0, 6215: 0, 6216: 0, 6217: 0, 6218: 0, 6219: 0, 6220: 0, 6221: 0, 6222: 0, 6223: 0, 6224: 0, 6225: 0, 6226: 0, 6227: 0, 6228: 0, 6229: 0, 6230: 0, 6231: 0, 6232: 0, 6233: 0, 6234: 0, 6235: 0, 6236: 0, 6237: 0, 6238: 0, 6239: 0, 6240: 0, 6241: 0, 6242: 0, 6243: 0, 6244: 0, 6245: 0, 6246: 0, 6247: 0, 6248: 0, 6249: 0, 6250: 0, 6251: 0, 6252: 0, 6253: 0, 6254: 0, 6255: 0, 6256: 0, 6257: 0, 6258: 0, 6259: 0, 6260: 0, 6261: 0, 6262: 0, 6263: 0, 6264: 0, 6265: 0, 6266: 0, 6267: 0, 6268: 0, 6269: 0, 6270: 0, 6271: 0, 6272: 0, 6273: 0, 6274: 0, 6275: 0, 6276: 0, 6277: 0, 6278: 0, 6279: 0, 6280: 0, 6281: 0, 6282: 0, 6283: 0, 6284: 0, 6285: 0, 6286: 0, 6287: 0, 6288: 0, 6289: 0, 6290: 0, 6291: 0, 6292: 0, 6293: 0, 6294: 0, 6295: 0, 6296: 0, 6297: 0, 6298: 0, 6299: 0, 6300: 0, 6301: 0, 6302: 0, 6303: 0, 6304: 0, 6305: 0, 6306: 0, 6307: 0, 6308: 0, 6309: 0, 6310: 0, 6311: 0, 6312: 0, 6313: 0, 6314: 0, 6315: 0, 6316: 0, 6317: 0, 6318: 0, 6319: 0, 6320: 0, 6321: 0, 6322: 0, 6323: 0, 6324: 0, 6325: 0, 6326: 0, 6327: 0, 6328: 0, 6329: 0, 6330: 0, 6331: 0, 6332: 0, 6333: 0, 6334: 0, 6335: 0, 6336: 0, 6337: 0, 6338: 0, 6339: 0, 6340: 0, 6341: 0, 6342: 0, 6343: 0, 6344: 0, 6345: 0, 6346: 0, 6347: 0, 6348: 0, 6349: 0, 6350: 0, 6351: 0, 6352: 0, 6353: 0, 6354: 0, 6355: 0, 6356: 0, 6357: 0, 6358: 0, 6359: 0, 6360: 0, 6361: 0, 6362: 0, 6363: 0, 6364: 0, 6365: 0, 6366: 0, 6367: 0, 6368: 0, 6369: 0, 6370: 0, 6371: 0, 6372: 0, 6373: 0, 6374: 0, 6375: 0, 6376: 0, 6377: 0, 6378: 0, 6379: 0, 6380: 0, 6381: 0, 6382: 0, 6383: 0, 6384: 0, 6385: 0, 6386: 0, 6387: 0, 6388: 0, 6389: 0, 6390: 0, 6391: 0, 6392: 0, 6393: 0, 6394: 0, 6395: 0, 6396: 0, 6397: 0, 6398: 0, 6399: 0, 6400: 0, 6401: 0, 6402: 0, 6403: 0, 6404: 0, 6405: 0, 6406: 0, 6407: 0, 6408: 0, 6409: 0, 6410: 0, 6411: 0, 6412: 0, 6413: 0, 6414: 0, 6415: 0, 6416: 0, 6417: 0, 6418: 0, 6419: 0, 6420: 0, 6421: 0, 6422: 0, 6423: 0, 6424: 0, 6425: 0, 6426: 0, 6427: 0, 6428: 0, 6429: 0, 6430: 0, 6431: 0, 6432: 0, 6433: 0, 6434: 0, 6435: 0, 6436: 0, 6437: 0, 6438: 0, 6439: 0, 6440: 0, 6441: 0, 6442: 0, 6443: 0, 6444: 0, 6445: 0, 6446: 0, 6447: 0, 6448: 0, 6449: 0, 6450: 0, 6451: 0, 6452: 0, 6453: 0, 6454: 0, 6455: 0, 6456: 0, 6457: 0, 6458: 0, 6459: 0, 6460: 0, 6461: 0, 6462: 0, 6463: 0, 6464: 0, 6465: 0, 6466: 0, 6467: 0, 6468: 0, 6469: 0, 6470: 0, 6471: 0, 6472: 0, 6473: 0, 6474: 0, 6475: 0, 6476: 0, 6477: 0, 6478: 0, 6479: 0, 6480: 0, 6481: 0, 6482: 0, 6483: 0, 6484: 0, 6485: 0, 6486: 0, 6487: 0, 6488: 0, 6489: 0, 6490: 0, 6491: 0, 6492: 0, 6493: 0, 6494: 0, 6495: 0, 6496: 0, 6497: 0, 6498: 0, 6499: 0, 6500: 0, 6501: 0, 6502: 0, 6503: 0, 6504: 0, 6505: 0, 6506: 0, 6507: 0, 6508: 0, 6509: 0, 6510: 0, 6511: 0, 6512: 0, 6513: 0, 6514: 0, 6515: 0, 6516: 0, 6517: 0, 6518: 0, 6519: 0, 6520: 0, 6521: 0, 6522: 0, 6523: 0, 6524: 0, 6525: 0, 6526: 0, 6527: 0, 6528: 0, 6529: 0, 6530: 0, 6531: 0, 6532: 0, 6533: 0, 6534: 0, 6535: 0, 6536: 0, 6537: 0, 6538: 0, 6539: 0, 6540: 0, 6541: 0, 6542: 0, 6543: 0, 6544: 0, 6545: 0, 6546: 0, 6547: 0, 6548: 0, 6549: 0, 6550: 0, 6551: 0, 6552: 0, 6553: 0, 6554: 0, 6555: 0, 6556: 0, 6557: 0, 6558: 0, 6559: 0, 6560: 0}\n",
            "Initial state = (0, 0, 0, 0), total reward = 0\n",
            "timestep      0 state = (0, 0, 0, 0) action =    2 demand =    0 new_state = (0, 2, 0, 0) reward =   -12.00 cumulative reward =   -12.00\n",
            "timestep      1 state = (0, 2, 0, 0) action =    2 demand =    0 new_state = (0, 4, 0, 0) reward =   -10.00 cumulative reward =   -22.00\n",
            "timestep      2 state = (0, 4, 0, 0) action =    2 demand =  0.0 new_state = (0, 6, 0, 0) reward =   -10.00 cumulative reward =   -32.00\n",
            "timestep      3 state = (0, 6, 0, 0) action =    2 demand =  3.0 new_state = (-3, 8, 3, 0) reward =   -12.00 cumulative reward =   -44.00\n",
            "timestep      4 state = (-3, 8, 3, 0) action =    5 demand =  0.0 new_state = (-1, 8, 0, 2) reward =   -16.00 cumulative reward =   -60.00\n",
            "timestep      5 state = (-1, 8, 0, 2) action =    3 demand =  1.0 new_state = (-2, 8, 1, 0) reward =   -16.00 cumulative reward =   -76.00\n",
            "timestep      6 state = (-2, 8, 1, 0) action =    4 demand =  2.0 new_state = (-4, 8, 2, 0) reward =   -10.00 cumulative reward =   -86.00\n",
            "timestep      7 state = (-4, 8, 2, 0) action =    6 demand =  0.0 new_state = (-4, 8, 0, 0) reward =   -18.00 cumulative reward =  -104.00\n",
            "timestep      8 state = (-4, 8, 0, 0) action =    6 demand =  1.0 new_state = (-3, 8, 1, 2) reward =   -24.00 cumulative reward =  -128.00\n",
            "timestep      9 state = (-3, 8, 1, 2) action =    5 demand =  4.0 new_state = (-4, 8, 4, 0) reward =   -32.00 cumulative reward =  -160.00\n",
            "timestep     10 state = (-4, 8, 4, 0) action =    6 demand =  1.0 new_state = (-4, 8, 1, 0) reward =   -30.00 cumulative reward =  -190.00\n",
            "timestep     11 state = (-4, 8, 1, 0) action =    6 demand =  2.0 new_state = (-4, 8, 2, 0) reward =   -38.00 cumulative reward =  -228.00\n",
            "timestep     12 state = (-4, 8, 2, 0) action =    6 demand =  1.0 new_state = (-4, 8, 1, 2) reward =   -44.00 cumulative reward =  -272.00\n",
            "timestep     13 state = (-4, 8, 1, 2) action =    6 demand =  2.0 new_state = (-4, 8, 2, 2) reward =   -50.00 cumulative reward =  -322.00\n",
            "timestep     14 state = (-4, 8, 2, 2) action =    6 demand =  1.0 new_state = (-4, 8, 1, 2) reward =   -46.00 cumulative reward =  -368.00\n",
            "timestep     15 state = (-4, 8, 1, 2) action =    6 demand =  1.0 new_state = (-4, 8, 1, 2) reward =   -44.00 cumulative reward =  -412.00\n",
            "timestep     16 state = (-4, 8, 1, 2) action =    6 demand =  0.0 new_state = (-4, 8, 0, 2) reward =   -42.00 cumulative reward =  -454.00\n",
            "timestep     17 state = (-4, 8, 0, 2) action =    6 demand =  1.0 new_state = (-4, 8, 1, 2) reward =   -38.00 cumulative reward =  -492.00\n",
            "timestep     18 state = (-4, 8, 1, 2) action =    6 demand =  1.0 new_state = (-3, 8, 1, 2) reward =   -36.00 cumulative reward =  -528.00\n",
            "timestep     19 state = (-3, 8, 1, 2) action =    5 demand =  0.0 new_state = (-1, 8, 0, 2) reward =   -32.00 cumulative reward =  -560.00\n",
            "timestep     20 state = (-1, 8, 0, 2) action =    3 demand =  1.0 new_state = (0, 8, 1, 2) reward =   -28.00 cumulative reward =  -588.00\n",
            "timestep     21 state = (0, 8, 1, 2) action =    2 demand =  0.0 new_state = (2, 8, 0, 2) reward =   -26.00 cumulative reward =  -614.00\n",
            "timestep     22 state = (2, 8, 0, 2) action =    0 demand =  0.0 new_state = (4, 8, 0, 2) reward =   -24.00 cumulative reward =  -638.00\n",
            "timestep     23 state = (4, 8, 0, 2) action =    0 demand =  1.0 new_state = (4, 8, 1, 2) reward =   -28.00 cumulative reward =  -666.00\n",
            "timestep     24 state = (4, 8, 1, 2) action =    0 demand =  1.0 new_state = (4, 8, 1, 2) reward =   -32.00 cumulative reward =  -698.00\n",
            "timestep     25 state = (4, 8, 1, 2) action =    0 demand =  0.0 new_state = (4, 8, 0, 2) reward =   -34.00 cumulative reward =  -732.00\n",
            "timestep     26 state = (4, 8, 0, 2) action =    0 demand =  0.0 new_state = (4, 8, 0, 2) reward =   -36.00 cumulative reward =  -768.00\n",
            "timestep     27 state = (4, 8, 0, 2) action =    0 demand =  1.0 new_state = (4, 8, 1, 2) reward =   -48.00 cumulative reward =  -816.00\n",
            "timestep     28 state = (4, 8, 1, 2) action =    0 demand =  1.0 new_state = (4, 8, 1, 2) reward =   -52.00 cumulative reward =  -868.00\n",
            "timestep     29 state = (4, 8, 1, 2) action =    0 demand =  4.0 new_state = (4, 8, 4, 2) reward =   -56.00 cumulative reward =  -924.00\n",
            "timestep     30 state = (4, 8, 4, 2) action =    0 demand =  0.0 new_state = (4, 8, 0, 2) reward =   -52.00 cumulative reward =  -976.00\n",
            "timestep     31 state = (4, 8, 0, 2) action =    0 demand =  2.0 new_state = (4, 6, 2, 2) reward =   -38.00 cumulative reward = -1014.00\n",
            "timestep     32 state = (4, 6, 2, 2) action =    0 demand =  0.0 new_state = (4, 6, 0, 2) reward =   -44.00 cumulative reward = -1058.00\n",
            "timestep     33 state = (4, 6, 0, 2) action =    0 demand =  1.0 new_state = (4, 4, 1, 2) reward =   -40.00 cumulative reward = -1098.00\n",
            "timestep     34 state = (4, 4, 1, 2) action =    0 demand =  2.0 new_state = (4, 5, 2, 0) reward =   -42.00 cumulative reward = -1140.00\n",
            "timestep     35 state = (4, 5, 2, 0) action =    0 demand =  1.0 new_state = (4, 7, 1, 0) reward =   -40.00 cumulative reward = -1180.00\n",
            "timestep     36 state = (4, 7, 1, 0) action =    0 demand =  1.0 new_state = (4, 7, 1, 1) reward =   -38.00 cumulative reward = -1218.00\n",
            "timestep     37 state = (4, 7, 1, 1) action =    0 demand =  1.0 new_state = (4, 7, 1, 1) reward =   -36.00 cumulative reward = -1254.00\n",
            "timestep     38 state = (4, 7, 1, 1) action =    0 demand =  2.0 new_state = (4, 8, 2, 0) reward =   -44.00 cumulative reward = -1298.00\n",
            "timestep     39 state = (4, 8, 2, 0) action =    0 demand =  0.0 new_state = (4, 8, 0, 0) reward =   -42.00 cumulative reward = -1340.00\n",
            "timestep     40 state = (4, 8, 0, 0) action =    0 demand =  4.0 new_state = (4, 8, 4, 1) reward =   -42.00 cumulative reward = -1382.00\n",
            "timestep     41 state = (4, 8, 4, 1) action =    0 demand =  1.0 new_state = (4, 8, 1, 1) reward =   -48.00 cumulative reward = -1430.00\n",
            "timestep     42 state = (4, 8, 1, 1) action =    0 demand =  2.0 new_state = (4, 8, 2, 2) reward =   -34.00 cumulative reward = -1464.00\n",
            "timestep     43 state = (4, 8, 2, 2) action =    0 demand =  3.0 new_state = (4, 8, 3, 0) reward =   -38.00 cumulative reward = -1502.00\n",
            "timestep     44 state = (4, 8, 3, 0) action =    0 demand =  0.0 new_state = (4, 8, 0, 2) reward =   -36.00 cumulative reward = -1538.00\n",
            "timestep     45 state = (4, 8, 0, 2) action =    0 demand =  2.0 new_state = (4, 8, 2, 0) reward =   -28.00 cumulative reward = -1566.00\n",
            "timestep     46 state = (4, 8, 2, 0) action =    0 demand =  2.0 new_state = (3, 8, 2, 1) reward =   -30.00 cumulative reward = -1596.00\n",
            "timestep     47 state = (3, 8, 2, 1) action =    0 demand =  1.0 new_state = (4, 8, 1, 2) reward =   -30.00 cumulative reward = -1626.00\n",
            "timestep     48 state = (4, 8, 1, 2) action =    0 demand =  0.0 new_state = (4, 8, 0, 1) reward =   -22.00 cumulative reward = -1648.00\n",
            "timestep     49 state = (4, 8, 0, 1) action =    0 demand =  2.0 new_state = (4, 8, 2, 1) reward =   -26.00 cumulative reward = -1674.00\n",
            "timestep     50 state = (4, 8, 2, 1) action =    0 demand =  0.0 new_state = (4, 8, 0, 1) reward =   -30.00 cumulative reward = -1704.00\n",
            "timestep     51 state = (4, 8, 0, 1) action =    0 demand =  2.0 new_state = (4, 8, 2, 2) reward =   -28.00 cumulative reward = -1732.00\n",
            "timestep     52 state = (4, 8, 2, 2) action =    0 demand =  1.0 new_state = (4, 8, 1, 0) reward =   -32.00 cumulative reward = -1764.00\n",
            "timestep     53 state = (4, 8, 1, 0) action =    0 demand =  1.0 new_state = (4, 8, 1, 2) reward =   -24.00 cumulative reward = -1788.00\n",
            "timestep     54 state = (4, 8, 1, 2) action =    0 demand =  0.0 new_state = (4, 8, 0, 1) reward =   -28.00 cumulative reward = -1816.00\n",
            "timestep     55 state = (4, 8, 0, 1) action =    0 demand =  0.0 new_state = (4, 8, 0, 2) reward =   -30.00 cumulative reward = -1846.00\n",
            "timestep     56 state = (4, 8, 0, 2) action =    0 demand =  0.0 new_state = (4, 7, 0, 2) reward =   -30.00 cumulative reward = -1876.00\n",
            "timestep     57 state = (4, 7, 0, 2) action =    0 demand =  1.0 new_state = (4, 7, 1, 0) reward =   -32.00 cumulative reward = -1908.00\n",
            "timestep     58 state = (4, 7, 1, 0) action =    0 demand =  1.0 new_state = (4, 6, 1, 2) reward =   -36.00 cumulative reward = -1944.00\n",
            "timestep     59 state = (4, 6, 1, 2) action =    0 demand =  1.0 new_state = (4, 5, 1, 2) reward =   -36.00 cumulative reward = -1980.00\n",
            "timestep     60 state = (4, 5, 1, 2) action =    0 demand =  0.0 new_state = (4, 5, 0, 1) reward =   -38.00 cumulative reward = -2018.00\n",
            "timestep     61 state = (4, 5, 0, 1) action =    0 demand =  2.0 new_state = (4, 5, 2, 0) reward =   -36.00 cumulative reward = -2054.00\n",
            "timestep     62 state = (4, 5, 2, 0) action =    0 demand =  1.0 new_state = (4, 5, 1, 2) reward =   -38.00 cumulative reward = -2092.00\n",
            "timestep     63 state = (4, 5, 1, 2) action =    0 demand =  0.0 new_state = (4, 6, 0, 0) reward =   -34.00 cumulative reward = -2126.00\n",
            "timestep     64 state = (4, 6, 0, 0) action =    0 demand =  0.0 new_state = (4, 4, 0, 2) reward =   -32.00 cumulative reward = -2158.00\n",
            "timestep     65 state = (4, 4, 0, 2) action =    0 demand =  2.0 new_state = (4, 4, 2, 0) reward =   -38.00 cumulative reward = -2196.00\n",
            "timestep     66 state = (4, 4, 2, 0) action =    0 demand =  1.0 new_state = (4, 5, 1, 1) reward =   -44.00 cumulative reward = -2240.00\n",
            "timestep     67 state = (4, 5, 1, 1) action =    0 demand =  3.0 new_state = (4, 5, 3, 1) reward =   -40.00 cumulative reward = -2280.00\n",
            "timestep     68 state = (4, 5, 3, 1) action =    0 demand =  2.0 new_state = (4, 7, 2, 0) reward =   -38.00 cumulative reward = -2318.00\n",
            "timestep     69 state = (4, 7, 2, 0) action =    0 demand =  1.0 new_state = (4, 8, 1, 0) reward =   -30.00 cumulative reward = -2348.00\n",
            "timestep     70 state = (4, 8, 1, 0) action =    0 demand =  0.0 new_state = (4, 8, 0, 1) reward =   -26.00 cumulative reward = -2374.00\n",
            "timestep     71 state = (4, 8, 0, 1) action =    0 demand =  2.0 new_state = (4, 8, 2, 1) reward =   -24.00 cumulative reward = -2398.00\n",
            "timestep     72 state = (4, 8, 2, 1) action =    0 demand =  0.0 new_state = (4, 8, 0, 1) reward =   -24.00 cumulative reward = -2422.00\n",
            "timestep     73 state = (4, 8, 0, 1) action =    0 demand =  0.0 new_state = (4, 8, 0, 0) reward =   -24.00 cumulative reward = -2446.00\n",
            "timestep     74 state = (4, 8, 0, 0) action =    0 demand =  0.0 new_state = (4, 7, 0, 2) reward =   -26.00 cumulative reward = -2472.00\n",
            "timestep     75 state = (4, 7, 0, 2) action =    0 demand =  0.0 new_state = (4, 6, 0, 1) reward =   -30.00 cumulative reward = -2502.00\n",
            "timestep     76 state = (4, 6, 0, 1) action =    0 demand =  3.0 new_state = (4, 6, 3, 0) reward =   -34.00 cumulative reward = -2536.00\n",
            "timestep     77 state = (4, 6, 3, 0) action =    0 demand =  1.0 new_state = (4, 8, 1, 0) reward =   -42.00 cumulative reward = -2578.00\n",
            "timestep     78 state = (4, 8, 1, 0) action =    0 demand =  0.0 new_state = (4, 7, 0, 2) reward =   -34.00 cumulative reward = -2612.00\n",
            "timestep     79 state = (4, 7, 0, 2) action =    0 demand =  2.0 new_state = (4, 6, 2, 1) reward =   -30.00 cumulative reward = -2642.00\n",
            "timestep     80 state = (4, 6, 2, 1) action =    0 demand =  1.0 new_state = (4, 6, 1, 2) reward =   -38.00 cumulative reward = -2680.00\n",
            "timestep     81 state = (4, 6, 1, 2) action =    0 demand =  0.0 new_state = (4, 5, 0, 2) reward =   -32.00 cumulative reward = -2712.00\n",
            "timestep     82 state = (4, 5, 0, 2) action =    0 demand =  2.0 new_state = (4, 4, 2, 1) reward =   -30.00 cumulative reward = -2742.00\n",
            "timestep     83 state = (4, 4, 2, 1) action =    0 demand =  1.0 new_state = (4, 6, 1, 0) reward =   -36.00 cumulative reward = -2778.00\n",
            "timestep     84 state = (4, 6, 1, 0) action =    0 demand =  1.0 new_state = (4, 7, 1, 0) reward =   -30.00 cumulative reward = -2808.00\n",
            "timestep     85 state = (4, 7, 1, 0) action =    0 demand =  3.0 new_state = (4, 6, 3, 2) reward =   -32.00 cumulative reward = -2840.00\n",
            "timestep     86 state = (4, 6, 3, 2) action =    0 demand =  1.0 new_state = (4, 8, 1, 0) reward =   -32.00 cumulative reward = -2872.00\n",
            "timestep     87 state = (4, 8, 1, 0) action =    0 demand =  1.0 new_state = (4, 8, 1, 0) reward =   -26.00 cumulative reward = -2898.00\n",
            "timestep     88 state = (4, 8, 1, 0) action =    0 demand =  1.0 new_state = (3, 8, 1, 0) reward =   -30.00 cumulative reward = -2928.00\n",
            "timestep     89 state = (3, 8, 1, 0) action =    0 demand =  3.0 new_state = (2, 8, 3, 2) reward =   -30.00 cumulative reward = -2958.00\n",
            "timestep     90 state = (2, 8, 3, 2) action =    0 demand =  2.0 new_state = (1, 8, 2, 1) reward =   -26.00 cumulative reward = -2984.00\n",
            "timestep     91 state = (1, 8, 2, 1) action =    1 demand =  1.0 new_state = (0, 8, 1, 0) reward =   -20.00 cumulative reward = -3004.00\n",
            "timestep     92 state = (0, 8, 1, 0) action =    2 demand =  0.0 new_state = (2, 8, 0, 2) reward =   -14.00 cumulative reward = -3018.00\n",
            "timestep     93 state = (2, 8, 0, 2) action =    0 demand =  1.0 new_state = (2, 8, 1, 1) reward =   -18.00 cumulative reward = -3036.00\n",
            "timestep     94 state = (2, 8, 1, 1) action =    0 demand =  0.0 new_state = (2, 8, 0, 0) reward =   -22.00 cumulative reward = -3058.00\n",
            "timestep     95 state = (2, 8, 0, 0) action =    0 demand =  3.0 new_state = (1, 8, 3, 2) reward =   -26.00 cumulative reward = -3084.00\n",
            "timestep     96 state = (1, 8, 3, 2) action =    1 demand =  0.0 new_state = (2, 8, 0, 1) reward =   -30.00 cumulative reward = -3114.00\n",
            "timestep     97 state = (2, 8, 0, 1) action =    0 demand =  3.0 new_state = (0, 8, 3, 1) reward =   -20.00 cumulative reward = -3134.00\n",
            "timestep     98 state = (0, 8, 3, 1) action =    2 demand =  1.0 new_state = (1, 8, 1, 2) reward =   -24.00 cumulative reward = -3158.00\n",
            "timestep     99 state = (1, 8, 1, 2) action =    1 demand =  0.0 new_state = (2, 8, 0, 1) reward =   -18.00 cumulative reward = -3176.00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3176.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up TensorFlow\n",
        "\n",
        "Next we'll set up our model in TensorFlow. First, some imports:"
      ],
      "metadata": {
        "id": "_HPX1ppg-TmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wSYk6yFwSZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy \n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "IcG-gFxhSqiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then a helper function to build the TF **model:**"
      ],
      "metadata": {
        "id": "RFO0_ZuC-sTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_states, num_actions):\n",
        "    model = Sequential()    \n",
        "    model.add(Dense(24, activation='relu', input_shape=(1,))) \n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(num_actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "5BNytuonShes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll build the model itself:"
      ],
      "metadata": {
        "id": "Eos3SOdM_KWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shortcut to size of observation and action spaces.\n",
        "num_states = env.observation_space.n\n",
        "#num_states = np.sum([sp.n for sp in env.observation_space.spaces])\n",
        "num_actions = env.action_space.n\n",
        "# Build the model.\n",
        "# NOTE: This must happen *after* the `from rl.x` imports.\n",
        "# (See https://stackoverflow.com/a/72438856/3453768)\n",
        "model = build_model(num_states, num_actions)"
      ],
      "metadata": {
        "id": "B4MEcBFtSndQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a summary of the model:"
      ],
      "metadata": {
        "id": "ZNol9lDX_cfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fizqVg6MSpQs",
        "outputId": "e6336a29-3e61-4609-d994-592a19f53b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 24)                48        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 125       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 773\n",
            "Trainable params: 773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need an RL **agent.** We'll use the `DQNAgent` class built into `keras` (part of TensorFlow). \n",
        "\n",
        "Our agent also needs a **policy.** We'll use the `EpsGreedyQPolicy`, again built into `keras`. (Feel free to play around with different policies. You'll have to `import` them like we did for `EpsGreedyQPolicy` above. I haven't been able to find good documentation for these policies, but you can find different policies to try by looking at the [source code](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).)"
      ],
      "metadata": {
        "id": "j3ziPVMQ_jI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = EpsGreedyQPolicy(eps=0.1) \n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "-1CEDkM4SyOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, build the DQN agent, store it in a variable called `dqn`, and \"compile\" it (a preprocessing step)."
      ],
      "metadata": {
        "id": "gmu1d7Z8BOoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, num_actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJASpBtRS1RF",
        "outputId": "5ea89152-5063-4a46-c98c-adc99a76611e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DQN Agent\n",
        "\n",
        "Now we're finally ready for the main step: training the DQN agent. The command below trains it for 60,000 episodes, which should take about 10 minutes and produce medium-good results. Feel free to change this number to do more or less training."
      ],
      "metadata": {
        "id": "LzS1LLIYBgin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y51byZphBblU",
        "outputId": "d449414c-41de-4085-f7c3-bed2fa7dd5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 60000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "\r    1/10000 [..............................] - ETA: 14:57 - reward: -12.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 108s 11ms/step - reward: -56.5924\n",
            "100 episodes - episode_reward: -5659.240 [-9504.000, -2556.000] - loss: 13369.975 - mae: 744.273 - mean_q: -529.900 - demand: 0.992\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: -59.8534\n",
            "100 episodes - episode_reward: -5985.340 [-9316.000, -2342.000] - loss: 42178.293 - mae: 1511.447 - mean_q: -1825.625 - demand: 0.985\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -58.8002\n",
            "100 episodes - episode_reward: -5880.020 [-9640.000, -2284.000] - loss: 48961.242 - mae: 1659.293 - mean_q: -2009.462 - demand: 0.985\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -58.0348\n",
            "100 episodes - episode_reward: -5803.480 [-9812.000, -2446.000] - loss: 50414.125 - mae: 1689.455 - mean_q: -2048.790 - demand: 0.995\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -61.6584\n",
            "100 episodes - episode_reward: -6165.840 [-9228.000, -2530.000] - loss: 52396.148 - mae: 1697.973 - mean_q: -2060.299 - demand: 0.970\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -55.9898\n",
            "done, took 665.161 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f388eeca350>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most likely, you'll see the `episode_reward` get gradually better as the training progresses (though not necessarily monotonically so)."
      ],
      "metadata": {
        "id": "z6xdFPo6DlvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Results\n",
        "\n",
        "The DQN agent has a feature to test the learned policy by playing multiple episodes and print the results. Let's play 50 of them."
      ],
      "metadata": {
        "id": "aMOS9JTBCrvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = dqn.test(env, nb_episodes=50, visualize=False)\n",
        "print(f\"Average reward per episode = {np.mean(results.history['episode_reward'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0INhM5lxS-ii",
        "outputId": "faa85496-6b63-4984-842f-5445d9a21438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: -3862.000, steps: 100\n",
            "Episode 2: reward: -3276.000, steps: 100\n",
            "Episode 3: reward: -4072.000, steps: 100\n",
            "Episode 4: reward: -3764.000, steps: 100\n",
            "Episode 5: reward: -2666.000, steps: 100\n",
            "Episode 6: reward: -3444.000, steps: 100\n",
            "Episode 7: reward: -3964.000, steps: 100\n",
            "Episode 8: reward: -3650.000, steps: 100\n",
            "Episode 9: reward: -3634.000, steps: 100\n",
            "Episode 10: reward: -5210.000, steps: 100\n",
            "Episode 11: reward: -3924.000, steps: 100\n",
            "Episode 12: reward: -3806.000, steps: 100\n",
            "Episode 13: reward: -3816.000, steps: 100\n",
            "Episode 14: reward: -3512.000, steps: 100\n",
            "Episode 15: reward: -3588.000, steps: 100\n",
            "Episode 16: reward: -3934.000, steps: 100\n",
            "Episode 17: reward: -3974.000, steps: 100\n",
            "Episode 18: reward: -3080.000, steps: 100\n",
            "Episode 19: reward: -3224.000, steps: 100\n",
            "Episode 20: reward: -5602.000, steps: 100\n",
            "Episode 21: reward: -3182.000, steps: 100\n",
            "Episode 22: reward: -3876.000, steps: 100\n",
            "Episode 23: reward: -3912.000, steps: 100\n",
            "Episode 24: reward: -3546.000, steps: 100\n",
            "Episode 25: reward: -4712.000, steps: 100\n",
            "Episode 26: reward: -3624.000, steps: 100\n",
            "Episode 27: reward: -3424.000, steps: 100\n",
            "Episode 28: reward: -3198.000, steps: 100\n",
            "Episode 29: reward: -4070.000, steps: 100\n",
            "Episode 30: reward: -4016.000, steps: 100\n",
            "Episode 31: reward: -3512.000, steps: 100\n",
            "Episode 32: reward: -3268.000, steps: 100\n",
            "Episode 33: reward: -3496.000, steps: 100\n",
            "Episode 34: reward: -2946.000, steps: 100\n",
            "Episode 35: reward: -2940.000, steps: 100\n",
            "Episode 36: reward: -4420.000, steps: 100\n",
            "Episode 37: reward: -3702.000, steps: 100\n",
            "Episode 38: reward: -3156.000, steps: 100\n",
            "Episode 39: reward: -4014.000, steps: 100\n",
            "Episode 40: reward: -3778.000, steps: 100\n",
            "Episode 41: reward: -3672.000, steps: 100\n",
            "Episode 42: reward: -4424.000, steps: 100\n",
            "Episode 43: reward: -3770.000, steps: 100\n",
            "Episode 44: reward: -4312.000, steps: 100\n",
            "Episode 45: reward: -3980.000, steps: 100\n",
            "Episode 46: reward: -2710.000, steps: 100\n",
            "Episode 47: reward: -5712.000, steps: 100\n",
            "Episode 48: reward: -3642.000, steps: 100\n",
            "Episode 49: reward: -3108.000, steps: 100\n",
            "Episode 50: reward: -5026.000, steps: 100\n",
            "Average reward per episode = -3783.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My DQN resulted in an average reward per episode of $-3783.0$. (Your mileage may vary.) Since this is an undiscounted episode with 100 periods, the average cost per period is $37.83$.\n",
        "\n",
        "Using a base-stock policy with a base-stock level of 2 at each node is a reasonable benchmark. `network` is already set up like this, so we can just simulate it."
      ],
      "metadata": {
        "id": "lo7ci-mXv7Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_cost_per_period, _ = sim.run_multiple_trials(network, num_trials=50, num_periods=episode_length)\n",
        "avg_cost_per_period"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ao7pQlbr303",
        "outputId": "6fffc043-2c72-41c8-d7cc-b32526fce4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:04<00:00, 10.39it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.582000000000008"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average cost per period from my simulation is $27.58$. The DQN is not competitive with the base-stock policy, but it's at least in the same ballpark, confirming that we are on the right track. More intensive training should improve the results."
      ],
      "metadata": {
        "id": "DIJo0uRVyLSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If You Have Extra Time\n",
        "\n",
        "Try to improve the results using different hyperparameters, training agents, etc.\n",
        "\n",
        "Or, try using DQN to optimize different supply chain networks other than the beer game system."
      ],
      "metadata": {
        "id": "ITXLNUV10ZDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vUGLTvzmr5oe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}