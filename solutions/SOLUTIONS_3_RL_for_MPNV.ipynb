{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOLUTIONS 3: RL for MPNV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQUSXAz5kgA9moFdSQCyAD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd0eaac467464adfaf639bc41e8f7449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7137ab6d03804ee5a5af2ecec262d696",
              "IPY_MODEL_8815b9d29e0d42caab7d17cd4951e75a",
              "IPY_MODEL_5e32c0f6b9884133ab3e82fd3339035b"
            ],
            "layout": "IPY_MODEL_c86abff7325d4e34bebb51f8f3c5307d"
          }
        },
        "7137ab6d03804ee5a5af2ecec262d696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd4dc0a1bd04de4be88d24d6cd14aaf",
            "placeholder": "​",
            "style": "IPY_MODEL_a44618485bbf4051954f589a215def54",
            "value": "100%"
          }
        },
        "8815b9d29e0d42caab7d17cd4951e75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffb5f8c6a3ed4d478fa68ba6b22b65eb",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a17c8e17ef314869926f3979fb450a99",
            "value": 2000
          }
        },
        "5e32c0f6b9884133ab3e82fd3339035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b505d7c3a23e4a0fa0386fca3221156c",
            "placeholder": "​",
            "style": "IPY_MODEL_0e3abb0e4208404db068967fad94e56c",
            "value": " 2000/2000 [12:44&lt;00:00,  2.93it/s]"
          }
        },
        "c86abff7325d4e34bebb51f8f3c5307d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd4dc0a1bd04de4be88d24d6cd14aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44618485bbf4051954f589a215def54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffb5f8c6a3ed4d478fa68ba6b22b65eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17c8e17ef314869926f3979fb450a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b505d7c3a23e4a0fa0386fca3221156c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3abb0e4208404db068967fad94e56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarrySnyder/RLforInventory/blob/main/solutions/SOLUTIONS_3_RL_for_MPNV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL for the Multi-Period Newsvendor Problem (MPNV) \n",
        "\n",
        "\n",
        "---\n",
        "> **Note:** This file is read-only. To work with it, you first need to save a copy to your Google Drive:\n",
        "> \n",
        "> 1. Go to the File menu. (The File menu inside the notebook, right below the filename—not the File menu in your browser, at the top of your screen.)\n",
        "> 2. Choose Save a copy in Drive. (Log in to your Google account, if necessary.) Feel free to move it to a different folder in your Drive, if you want.\n",
        "> 3. Colab should open up a new browser tab with your copy of the notebook. \n",
        "> 4. Close the original read-only notebook in your browser.\n",
        "---\n",
        "\n",
        "---\n",
        "> This notebook is part of the *Summer Bootcamp at Kellogg: RL in Operations* workshop at Northwestern University, August 2022. The notebooks are for Day 4, taught by Prof. Larry Snyder, Lehigh University.\n",
        "---\n",
        "\n",
        "Recall from the previous notebook that the multi-period newsvendor problem (MPNV) deciding, in each time period, how much to order in advance of observing a random demand. If we begin the period with an inventory level of $s$ (this is the **state**), place an order of size $a$ (this is the **action**), and experience a demand of $d$, then the cost in the period is\n",
        "\n",
        "$$h(s+a-d)^+ + p(d-(s+a))^+.$$\n",
        "\n",
        "The **Bellman equation** for the value function $v_\\pi$ is:\n",
        "\n",
        "$$v_\\pi(s) = {\\mathbb E}_D\\left[ -\\left(h(s + a -d)^+ + p(d - (s+a))^+\\right) + \\gamma v_\\pi(s+a-D)\\right],$$\n",
        "\n",
        "where ${\\mathbb E}_D$ denotes expectation over the random demand. For the optimal policy, the **Bellman optimality equation** is:\n",
        "\n",
        "$$v_*(s) = \\max_{a\\ge 0} {\\mathbb E}_D\\left[ -\\left(h(s+a-d)^+ + p(d-(s+a))^+\\right) + \\gamma v_*(s+a-D)\\right].$$\n",
        "\n"
      ],
      "metadata": {
        "id": "mQmitR3h8t3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the simulation features in `stockpyl` package (https://pypi.org/project/stockpyl/) to build our RL environment. "
      ],
      "metadata": {
        "id": "lHVbwBxRZ1f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Python Stuff\n"
      ],
      "metadata": {
        "id": "XMUCsMuHnN2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll install `stockpyl`. You should only need to do this once. If you get a message like\n",
        "\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [sphinxcontrib]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```\n",
        "\n",
        "you can ignore it.\n",
        "\n"
      ],
      "metadata": {
        "id": "dXIIxdUDaQF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockpyl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OeapNyxaaeYV",
        "outputId": "ee4cb5c3-abe0-46f9-deec-277f6905104f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stockpyl\n",
            "  Downloading stockpyl-0.0.13-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting build>=0.0.2\n",
            "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (0.8.10)\n",
            "Requirement already satisfied: setuptools>=49.6 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (57.4.0)\n",
            "Requirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (4.64.0)\n",
            "Collecting sphinx-toolbox>=3.1.2\n",
            "  Downloading sphinx_toolbox-3.1.2-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting sphinx-rtd-theme>=1.0.0\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 36.1 MB/s \n",
            "\u001b[?25hCollecting sphinx==4.5.0\n",
            "  Downloading Sphinx-4.5.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (3.2.2)\n",
            "Collecting jsonpickle>=1.0\n",
            "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from stockpyl) (2.6.3)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.6.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (4.12.0)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.1.5)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (0.17.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.11.3)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.10.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (1.4.1)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx==4.5.0->stockpyl) (21.3)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx==4.5.0->stockpyl) (2022.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: pep517>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from build>=0.0.2->stockpyl) (0.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx==4.5.0->stockpyl) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx==4.5.0->stockpyl) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0->stockpyl) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0->stockpyl) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (3.0.4)\n",
            "Collecting ruamel.yaml>=0.16.12\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting sphinx-jinja2-compat>=0.1.0\n",
            "  Downloading sphinx_jinja2_compat-0.1.2-py3-none-any.whl (12 kB)\n",
            "Collecting beautifulsoup4>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting apeye>=0.4.0\n",
            "  Downloading apeye-1.2.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachecontrol[filecache]>=0.12.6 in /usr/local/lib/python3.7/dist-packages (from sphinx-toolbox>=3.1.2->stockpyl) (0.12.11)\n",
            "Collecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.2-py3-none-any.whl (12 kB)\n",
            "Collecting dict2css>=0.2.3\n",
            "  Downloading dict2css-0.3.0-py3-none-any.whl (25 kB)\n",
            "Collecting html5lib>=1.1\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.1-py3-none-any.whl (10.0 kB)\n",
            "Collecting sphinx-prompt>=1.1.0\n",
            "  Downloading sphinx_prompt-1.5.0-py3-none-any.whl (4.5 kB)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting domdf-python-tools>=2.9.0\n",
            "  Downloading domdf_python_tools-3.3.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 68.8 MB/s \n",
            "\u001b[?25hCollecting autodocsumm>=0.2.0\n",
            "  Downloading autodocsumm-0.2.9-py3-none-any.whl (13 kB)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting platformdirs>=2.3.0\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting requests>=2.5.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from cachecontrol[filecache]>=0.12.6->sphinx-toolbox>=3.1.2->stockpyl) (1.0.4)\n",
            "Collecting cssutils>=2.2.0\n",
            "  Downloading cssutils-2.5.1-py3-none-any.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 74.5 MB/s \n",
            "\u001b[?25hCollecting natsort>=7.0.1\n",
            "  Downloading natsort-8.1.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.1->sphinx-toolbox>=3.1.2->stockpyl) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.5.0->stockpyl) (2.1.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting sphinx-autodoc-typehints>=1.11.1\n",
            "  Downloading sphinx_autodoc_typehints-1.19.1-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-tabs<3.5.0,>=1.2.1\n",
            "  Downloading sphinx_tabs-3.4.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, requests, natsort, sphinx, soupsieve, ruamel.yaml.clib, platformdirs, mypy-extensions, lockfile, domdf-python-tools, cssutils, typing-inspect, sphinx-tabs, sphinx-prompt, sphinx-jinja2-compat, sphinx-autodoc-typehints, ruamel.yaml, html5lib, dict2css, beautifulsoup4, autodocsumm, apeye, sphinx-toolbox, sphinx-rtd-theme, jsonpickle, build, stockpyl\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: natsort\n",
            "    Found existing installation: natsort 5.5.0\n",
            "    Uninstalling natsort-5.5.0:\n",
            "      Successfully uninstalled natsort-5.5.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed apeye-1.2.0 autodocsumm-0.2.9 beautifulsoup4-4.11.1 build-0.8.0 cssutils-2.5.1 dict2css-0.3.0 domdf-python-tools-3.3.0 html5lib-1.1 jsonpickle-2.2.0 lockfile-0.12.2 mypy-extensions-0.4.3 natsort-8.1.0 platformdirs-2.5.2 requests-2.28.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 soupsieve-2.3.2.post1 sphinx-4.5.0 sphinx-autodoc-typehints-1.19.1 sphinx-jinja2-compat-0.1.2 sphinx-prompt-1.5.0 sphinx-rtd-theme-1.0.0 sphinx-tabs-3.4.0 sphinx-toolbox-3.1.2 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 stockpyl-0.0.13 typing-inspect-0.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ytg9sReD8fRd"
      },
      "outputs": [],
      "source": [
        "# Import the packages we will need.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from tabulate import tabulate\n",
        "from stockpyl.supply_chain_network import single_stage_system\n",
        "from stockpyl.newsvendor import newsvendor_poisson\n",
        "from stockpyl import sim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Environment\n",
        "\n",
        "The code below creates an environment class for the MPNV. The class implements functions `reset()` and `step()` to initialize the environment and simulate one time step. \n",
        "\n",
        "* `reset()` returns the initial state of the environment\n",
        "* `step(action)` takes the action specified and returns a tuple `(new_state, reward, done)`, where `done` is a flag indicating whether the episode has terminated\n",
        "\n",
        "(If you are familiar with OpenAI `gym`, these functions will be familiar to you.)\n",
        "\n",
        "The code is missing some pieces. Your job is to fill in the missing pieces.\n",
        "\n",
        "---\n",
        "> **Note:** In the code below, the portions that you need to complete are marked with\n",
        "> \n",
        "> ```python\n",
        "> # #################\n",
        "> # TODO:\n",
        "> ```\n",
        "> \n",
        "> In place of the missing code is a line that says \n",
        "> \n",
        "> ```python\n",
        "> \traise NotImplementedError\n",
        "> ```\n",
        "> \n",
        "> This is a way of telling Python to raise an exception (error) because there's something missing here. You should **delete (or comment out) this line** after you write your code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V7Y48VMEa5SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNVEnv(object):\n",
        "    \"\"\"Multi-period newsvendor (MPNV) problem environment. A state represents an inventory level. \n",
        "    An action is an order quantity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : SupplyChainNetwork\n",
        "        The network to simulate.\n",
        "    episode_length : int\n",
        "        The number of periods in one episode.\n",
        "    min_state : int\n",
        "        The minimum value of the state space to consider.\n",
        "    max_state : int\n",
        "        The maximum value of the state space to consider.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network, episode_length: int, min_state: int, max_state: int, \\\n",
        "        gamma: float = 0.95):\n",
        "\n",
        "        # Store problem data.\n",
        "        self.network = network\n",
        "        self.episode_length = episode_length\n",
        "        self.min_state = min_state\n",
        "        self.max_state = max_state\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Build list of state and action spaces. State space is specified by min_state and max_state.\n",
        "        # Action space is the largest possible order quantity, i.e., starting with IL = min_state\n",
        "        # and ordering up to max_state.\n",
        "        self.state_space = list(range(min_state, max_state + 1))\n",
        "        self.action_space = list(range(max_state - min_state + 1))\n",
        "\n",
        "        # Initial states are the non-negative states. (Of course it's possible to start with a negative\n",
        "        # IL, but let's keep things simpler.)\n",
        "        self.initial_states = list(range(max_state + 1))\n",
        "\n",
        "        # Calculate allowable actions for each state. \n",
        "        # In state x, allowable actions are {0, ..., x_max - x}, where x_max is\n",
        "        # the upper range of the state space. \n",
        "        self.allowable_actions = {}\n",
        "        for s in self.state_space:\n",
        "            min_a = 0\n",
        "            max_a = max(self.state_space) - s\n",
        "            self.allowable_actions[s] = list(range(min_a, max_a + 1))\n",
        "\n",
        "        # Initialize current state info.\n",
        "        self.state = None\n",
        "\n",
        "        # Get a shortcut to the (single) node in the network, for convenience.\n",
        "        self.node = network.nodes[0]\n",
        "\n",
        "    def get_random_action(self, state):\n",
        "        \"\"\"Return a randomly selected action, with equal probability, for the\n",
        "        given state, chosen from the state's allowable actions.\"\"\"\n",
        "        action = np.random.choice(self.allowable_actions[state])\n",
        "        return action\n",
        "\n",
        "    def get_greedy_action(self, state, Q):\n",
        "        \"\"\"Return a greedy action, i.e., an action `a` that maximizes `Q[state, a]`.\"\"\"\n",
        "        # Set `action` to an action that maximizes `Q[state, . ]`.\n",
        "        allowable = self.allowable_actions[state]\n",
        "        a_ind = np.argmax([Q[state, a] for a in allowable])\n",
        "        action = allowable[a_ind]\n",
        "        return action\n",
        "\n",
        "    def get_epsilon_greedy_action(self, state, Q, epsilon):\n",
        "        \"\"\"Return an epsilon-greedy action, i.e., with probability epsilon choose a\n",
        "        random action from the allowable actions for the state, and with probability\n",
        "        1 - epsilon choose a greedy action for the state. i.e., an action `a` that \n",
        "        maximizes `Q[state, a]`.\"\"\"\n",
        "        # Set `action`: with probability epsilon, choose a random action, and\n",
        "        # with probability 1 - epsilon, choose a greedy action. \n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.get_random_action(state)\n",
        "        else:\n",
        "            action = self.get_greedy_action(state, Q)\n",
        "   \n",
        "        return action\n",
        "\n",
        "    def get_greedy_policy(self, Q):\n",
        "        \"\"\"Return a greedy policy, i.e., in which the action `a` for every state `s`\n",
        "        maximizes `Q[state, a]`.\"\"\"\n",
        "        return {s: self.get_greedy_action(s, Q) for s in self.state_space}\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment and the simulation. Choose an initial state randomly from\n",
        "        the list of possible initial states. Return it and set it in self.inventory_level.\"\"\"\n",
        "\n",
        "        # Determine initial IL and store it in environment's state.\n",
        "        initial_state = np.random.choice(self.initial_states)\n",
        "        self.state = initial_state\n",
        "        \n",
        "        # Set node's initial IL attribute. (This will force the simulation to start with\n",
        "        # the node at this inventory level.)\n",
        "        self.node.initial_inventory_level = initial_state\n",
        "\n",
        "        # Reset the simulation environment.\n",
        "        sim.initialize(self.network, self.episode_length)\n",
        "\n",
        "        return initial_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Run one time step of the environment by taking the specified action.\n",
        "        Update the environment state to the new state. \n",
        "        Return a tuple (new_state, reward, done, info).\"\"\"\n",
        "        # Build a dict specifying the order quantity to be used in this time\n",
        "        # period. The dict should have a single key (equal to `self.node`)\n",
        "        # and a single value (equal to the order quantity). \n",
        "        # Build dict specifying order quantity to use in this time period.\n",
        "        # (This dict will be used to override the order quantity that the \n",
        "        # stockpyl simulation would choose on its own.)\n",
        "        # IMPORTANT: If the order quantity would make the inventory level \n",
        "        # greater than `max_state`, truncate it so the inventory level equals\n",
        "        # `max_state`.\n",
        "        order_quantity = min(action, self.max_state - self.state)\n",
        "        order_quantity_override = {self.node: order_quantity}\n",
        "\n",
        "        # Simulate one time period.\n",
        "        sim.step(self.network, order_quantity_override=order_quantity_override)\n",
        "\n",
        "        # Determine reward by querying the simulation's state variables.\n",
        "        reward = -self.node.state_vars_current.total_cost_incurred\n",
        "\n",
        "        # If episode length has been reached, terminate.\n",
        "        done = self.network.period == self.episode_length - 1\n",
        "\n",
        "        # Get new inventory level from simulation. \n",
        "        # (Round to int -- should already be integer but sometimes there are small rounding errors.)\n",
        "        IL = int(self.node.state_vars_current.inventory_position())\n",
        "        # If new IL is outside the bounds of state space, truncate.\n",
        "        IL = int(np.clip(IL, self.min_state, self.max_state))\n",
        "\n",
        "        # Update state.\n",
        "        self.state = IL\n",
        "\n",
        "        # Fill the demand into the info dict.\n",
        "        info = {'demand': self.node.state_vars_current.inbound_order[None]}\n",
        "\n",
        "        # Return a tuple (new_state, reward, done, info).\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def play_episode(self, policy, messages=False):\n",
        "        \"\"\"Play one episode of the environment following the specified policy. \n",
        "        Return the total discounted reward over the episode.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize environment.\n",
        "        self.reset()\n",
        "        cumul_reward = 0\n",
        "\n",
        "        if messages:\n",
        "            print(f\"policy = {policy}\")\n",
        "            print(f\"Initial state = {self.state}, total reward = {cumul_reward}\")\n",
        "\n",
        "        # Step through until terminal state reached.\n",
        "        for t in range(self.episode_length):\n",
        "            \n",
        "            # Determine action.\n",
        "            action = policy[self.state]\n",
        "\n",
        "            if messages:\n",
        "                print(f\"timestep {t:6} state = {self.state:4} action = {action:4} \", end=\"\")\n",
        "\n",
        "            # Step.\n",
        "            new_state, reward, done, info = self.step(action)\n",
        "\n",
        "            # Update cumulative reward.\n",
        "            cumul_reward += self.gamma**t * reward\n",
        "\n",
        "            if messages:\n",
        "                print(f\"demand = {info['demand']:4} new_state = {new_state:4} reward = {reward:8.2f} total reward = {cumul_reward:8.2f}\")\n",
        "\n",
        "        return cumul_reward\n",
        "\n",
        "    def play_episode_batch(self, policy, num_episodes):\n",
        "        \"\"\"Play `num_episodes` episode of the environment following the specified policy. \n",
        "        Return the average total discounted reward over all episodes.\n",
        "\n",
        "        `policy` is a dict in which keys are states and values are actions.\n",
        "        If `messages` is True, will print state and action in each time step.\"\"\"\n",
        "\n",
        "        avg_total_reward = 0\n",
        "        for _ in range(num_episodes):\n",
        "            self.reset()\n",
        "            total_reward = self.play_episode(policy, messages=False)\n",
        "            avg_total_reward += total_reward / num_episodes\n",
        "\n",
        "        return avg_total_reward"
      ],
      "metadata": {
        "id": "fyiDsHkocwa0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function plots a given policy in two ways: order quantity vs. inventory position and order-up-to level (= inventory position + order quantity) vs. inventory position."
      ],
      "metadata": {
        "id": "p09GZHyg_wuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy(env: MPNVEnv, policy: dict, title: str = None):\n",
        "    \"\"\"Plot the policy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pi : \n",
        "        Probability distribution for a policy. A dict whose keys are states and\n",
        "        whose values are actions. (Note that this is a different structure\n",
        "        than what was used in the \"MPNV as MDP\" notebook.)\n",
        "    title : \n",
        "        Optional title for the figure.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=plt.figaspect(1/2))\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Order quantity plot.\n",
        "    ax = plt.subplot(121)\n",
        "    x_list = env.state_space\n",
        "    y_list = [policy[x] for x in x_list]\n",
        "    plt.plot(x_list, y_list)\n",
        "    plt.xlabel('Starting Inventory Level')\n",
        "    plt.ylabel('Order Quantity')\n",
        "\n",
        "    # Order-up-to level plot.\n",
        "    ax = plt.subplot(122)\n",
        "    y_list = [x + policy[x] for x in x_list]\n",
        "    plt.plot(x_list, y_list)\n",
        "    plt.xlabel('Starting Inventory Level')\n",
        "    plt.ylabel('Order-Up-To Level')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "P19CPSZkvbze"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular RL\n",
        "\n",
        "Now our goal is to implement one or more tabular RL algorithms and apply them to the MPNV environment you built above.\n",
        "\n",
        "---\n",
        "> At this point you have three choices:\n",
        "> 1. Plug in tabular RL code that you wrote earlier in the workshop or at some other point.\n",
        "> 2. Use RL code from an external Python package or repo.\n",
        "> 3. Use my RL code that is already pasted below.\n",
        ">\n",
        "> If you go with option 1 or 2, your tabular algorithm must be able to interact with the `MPNVEnv` class you built above. In particular, it should call the `reset()` and `step()` functions to initialize the environment and to take one step through the simulation. (This is a pretty standard API structure, and many RL codes use it.) Your code can also access other features we built into the `MPNVEnv` class, such as `allowable_actions`, `get_epsilon_greedy_action()`, etc., but that is optional.\n",
        ">\n",
        "> Option 3 is probably the simplest, since the work is already done. However, my RL code is extremely no-frills; it is not as robust or efficient as many existing RL implementations. \n",
        "---\n",
        "\n",
        "Here is my tabular RL code:"
      ],
      "metadata": {
        "id": "ALSyN7pu_z2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TabularRL():\n",
        "    \"\"\"Class for tabular reinforcement learning for MPNV problem. (The code is fairly generic, but it assumes\n",
        "    data structures and state/action encodings that are specific to the MPNV.)\n",
        "    \"\"\"\n",
        "    def __init__(self, env: MPNVEnv = None):\n",
        "        self.env = env\n",
        "\n",
        "    def sarsa(self,\n",
        "            num_episodes: int,\n",
        "            initial_Q: dict = None,\n",
        "            epsilon: float = 0.1,\n",
        "            step_size: float = 0.5,     # alpha\n",
        "            progress_bar: bool = True,\n",
        "            messages: bool = False\n",
        "        ):\n",
        "        \"\"\"Sarsa (on-policy control) for estimating Q ~ q_* (Sutton and Barto, Section 6.5, p. 130.).\n",
        "        Returns Q and the final policy.\"\"\"\n",
        "        \n",
        "        # Initialize.\n",
        "        Q = initial_Q if initial_Q else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "\n",
        "        # Intialize progress bar.\n",
        "        pbar = tqdm(total=num_episodes, disable=not progress_bar)\n",
        "\n",
        "        # Loop through episodes.\n",
        "        for _ in range(num_episodes):\n",
        "\n",
        "            # Update progress bar.\n",
        "            pbar.update()\n",
        "\n",
        "            # Reset the environment and get first action.\n",
        "            S = self.env.reset()\n",
        "            A = self.env.get_epsilon_greedy_action(S, Q, epsilon)\n",
        "\n",
        "            # Loop through time steps.\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                # Take action.\n",
        "                S_prime, R, done, info = self.env.step(A)\n",
        "\n",
        "                # Choose A' from S'.\n",
        "                A_prime = self.env.get_epsilon_greedy_action(S_prime, Q, epsilon)\n",
        "\n",
        "                # Update Q.\n",
        "                Q[S, A] += step_size * (R + self.env.gamma * Q[S_prime, A_prime] - Q[S, A])\n",
        "\n",
        "                if messages:\n",
        "                    print(f\"S = {S} A = {A} R = {R} S' = {S_prime} A' = {A_prime}\")\n",
        "\n",
        "                # Update S and A.\n",
        "                S = S_prime\n",
        "                A = A_prime\n",
        "\n",
        "        return Q, self.env.get_greedy_policy(Q)\n",
        "\n",
        "    def Q_learning(self,\n",
        "            num_episodes: int,\n",
        "            initial_Q: dict = None,\n",
        "            epsilon: float = 0.1,\n",
        "            step_size: float = 0.5,     # alpha\n",
        "            progress_bar: bool = True,\n",
        "            messages: bool = False,\n",
        "            test_freq: int = None,\n",
        "            test_episodes: int = None\n",
        "        ):\n",
        "        \"\"\"Q-learning (off-policy control) for estimating pi ~ pi_* (Section 6.5, p. 130.).\n",
        "        Returns Q and the final policy.\n",
        "        \n",
        "        If ``test_freq`` is not None, the current policy is tested every ``test_freq`` episodes by\n",
        "        running ``test_epidodes`` episodes and printing the average reward. (This is mostly for \n",
        "        tracking the progress of the training while it's going on.)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize.\n",
        "        Q = initial_Q if initial_Q else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "\n",
        "        # Intialize progress bar.\n",
        "        pbar = tqdm(total=num_episodes, disable=not progress_bar)\n",
        "\n",
        "        # Loop through episodes.\n",
        "        for ep in range(num_episodes):\n",
        "\n",
        "            # Update progress bar.\n",
        "            pbar.update()\n",
        "\n",
        "            # Test.\n",
        "            if test_freq and ep % test_freq == 0:\n",
        "                pol = self.env.get_greedy_policy(Q)\n",
        "                avg_reward = self.env.play_episode_batch(policy=pol, num_episodes=test_episodes)\n",
        "                tqdm.write(f\"Training episode {ep:8d}, ran {test_episodes} episodes, average reward per episode = {avg_reward:8.4f}\")\n",
        "\n",
        "            # Reset the environment.\n",
        "            S = self.env.reset()\n",
        "\n",
        "            # Loop through time steps.\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                # Choose action.\n",
        "                A = self.env.get_epsilon_greedy_action(S, Q, epsilon)\n",
        "\n",
        "                # Take action.\n",
        "                S_prime, R, done, info = self.env.step(A)\n",
        "\n",
        "                # Update Q.\n",
        "                max_Q = max([Q[S_prime, a] for a in self.env.allowable_actions[S_prime]])\n",
        "                Q[S, A] += step_size * (R + self.env.gamma * max_Q - Q[S, A])\n",
        "\n",
        "                if messages:\n",
        "                    print(f\"S = {S} A = {A} R = {R} S' = {S_prime} max_Q = {max_Q}\")\n",
        "\n",
        "                # Update S.\n",
        "                S = S_prime\n",
        "\n",
        "        return Q, self.env.get_greedy_policy(Q)\n",
        "\n",
        "\n",
        "    def double_Q_learning(self,\n",
        "            num_episodes: int,\n",
        "            initial_Q1: dict = None,\n",
        "            initial_Q2: dict = None,\n",
        "            epsilon: float = 0.1,\n",
        "            step_size: float = 0.5,     # alpha\n",
        "            progress_bar: bool = True,\n",
        "            messages: bool = False\n",
        "        ):\n",
        "        \"\"\"Double Q-learning for estimating Q_1 ~ Q_2 ~ q_* (Section 6.7, p. 136.).\n",
        "        Returns Q and the final policy.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize.\n",
        "        Q1 = initial_Q1 if initial_Q1 else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "        Q2 = initial_Q2 if initial_Q2 else \\\n",
        "            {(s, a): 0 for s in self.env.state_space for a in self.env.allowable_actions[s]}\n",
        "\n",
        "        # Intialize progress bar.\n",
        "        pbar = tqdm(total=num_episodes, disable=not progress_bar)\n",
        "\n",
        "        # Loop through episodes.\n",
        "        for _ in range(num_episodes):\n",
        "\n",
        "            # Update progress bar.\n",
        "            pbar.update()\n",
        "\n",
        "            # Choose initial state.\n",
        "            S = self.env.get_initial_state()\n",
        "\n",
        "            # Reset the environment.\n",
        "            self.env.reset()\n",
        "\n",
        "            # Loop through time steps.\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                # Choose action.\n",
        "                Q_sum = {(s, a): Q1[s, a] + Q2[s, a] for s, a in Q1.keys()}\n",
        "                A = self.env.get_epsilon_greedy_action(S, Q_sum, epsilon)\n",
        "\n",
        "                # Take action.\n",
        "                S_prime, R, done, info = self.env.step(A)\n",
        "\n",
        "                # Update Q.\n",
        "                if np.random.random() < 0.5:\n",
        "                    # Get a that maximizes Q1[S', a].\n",
        "                    Q_S_prime = {a: Q1[S_prime, a] for a in self.env.allowable_actions[S_prime]}\n",
        "                    argmax_Q = max(Q_S_prime, key=Q_S_prime.get)\n",
        "                    Q1[S, A] += step_size * (R + self.env.gamma * Q2[S_prime, argmax_Q] - Q1[S, A])\n",
        "                else:\n",
        "                    Q_S_prime = {a: Q2[S_prime, a] for a in self.env.allowable_actions[S_prime]}\n",
        "                    argmax_Q = max(Q_S_prime, key=Q_S_prime.get)\n",
        "                    Q2[S, A] += step_size * (R + self.env.gamma * Q1[S_prime, argmax_Q] - Q2[S, A])\n",
        "\n",
        "                if messages:\n",
        "                    print(f\"S = {S} A = {A} R = {R} S' = {S_prime} argmax_Q = {argmax_Q}\")\n",
        "\n",
        "                # Update S.\n",
        "                S = S_prime\n",
        "            \n",
        "        Q_sum = {(s, a): Q1[s, a] + Q2[s, a] for s, a in Q1.keys()}\n",
        "        return Q_sum, self.env.get_greedy_policy(Q_sum)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "NKnNvRQGvd_u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MPNV Instance\n",
        "\n",
        "Let's use the same instance we used in the \"MPNV as MDP\" notebook:\n",
        "\n",
        "* $h = 1$\n",
        "* $p = 10$\n",
        "* $\\mu = 5$\n",
        "\n"
      ],
      "metadata": {
        "id": "zJj8KeyiVyn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build stockpyl SupplyChainNetwork object.\n",
        "network = single_stage_system(\n",
        "    local_holding_cost=1,\n",
        "    stockout_cost=10,\n",
        "    demand_type='P',\n",
        "    mean=5,\n",
        "    shipment_lead_time=1    # this is necessary to convert between our sequence of events and stockpyl's\n",
        ")"
      ],
      "metadata": {
        "id": "VoLvgiEuvyUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the MDP notebook, we set the state space limits to $[-15, 20]$. Here we'll use a narrow range in order to help the RL agent train more quickly:"
      ],
      "metadata": {
        "id": "cUcqFZ8FWagc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_state = -10\n",
        "max_state = 10"
      ],
      "metadata": {
        "id": "IRqXIvULW21d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and testing parameters."
      ],
      "metadata": {
        "id": "2u55g2hxXKmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_length = 1000\n",
        "num_training_episodes = 2000\n",
        "num_testing_episodes = 100\n",
        "in_training_test_freq = 1000\n",
        "in_training_test_episodes = 100\n",
        "trl_epsilon = 0.05\n",
        "trl_step_size = 0.1"
      ],
      "metadata": {
        "id": "I0o-umG2vp-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's build the `MPNVEnv` and `TDLearning` objects."
      ],
      "metadata": {
        "id": "XESXIFZMXbg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MPNVEnv object.\n",
        "env = MPNVEnv(\n",
        "    network=network,\n",
        "    episode_length=episode_length,\n",
        "    min_state=min_state,\n",
        "    max_state=max_state,\n",
        "    gamma=0.95\n",
        ")"
      ],
      "metadata": {
        "id": "t_V7n7b4v0Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build tabular RL object.\n",
        "trl = TabularRL(env)"
      ],
      "metadata": {
        "id": "TImGPz3iv2w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Let's train the RL agent! If you used my tabular RL code, you can choose either the `sarsa()`, `Q_learning()`, or `double_Q_learning()` functions. If you used your own code, plug in the appropriate call here.\n",
        "\n",
        "This will take several minutes (or more, depending on your settings) to execute."
      ],
      "metadata": {
        "id": "rv0Crh5VXygx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run tabular RL algorithm.\n",
        "#\tQ, pol = td.sarsa(\n",
        "Q, pol = trl.Q_learning(\n",
        "#\tQ, pol = td.double_Q_learning(\n",
        "    num_episodes=num_training_episodes,\n",
        "    epsilon=trl_epsilon,\n",
        "    step_size=trl_step_size,\n",
        "    progress_bar=True,\n",
        "    messages=False,\n",
        "    test_freq=in_training_test_freq,\n",
        "    test_episodes=in_training_test_episodes\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "bd0eaac467464adfaf639bc41e8f7449",
            "7137ab6d03804ee5a5af2ecec262d696",
            "8815b9d29e0d42caab7d17cd4951e75a",
            "5e32c0f6b9884133ab3e82fd3339035b",
            "c86abff7325d4e34bebb51f8f3c5307d",
            "bdd4dc0a1bd04de4be88d24d6cd14aaf",
            "a44618485bbf4051954f589a215def54",
            "ffb5f8c6a3ed4d478fa68ba6b22b65eb",
            "a17c8e17ef314869926f3979fb450a99",
            "b505d7c3a23e4a0fa0386fca3221156c",
            "0e3abb0e4208404db068967fad94e56c"
          ]
        },
        "id": "gp2Izr-Hv6MC",
        "outputId": "81f56d53-6275-474d-95a8-38f60abeaab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd0eaac467464adfaf639bc41e8f7449"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training episode        0, ran 100 episodes, average reward per episode = -18954.0731\n",
            "Training episode     1000, ran 100 episodes, average reward per episode = -275.5872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Results\n",
        "\n",
        "We'll plot the policy below. Remember that we're hoping to see a **base-stock policy,** which will be evident if the plot on the left (order quantity vs. inventory position) should decrease linearly with a slope of $-1$ until it flattens out at $y=0$; and the plot on the right (order-up-to level vs. inventory position) should be flat at first and then increase linearly with a slope of $1$."
      ],
      "metadata": {
        "id": "U3CYnAB6Y_Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot policy.\n",
        "title=f\"{num_training_episodes} training episodes, {num_testing_episodes} testing episodes, {episode_length} periods per episode\"\n",
        "plot_policy(env=env, policy=pol, title=title)\n"
      ],
      "metadata": {
        "id": "AbzPygPXwI3G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "6eb5be33-b3de-4f1e-dc52-0698a3e8f04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your RL agent performed similarly to mine, the plot is base-stock-ish, but not quite a base-stock policy. That's OK—we did a quicky and dirty training. More careful and longer training will result in a more accurate solution."
      ],
      "metadata": {
        "id": "58HFaSvlZbFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> If you wish, play around with the hyperparameters to try to improve the training process.\n",
        "---"
      ],
      "metadata": {
        "id": "NVHiY9S_ZrBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also test the policy by playing a batch of episodes."
      ],
      "metadata": {
        "id": "ckQmaMgAYr1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test policy.\n",
        "avg_reward = env.play_episode_batch(pol, num_testing_episodes)\n",
        "print(f\"Average reward per episode = {avg_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmS_k4gQwAZk",
        "outputId": "c212a1cd-f642-4dbf-b50e-d15e2c3291dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward per episode = -296.21957094001186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that the problem can be solved analytically. Let's compare the RL agent's solution with the analytical solution. The code below reports both the optimal expected cost per period and the corresponding expected discount cost over the horizon. \n",
        "\n",
        "The expected discounted horizon cost calculation uses the fact that, as in the \"MPNV as MAB\" notebook, we can convert an expected cost per period, $g$, to an expected discounte cost over $T$ time periods, as follows:\n",
        "\n",
        "$$\\sum_{t=0}^{T-1} g\\gamma^i = g\\frac{1 - \\gamma^{T}}{1-\\gamma}$$\n"
      ],
      "metadata": {
        "id": "P0VjbU7kYyFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to analytical solution.\n",
        "n = network.nodes[0]\n",
        "opt_S, opt_cost = newsvendor_poisson(\n",
        "    holding_cost=n.local_holding_cost,\n",
        "    stockout_cost=n.stockout_cost,\n",
        "    demand_mean=n.demand_source.mean\n",
        ")\n",
        "exp_disc_cost = opt_cost * (1 - env.gamma**episode_length) / (1 - env.gamma)\n",
        "print(f\"Optimal base-stock level = {opt_S} with expected cost per period = {opt_cost:.4f} and expected discounted horizon cost {exp_disc_cost:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKjIJ4nVwBMm",
        "outputId": "e6b06e38-5b9b-4c8f-e451-73035ba52e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal base-stock level = 8.0 with expected cost per period = 4.3432 and expected discounted horizon cost 86.8640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also simulate the system using `stockpyl` to get an average cost per period."
      ],
      "metadata": {
        "id": "aUKL4fxz9FpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate system. First we have to specify the inventory policy that the node\n",
        "# should use (base-stock with base-stock level 8).\n",
        "from stockpyl.policy import Policy\n",
        "network.nodes[0].inventory_policy = Policy(type='BS', base_stock_level=opt_S, node=network.nodes[0])\n",
        "avg_sim_cost, _ = sim.run_multiple_trials(network, 10, 1000, progress_bar=False)\n",
        "print(f\"Average cost per period is {avg_sim_cost:4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZaoAPRy9E33",
        "outputId": "613b15a7-e6a4-406a-88c7-30230a8595a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cost per period is 4.346300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize:"
      ],
      "metadata": {
        "id": "yhRQ2hmLEMf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate([\n",
        "    [\"Optimal base-stock level\", opt_S],\n",
        "    [\"Average discounted reward per episode from environment play()\", avg_reward],\n",
        "    [\"Optimal expected discounted reward per episode\", exp_disc_cost],\n",
        "    [\"Optimal expected cost per period\", opt_cost],\n",
        "    [\"Simulated average cost per period\", avg_sim_cost]\n",
        "]))"
      ],
      "metadata": {
        "id": "gtSPG04I81GN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f77d7b-f22f-412d-c459-6bc49d3775fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------  ---------\n",
            "Optimal base-stock level                                          8\n",
            "Average discounted reward per episode from environment play()  -281.451\n",
            "Optimal expected discounted reward per episode                   86.864\n",
            "Optimal expected cost per period                                  4.3432\n",
            "Simulated average cost per period                                 4.4347\n",
            "-------------------------------------------------------------  ---------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If You Have Extra Time\n",
        "\n",
        "As in the \"MPNV as MDP\" notebook, try adding a fixed cost $K$ to the environment. Train the RL algorithm using the same instance as before, plus $K=20$. Check whether the RL agent learned to follow an $(s,S)$ policy. \n",
        "\n",
        "Also use `stockpyl` to find the optimal $s$ and $S$, via the `s_s_discrete_exact()` function (see documentation [here](https://stockpyl.readthedocs.io/en/latest/api/seio/ss.html#stockpyl.ss.s_s_discrete_exact)). In particular, the following code solves the $(s,S)$ problem for a Poisson distribution with mean `mu`:\n",
        "\n",
        "```python\n",
        "from stockpyl.ss import s_s_discrete_exact\n",
        "s, S, cost = s_s_discrete_exact(holding_cost=h, stockout_cost=p, fixed_cost=K, use_poisson=True, demand_mean=mu)\n",
        "```\n",
        "\n",
        "(Unfortunately, `stockpyl` cannot (yet) handle fixed costs in the simulation, so you can't simulate this system.)\n",
        "\n"
      ],
      "metadata": {
        "id": "wZjvMPx0AntA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PefhsuheAoL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}